{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#imports \n",
    "import skimage\n",
    "from skimage.io import imread, imshow, imsave, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#get file names\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "\n",
    "#calculate mode\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='blue'/> DataSet Exploration Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Different dataset formats:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOLO: <object-class> <x_center> <y_center> <width> <height>\n",
    "BrainWash: (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert__xmin_ymin_xmax_ymax__2__class_xcent_ycent_width_height(box, imageW, imageH):\n",
    "    xmin = (float(box[0])) / imageW\n",
    "    ymin = (float(box[1])) / imageH\n",
    "    xmax = (float(box[2])) / imageW\n",
    "    ymax = (float(box[3])) / imageH\n",
    "    \n",
    "    h = (ymax - ymin)\n",
    "    w = (xmax - xmin)\n",
    "    xcent = xmin + w/2\n",
    "    ycent = ymin + h/2\n",
    "\n",
    "    # add 0 for class head\n",
    "    return (0, xcent, ycent, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert__class_xmin_ymin_w_h__2__xcent_ycent_width_height(box):\n",
    "    classN = int(box[0])\n",
    "    xmin = float(box[1])\n",
    "    ymin = float(box[2])\n",
    "    w = float(box[3])\n",
    "    h = float(box[4])\n",
    "    \n",
    "    xcent = xmin + w/2\n",
    "    ycent = ymin + h/2\n",
    "    \n",
    "    return (classN, xcent, ycent, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parseBrainWashLabelString(stringsList, imageW, imageH):\n",
    "    outList = []\n",
    "    \n",
    "    split2 = stringsList[1][1:-2].split(\"),\")\n",
    "    for i in range(len(split2)):\n",
    "        split2[i] = split2[i].replace(\"(\", '')\n",
    "        split2[i] = split2[i].replace(\")\", '')\n",
    "        split2[i] = split2[i].replace(' ', '')\n",
    "        \n",
    "        outList.append(split2[i].split(','))\n",
    "        #use below to convert to yolo. Use above to keep original format\n",
    "#         outList.append(\n",
    "#             convert__xmin_ymin_xmax_ymax__2__class_xcent_ycent_width_height(\n",
    "#                 split2[i].split(','), imageW, imageH\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "    return outList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processIDL_file(filePath, headLabelsDict):\n",
    "    idlFile = open(filePath, 'r')\n",
    "    fileLines = idlFile.readlines()\n",
    "    \n",
    "    for fileLine in fileLines:\n",
    "        split1 = fileLine.split(\":\")\n",
    "        imageName = split1[0][:-1].split(\"/\")[-1]\n",
    "        split2 = imageName.split('_')[1].split('.')[0].split('x')\n",
    "        imgWidth = int(split2[0])\n",
    "        imgHeight = int(split2[1])\n",
    "        \n",
    "        saveImageName = imageName.replace(\".png\",\".jpg\")\n",
    "        saveImageName = saveImageName.replace('\\\"','')\n",
    "        saveImageName = saveImageName.replace(';','')\n",
    "        \n",
    "        if not(imgWidth == 640 and imgHeight == 480):\n",
    "            print(\"\\n***different image size obsereved****\\n\")\n",
    "        \n",
    "        if len(split1) > 1:\n",
    "            headLabelsDict[saveImageName] = parseBrainWashLabelString(split1, imgWidth, imgHeight)\n",
    "        else:\n",
    "            headLabelsDict[saveImageName] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processBordPredFile__xmin_ymin_w_h(imageId, boardPredsPath):\n",
    "    labelsList = []\n",
    "    \n",
    "    resFile = open(boardPredsPath + imageId.replace('.jpg', '.txt'), 'r')\n",
    "    resLines =resFile.readlines()\n",
    "    \n",
    "    for line in resLines: \n",
    "        line = line.replace(\"\\n\", '')\n",
    "        split1 = line.split(' ')\n",
    "        labelsList.append(convert__class_xmin_ymin_w_h__2__xcent_ycent_width_height(split1))\n",
    "            \n",
    "    return labelsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def writeLabelFile__class_xcent_ycent_width_height(imageId, outputLabelsPath, labelsList):\n",
    "    outFile = open(outputLabelsPath + imageId.replace('.jpg', '.txt'), 'w')\n",
    "    \n",
    "    for label in labelsList:\n",
    "        strL = str(label[0]) + ' ' + str(label[1]) + ' ' + str(label[2]) + ' ' +\\\n",
    "        str(label[3]) + ' ' + str(label[4]) + '\\n' \n",
    "        outFile.write(strL)\n",
    "    \n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualizeLabelLine(labelList, img):\n",
    "    xcent = float(labelList[1]) * img.shape[1]\n",
    "    ycent = float(labelList[2]) * img.shape[0]\n",
    "    w = float(labelList[3]) * img.shape[1]\n",
    "    h = float(labelList[4]) * img.shape[0]\n",
    "    \n",
    "    xmin = int(xcent - w/2)\n",
    "    ymin = int(ycent - h/2)\n",
    "    xmax = int(xcent + w/2)\n",
    "    ymax = int(ycent + h/2)\n",
    "    cv2.rectangle(img,(xmin, ymin),(xmax, ymax),(255,0,0),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualizePredLine(labelList, img):\n",
    "    xcent = float(labelList[2]) * img.shape[1]\n",
    "    ycent = float(labelList[3]) * img.shape[0]\n",
    "    w = float(labelList[4]) * img.shape[1]\n",
    "    h = float(labelList[5]) * img.shape[0]\n",
    "    \n",
    "    xmin = int(xcent - w/2)\n",
    "    ymin = int(ycent - h/2)\n",
    "    xmax = int(xcent + w/2)\n",
    "    ymax = int(ycent + h/2)\n",
    "    cv2.rectangle(img,(xmin, ymin),(xmax, ymax),(255,0,0),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## pipeline (using above methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualizing BrainWash Original  xmin_ymin_xmax_ymax\n",
    "headLabelsDict = {}\n",
    "processIDL_file('/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_test.idl',\n",
    "                headLabelsDict)\n",
    "processIDL_file('/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_val.idl',\n",
    "                headLabelsDict)\n",
    "processIDL_file('/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_train.idl',\n",
    "                headLabelsDict)\n",
    "\n",
    "imagesPath = \"/home/hooman/brainwash_headDetection_dataset/brainwash/11_13_2014/brainwash_11_13_2014_images_hsJPG/\"\n",
    "visPath = \"/home/hooman/brainwash_headDetection_dataset/brainwash/trainSetVis_11_13_2014/\"\n",
    "\n",
    "for imgId in os.listdir(imagesPath):\n",
    "    print(imgId)\n",
    "    img = imread(imagesPath + imgId)\n",
    "\n",
    "    labels = headLabelsDict[imgId]\n",
    "    for label in labels:\n",
    "        xmin = int(float(label[0]))\n",
    "        ymin = int(float(label[1]))\n",
    "        xmax = int(float(label[2]))\n",
    "        ymax = int(float(label[3]))\n",
    "\n",
    "        cv2.rectangle(img,(xmin, ymin),(xmax, ymax),(255,0,0),3)\n",
    "\n",
    "    imsave(visPath + imgId, img)\n",
    "#     plt.imshow(img)\n",
    "#     plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting brain wash original and board preds to YOLO\n",
    "imagesPath = \"/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_10_27_2014_images_hsJPG/\"\n",
    "outputLabelsPath = \"/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_10_27_2014_hsLabels_justHead/\"\n",
    "boardPredsPath = \"/home/hooman/brainwash_headDetection_dataset/work_results_text/\"\n",
    "\n",
    "headLabelsDict = {}\n",
    "processIDL_file('/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_test.idl',\n",
    "                headLabelsDict)\n",
    "processIDL_file('/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_val.idl',\n",
    "                headLabelsDict)\n",
    "processIDL_file('/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_train.idl',\n",
    "                headLabelsDict)\n",
    "\n",
    "for imageId in os.listdir(imagesPath):\n",
    "    predLabelsList = processBordPredFile__xmin_ymin_w_h(imageId, boardPredsPath)\n",
    "    combinedList = headLabelsDict[imageId] + predLabelsList\n",
    "    writeLabelFile__class_xcent_ycent_width_height(imageId, outputLabelsPath, combinedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# spliting test,valid,train for Brainwash\n",
    "filePath = '/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_train.idl'\n",
    "outFilePath = '/home/hooman/brainwash_headDetection_dataset/brainwash/official_trainSet2.txt'\n",
    "path2Append = '/home/hooman/brainwash_headDetection_dataset/brainwash/'\n",
    "\n",
    "outFile = open(outFilePath, 'w')\n",
    "\n",
    "idlFile = open(filePath, 'r')\n",
    "fileLines = idlFile.readlines()\n",
    "\n",
    "headLabelsDict = {}\n",
    "for fileLine in fileLines:\n",
    "    split1 = fileLine.split(\":\")\n",
    "    imageName = split1[0][:-1].split(\"/\")[-1]\n",
    "    finalImageName = imageName.replace(\".png\",\".jpg\")\n",
    "    finalImageName = finalImageName.replace('\\\"','')\n",
    "    finalImageName = finalImageName.replace(';','')\n",
    "    \n",
    "    imagePath = split1[0][:-1].split(\"/\")[0]\n",
    "    \n",
    "    if \"10_27_2014\" in imagePath:\n",
    "        savePath = path2Append + \"10_27_2014_all/\" +finalImageName\n",
    "    elif \"11_13_2014\" in imagePath:\n",
    "        savePath = path2Append + \"11_13_2014_all/\" +finalImageName\n",
    "    elif \"11_24_2014\" in imagePath:\n",
    "        savePath = path2Append + \"11_24_2014_all/\" +finalImageName\n",
    "    else:\n",
    "        print(imageName)\n",
    "        print(imagePath)\n",
    "        print(\"Error cannot decide what folder this belongs to.\")\n",
    "        break\n",
    "\n",
    "    outStr = str(savePath) + '\\n'\n",
    "    outFile.write(outStr)\n",
    "    #print(outStr)\n",
    "    \n",
    "outFile.close()\n",
    "idlFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualizing the training or test set from txt files\n",
    "path2TrainingSet = \"/home/hooman/brainwash_headDetection_dataset/brainwash/official_valSet.txt\"\n",
    "path2Vis = \"/home/hooman/brainwash_headDetection_dataset/brainwash/valSetVis/\"\n",
    "\n",
    "trains = open(path2TrainingSet, 'r')\n",
    "fileLines = trains.readlines()\n",
    "\n",
    "for fileLine in fileLines:\n",
    "    imagePath = fileLine.replace(\"\\n\", '')\n",
    "    img = imread(imagePath)\n",
    "    \n",
    "    labelFile = open(imagePath.replace(\".jpg\", \".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        visualizeLabelLine(line.split(\" \"), img)\n",
    "    \n",
    "    #print(path2Vis + imagePath.split(\"/\")[-1])\n",
    "    imsave(path2Vis + imagePath.split(\"/\")[-1], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#vis labels on images from directory\n",
    "pathIm = \"/home/hooman/justHead_testSet_fromTensent/images/\"\n",
    "pathLabels = '/home/hooman/justHead_testSet_fromTensent/yoloLabels_40-40/'\n",
    "path2Vis = '/home/hooman/justHead_testSet_fromTensent/vis_GT_40-40/'\n",
    "for imId in os.listdir(pathIm):\n",
    "    img = imread(pathIm + imId)\n",
    "    \n",
    "    try:\n",
    "        labelFile = open(pathLabels + imId.replace(\".jpg\",\".txt\"), 'r')\n",
    "    except:\n",
    "        print(\"Error didnot find labels for image\")\n",
    "        print(imId)\n",
    "#         shutil.copy(pathIm + imId, \"/home/hooman/brainwash_headDetection_dataset/brainwash/\"\\\n",
    "#                     \"11_13_2014_imagesWithNoCombinedLabels/\" +imId)\n",
    "        break\n",
    "\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        visualizeLabelLine(line.split(\" \"), img)\n",
    "\n",
    "    #print(path2Vis + imagePath.split(\"/\")[-1])\n",
    "    try:\n",
    "        imsave(path2Vis + imId, img)\n",
    "    except:\n",
    "        print(\"Error could not save image\")\n",
    "        print(imId)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#vis preds on images from directory\n",
    "pathIm = \"/home/hooman/coco/hsTest100_images/\"\n",
    "for imId in os.listdir(pathIm):\n",
    "    img = imread(pathIm + imId)\n",
    "    \n",
    "   \n",
    "    labelFile = open(\n",
    "        \"/home/hooman/headAndTorso_detection/release/text_res_balTestSetHH_3Class/\" + imId.replace(\".jpg\",\".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        visualizePredLine(line.split(\" \"), img)\n",
    "\n",
    "    #print(path2Vis + imagePath.split(\"/\")[-1])\n",
    "    imsave(\"/home/hooman/headAndTorso_detection/release/visFormahdi/\" + imId, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# concat multiple label files\n",
    "labelDir1 = \"/home/hooman/AlphaPose/11_13_2014_torosLabels_all/\"\n",
    "labelDir2 = \"/home/hooman/AlphaPose/11_13_2014_personLabels_all/\"\n",
    "labelDir3 = \"/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_11_13_2014_hsLabels_justHead/\"\n",
    "filanLabelDir = \"/home/hooman/brainwash_headDetection_dataset/brainwash/11_13_2014_combinedLabels_yoloAndPose/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        with open(labelDir1 + textFileName) as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "        with open(labelDir2 + textFileName) as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "        with open(labelDir3 + textFileName) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# concat hs and mahdi labels for hardhat1\n",
    "labelDir1 = \"/home/hooman/hardHat_dataset/labels_hs/\"\n",
    "labelDir2 = \"/home/hooman/hardHat_dataset/labels_mahdi/\"\n",
    "\n",
    "filanLabelDir = \"/home/hooman/hardHat_dataset/combined_labels/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        \n",
    "        lab1F = open(labelDir1 + textFileName, 'r')\n",
    "        lab1Lines = lab1F.readlines()\n",
    "        for ln in lab1Lines:\n",
    "            newLine1 = ln[0] + ln[10:]\n",
    "            outfile.write(newLine1)\n",
    "\n",
    "            \n",
    "        lab2F = open(labelDir2 + textFileName, 'r')\n",
    "        lab2Lines = lab2F.readlines()\n",
    "        for ln in lab2Lines:\n",
    "            if ln[0] == \"0\":\n",
    "                newLine = \"2\" + ln[1:]\n",
    "            if ln[0] == \"1\" or ln[0] == \"2\":\n",
    "                newLine = \"1\" + ln[1:]\n",
    "                \n",
    "            outfile.write(newLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# concat hs and mahdi labels for hardhat2\n",
    "labelDir1 = \"/home/hooman/hardHat_dataset/5class_labels/\"\n",
    "labelDir2 = \"/home/hooman/hardHat_dataset/labels_mahdi/\"\n",
    "\n",
    "filanLabelDir = \"/home/hooman/hardHat_dataset/combined_labels/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        \n",
    "        lab1F = open(labelDir1 + textFileName, 'r')\n",
    "        lab1Lines = lab1F.readlines()\n",
    "        for ln in lab1Lines:\n",
    "            if ln[0] == \"0\" or ln[0] == \"1\" or ln[0] == \"2\" or ln[0] == \"3\" or ln[0] == \"4\":\n",
    "                newLine1 = \"0\" + ln[1:]\n",
    "            outfile.write(newLine1)\n",
    "\n",
    "            \n",
    "        lab2F = open(labelDir2 + textFileName, 'r')\n",
    "        lab2Lines = lab2F.readlines()\n",
    "        for ln in lab2Lines:\n",
    "            if ln[0] == \"0\":\n",
    "                newLine = \"2\" + ln[1:]\n",
    "            if ln[0] == \"1\" or ln[0] == \"2\":\n",
    "                newLine = \"1\" + ln[1:]\n",
    "                \n",
    "            outfile.write(newLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating labels for Mahdi\n",
    "labelDir1 = \"/home/hooman/hardHat_dataset/5class_labels/\"\n",
    "labelDir2 = \"/home/hooman/hardHat_dataset/labels_mahdi/\"\n",
    "\n",
    "filanLabelDir = \"/home/hooman/hardHat_dataset/labels4Mahdi/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        \n",
    "        lab1F = open(labelDir1 + textFileName, 'r')\n",
    "        lab1Lines = lab1F.readlines()\n",
    "        for ln in lab1Lines:\n",
    "            if ln[0] == \"1\" or ln[0] == \"2\" or ln[0] == \"3\" or ln[0] == \"0\":\n",
    "                newLine1 = \"4\" + ln[1:]\n",
    "            if ln[0] == \"4\":\n",
    "                newLine1 = \"3\" + ln[1:]\n",
    "            outfile.write(newLine1)\n",
    "\n",
    "            \n",
    "        lab2F = open(labelDir2 + textFileName, 'r')\n",
    "        lab2Lines = lab2F.readlines()\n",
    "        for ln in lab2Lines:   \n",
    "            outfile.write(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pick random images\n",
    "import random\n",
    "path1 = \"/home/hooman/work_uniform/hardHatFromMahdi/justPreds/\"\n",
    "path2 = \"/home/hooman/work_uniform/hardHatFromMahdi/randomTestSet_200/\"\n",
    "nbOfItems2pick = 200\n",
    "\n",
    "all_list = os.listdir(path1)\n",
    "random.shuffle(all_list) #shuffle method\n",
    "\n",
    "for i in range(nbOfItems2pick):\n",
    "    shutil.copy(path1 + all_list[i], path2 + all_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert vest,noVest,hat,NoHat predsOrLabels to torso, head\n",
    "labelDir1 = \"/home/hooman/hardHat_dataset/randomTestSet_200_correctedLabels_vest/\"\n",
    "filanLabelDir = \"/home/hooman/hardHat_dataset/randomTestSet_200_correctedLabels_hs/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        \n",
    "        lab1F = open(labelDir1 + textFileName, 'r')\n",
    "        lab1Lines = lab1F.readlines()\n",
    "        for ln in lab1Lines:\n",
    "            if ln[0] == \"0\": #person\n",
    "                newLine1 = \"2\" + ln[1:]\n",
    "            \n",
    "            if ln[0] == \"1\" or ln[0] == \"2\": #torso\n",
    "                newLine1 = \"1\" + ln[1:]\n",
    "            \n",
    "            if ln[0] == \"3\" or ln[0] == \"4\": #head\n",
    "                newLine1 = \"0\" + ln[1:]\n",
    "\n",
    "            outfile.write(newLine1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make text file out of folder and shuffle\n",
    "import random\n",
    "outFile = open(\"/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther\"\\\n",
    "               \"/trainingSet_contrast0.2-0.8.txt\", 'w')\n",
    "locPath = \"/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther\"\\\n",
    "\"/images_aug_contrast0.2-0.8/\"\n",
    "putInFilePath = \"/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther\"\\\n",
    "\"/imagesAndLabels_contrast0.2-0.8/\"\n",
    "\n",
    "namesList = os.listdir(locPath)\n",
    "random.shuffle(namesList) #shuffle method\n",
    "\n",
    "for fileName in namesList:\n",
    "    outStr = putInFilePath + fileName + '\\n'\n",
    "    outFile.write(outStr)\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# shuffle a textFile\n",
    "import random\n",
    "outFile = open(\"/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther\"\\\n",
    "               \"/trainingSet_all_shuffled1.txt\", 'w')\n",
    "\n",
    "inFile = open(\"/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther\"\\\n",
    "               \"/trainingSet_all.txt\", 'r')\n",
    "\n",
    "inLines = inFile.readlines()\n",
    "random.shuffle(inLines) #shuffle method\n",
    "\n",
    "outFile.writelines(inLines)\n",
    "outFile.close()\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check how many of each class in the entire set\n",
    "\n",
    "path = '/home/hooman/hardHat_dataset/temp_1000/'\n",
    "countHat = 0\n",
    "countNoHat = 0\n",
    "countPersonFromHead = 0\n",
    "countPerson = 0\n",
    "for fileName in os.listdir(path):\n",
    "    file = open(path + fileName, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for ln in lines:\n",
    "        if ln[0] == \"3\":\n",
    "            countNoHat +=1\n",
    "        if ln[0] == \"4\":\n",
    "            countHat +=1\n",
    "            \n",
    "        if ln[0] == \"0\":\n",
    "            countPerson +=1\n",
    "            \n",
    "        if ln[0] == \"3\" or ln[0] == \"4\":\n",
    "            countPersonFromHead+=1\n",
    "            \n",
    "print(\"noHatCount:   \" + str(countNoHat))\n",
    "print(\"hatCount:   \" + str(countHat))\n",
    "print(\"countPersonFromHead:   \" + str(countPersonFromHead))      \n",
    "print(\"countPerson:   \" + str(countPerson))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# vest preds to just head\n",
    "labelDir1 = \"/home/hooman/darknet_alex/hsHeadDetector_try6_onHH_vestClasses_1000Images/preds_BW_ck1000_0.05/\"\n",
    "filanLabelDir = \"/home/hooman/darknet_alex/hsHeadDetector_try6_onHH_vestClasses_1000Images/preds_BW_ck1000_0.05_justHead/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    if \".txt\" in textFileName:\n",
    "        with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "\n",
    "            lab1F = open(labelDir1 + textFileName, 'r')\n",
    "            lab1Lines = lab1F.readlines()\n",
    "            for ln in lab1Lines:\n",
    "                if ln[0] == \"3\" or ln[0] == \"4\": #head\n",
    "                    newLine1 = \"0\" + ln[1:]\n",
    "\n",
    "                outfile.write(newLine1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hs (head torso person) preds to just head\n",
    "labelDir1 = \"/home/hooman/justHead_testSet_fromTensent/predsText/\"\n",
    "filanLabelDir = \"/home/hooman/justHead_testSet_fromTensent/predsText_justHead/\"\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    if \".txt\" in textFileName:\n",
    "        with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "\n",
    "            lab1F = open(labelDir1 + textFileName, 'r')\n",
    "            lab1Lines = lab1F.readlines()\n",
    "            for ln in lab1Lines:\n",
    "                if ln[0] == \"0\": #head\n",
    "                    newLine1 = ln\n",
    "                    outfile.write(newLine1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert preds to labels\n",
    "labelDir1 = \"/home/hooman/otherHardHat_dataset/Test/dirty/\"\n",
    "filanLabelDir = \"/home/hooman/otherHardHat_dataset/Test/cleaned/\"\n",
    "\n",
    "for textFileName in os.listdir(labelDir1):\n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        \n",
    "        lab1F = open(labelDir1 + textFileName, 'r')\n",
    "        lab1Lines = lab1F.readlines()\n",
    "        for ln in lab1Lines:\n",
    "            newLine1 = ln[0] + ln[8:]\n",
    "            outfile.write(newLine1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#remove labels less than 40*40\n",
    "path2Labels = \"/home/hooman/justHead_testSet_fromTensent/yoloLabels/\"\n",
    "filanLabelDir = \"/home/hooman/justHead_testSet_fromTensent/yoloLabels_40-40/\"\n",
    "\n",
    "for textFileName in os.listdir(path2Labels):\n",
    "    labelFile = open(path2Labels+ textFileName, 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    \n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        for line in labelLines:\n",
    "            labelList = line.split(\" \")\n",
    "            xcent = float(labelList[1]) * img.shape[1]\n",
    "            ycent = float(labelList[2]) * img.shape[0]\n",
    "            w = float(labelList[3]) * img.shape[1]\n",
    "            h = float(labelList[4]) * img.shape[0]\n",
    "\n",
    "            if w > 40.0 and h > 40.0:\n",
    "                outfile.write(line)\n",
    "                \n",
    "        labelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#remove preds less than 40*40\n",
    "path2Labels = \"/home/hooman/headAndTorso_detection/release/text_res_balTestSetHH_3Class_justHead/\"\n",
    "filanLabelDir = \"/home/hooman/headAndTorso_detection/release/text_res_balTestSetHH_3Class_justHead_above40x40/\"\n",
    "\n",
    "for textFileName in os.listdir(path2Labels):\n",
    "    labelFile = open(path2Labels + textFileName, 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    \n",
    "    with open(filanLabelDir + textFileName, 'w') as outfile:\n",
    "        for line in labelLines:\n",
    "            labelList = line.split(\" \")\n",
    "            xcent = float(labelList[2]) * img.shape[1]\n",
    "            ycent = float(labelList[3]) * img.shape[0]\n",
    "            w = float(labelList[4]) * img.shape[1]\n",
    "            h = float(labelList[5]) * img.shape[0]\n",
    "\n",
    "            if w > 40.0 and h > 40.0:\n",
    "                outfile.write(line)\n",
    "                \n",
    "        labelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert tensent json labels(xmin, xmax, ymin, ymax) to yolo(xcent, ycent, w, h)\n",
    "\n",
    "imagesPath = '/home/hooman/justHead_testSet_fromTensent/images/'\n",
    "path2SaveLabels = '/home/hooman/justHead_testSet_fromTensent/yoloLabels/'\n",
    "jsonFile = open('/home/hooman/justHead_testSet_fromTensent/gongdi_results.json')\n",
    "data = json.load(jsonFile)\n",
    "\n",
    "for item in data:\n",
    "    imageName = item['name'].split('//')[-1]\n",
    "    if '.jpeg' in imageName:\n",
    "        imageName = imageName.replace('.jpeg', '.jpg')\n",
    "    if '.png' in imageName:\n",
    "        imageName = imageName.replace('.png', '.jpg')\n",
    "\n",
    "    try:\n",
    "        image = imread(imagesPath + imageName)\n",
    "        imH, imW, _ = image.shape\n",
    "    except:\n",
    "        print('Error could not open image:')\n",
    "        print(imageName) \n",
    "        print('\\n')\n",
    "        break\n",
    "        \n",
    "    with open(path2SaveLabels + imageName.replace(\".jpg\", \".txt\"), 'w') as outfile:\n",
    "        if item[\"labels\"]:\n",
    "            for lab in item[\"labels\"]:\n",
    "                classId = -1\n",
    "                if lab[\"category\"] == \"nohelmet\":\n",
    "                    classId = 0\n",
    "                elif lab[\"category\"] == \"helmet\":\n",
    "                    #classId = 1\n",
    "                    classId = 0\n",
    "                else:\n",
    "                    print(\"ERROR unexpected category\")\n",
    "                    print(imageName)\n",
    "                    print('\\n')\n",
    "                    break\n",
    "\n",
    "                xmin = lab[\"box2d\"][\"x1\"] / imW\n",
    "                xmax = lab[\"box2d\"][\"x2\"] / imW\n",
    "                ymin = lab[\"box2d\"][\"y1\"] / imH\n",
    "                ymax = lab[\"box2d\"][\"y2\"] / imH\n",
    "\n",
    "                xcent = (xmax + xmin)/2\n",
    "                ycent = (ymax + ymin)/2\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "\n",
    "                newLine = str(classId) + ' ' + str(xcent) + ' ' + str(ycent) +\\\n",
    "                ' ' + str(width) + ' ' + str(height) + '\\n'\n",
    "                outfile.write(newLine)\n",
    "        else:\n",
    "            print('imageWith no label')\n",
    "            print(imageName)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#find missing labels\n",
    "imagesPath = \"/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/images/\"\n",
    "labelsPath = \"/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/yoloLabels/\"\n",
    "path2MvImagesWithNoLabels = '/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/temp/'\n",
    "\n",
    "labelsDic = {}\n",
    "for labelName in os.listdir(labelsPath):\n",
    "    labelsDic[labelName.replace(\".txt\", \".jpg\")] = 1\n",
    "    \n",
    "for imgName in os.listdir(imagesPath):\n",
    "    if imgName not in labelsDic.keys():\n",
    "        shutil.move(imagesPath + imgName, path2MvImagesWithNoLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# split images into 4 \n",
    "imagesPath = '/home/hooman/justHead_testSet_fromTensent/images/'\n",
    "splitImagesPath = '/home/hooman/justHead_testSet_fromTensent/imagesSplit_overlap55/'\n",
    "overlap = 55#10\n",
    "for imageName in os.listdir(imagesPath):\n",
    "    image = imread(imagesPath + imageName)\n",
    "\n",
    "    imH, imW, _ = image.shape\n",
    "    hh = int(imH/2)\n",
    "    hw = int(imW/2)\n",
    "\n",
    "    imL1 = image[0:hh+overlap,0:hw+overlap,:]\n",
    "    imL2 = image[hh:imH,0:hw+overlap,:]\n",
    "    imR1 = image[0:hh+overlap,hw:imW,:]\n",
    "    imR2 = image[hh:imH,hw:imW,:]\n",
    "\n",
    "    imsave(splitImagesPath + 'imL1_' + imageName, imL1)\n",
    "    imsave(splitImagesPath + 'imL2_' + imageName, imL2)\n",
    "    imsave(splitImagesPath + 'imR1_' + imageName, imR1)\n",
    "    imsave(splitImagesPath + 'imR2_' + imageName, imR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# stich the predictions on split images back together\n",
    "imagesPath = '/home/hooman/justHead_testSet_fromTensent/images/'\n",
    "splitPredsPath = '/home/hooman/justHead_testSet_fromTensent/predsText/'\n",
    "stichedPresPath = '/home/hooman/justHead_testSet_fromTensent/predsText_stiched/'\n",
    "overlap = 55#10\n",
    "\n",
    "for imageName in os.listdir(imagesPath):\n",
    "    image = imread(imagesPath + imageName)\n",
    "    imH, imW, _ = image.shape\n",
    "    hh = int(imH/2)\n",
    "    hw = int(imW/2)\n",
    "\n",
    "    with open(stichedPresPath + imageName.replace(\".jpg\",\".txt\"), 'w') as outfile:\n",
    "        \n",
    "        predFileL1 = open(splitPredsPath+ 'imL1_' + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "        labelLines = predFileL1.readlines()\n",
    "        for line in labelLines:\n",
    "            labelList = line.split(\" \")\n",
    "            \n",
    "            predClass = labelList[0]\n",
    "            predProb = labelList[1]\n",
    "            xcent = float(labelList[2]) * (hw+overlap)\n",
    "            ycent = float(labelList[3]) * (hh+overlap)\n",
    "            w = float(labelList[4]) * (hw+overlap)\n",
    "            h = float(labelList[5]) * (hh+overlap)\n",
    "\n",
    "            new_xcent = xcent / imW\n",
    "            new_ycent = ycent / imH\n",
    "            new_w = w / imW\n",
    "            new_h = h / imH\n",
    "            \n",
    "            outStr = str(predClass) + ' ' + str(predProb) + ' ' + str(new_xcent) +\\\n",
    "            ' ' + str(new_ycent) + ' ' + str(new_w) + ' ' + str(new_h) + '\\n' \n",
    "            outfile.write(outStr)\n",
    "            \n",
    "        predFileL1.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "        predFileL2 = open(splitPredsPath+ 'imL2_' + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "        labelLines = predFileL2.readlines()\n",
    "        for line in labelLines:\n",
    "            labelList = line.split(\" \")\n",
    "            \n",
    "            predClass = labelList[0]\n",
    "            predProb = labelList[1]\n",
    "            xcent = float(labelList[2]) * (hw+overlap)\n",
    "            ycent = float(labelList[3]) * (imH - hh)\n",
    "            w = float(labelList[4]) * (hw+overlap)\n",
    "            h = float(labelList[5]) * (imH - hh)\n",
    "\n",
    "            new_xcent = xcent / imW\n",
    "            new_ycent = (ycent + hh) / imH\n",
    "            new_w = w / imW\n",
    "            new_h = h / imH\n",
    "\n",
    "            outStr = str(predClass) + ' ' + str(predProb) + ' ' + str(new_xcent) +\\\n",
    "            ' ' + str(new_ycent) + ' ' + str(new_w) + ' ' + str(new_h) + '\\n' \n",
    "            outfile.write(outStr)\n",
    "        \n",
    "        predFileL2.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        predFileR1 = open(splitPredsPath+ 'imR1_' + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "        labelLines = predFileR1.readlines()\n",
    "        for line in labelLines:\n",
    "            labelList = line.split(\" \")\n",
    "            \n",
    "            predClass = labelList[0]\n",
    "            predProb = labelList[1]\n",
    "            xcent = float(labelList[2]) * (imW - hw)\n",
    "            ycent = float(labelList[3]) * (hh+overlap)\n",
    "            w = float(labelList[4]) * (imW - hw)\n",
    "            h = float(labelList[5]) * (hh+overlap)\n",
    "\n",
    "            new_xcent = (xcent + hw) / imW\n",
    "            new_ycent = ycent / imH\n",
    "            new_w = w / imW\n",
    "            new_h = h / imH\n",
    "            \n",
    "            outStr = str(predClass) + ' ' + str(predProb) + ' ' + str(new_xcent) +\\\n",
    "            ' ' + str(new_ycent) + ' ' + str(new_w) + ' ' + str(new_h) + '\\n'  \n",
    "            outfile.write(outStr)\n",
    "        \n",
    "        predFileR1.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        predFileR2 = open(splitPredsPath+ 'imR2_' + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "        labelLines = predFileR2.readlines()\n",
    "        for line in labelLines:\n",
    "            labelList = line.split(\" \")\n",
    "            \n",
    "            predClass = labelList[0]\n",
    "            predProb = labelList[1]\n",
    "            xcent = float(labelList[2]) * (imW - hw)\n",
    "            ycent = float(labelList[3]) * (imH - hh)\n",
    "            w = float(labelList[4]) * (imW - hw)\n",
    "            h = float(labelList[5]) * (imH - hh)\n",
    "\n",
    "            new_xcent = (xcent + hw) / imW\n",
    "            new_ycent = (ycent + hh) / imH\n",
    "            new_w = w / imW\n",
    "            new_h = h / imH\n",
    "            \n",
    "            outStr = str(predClass) + ' ' + str(predProb) + ' ' + str(new_xcent) +\\\n",
    "            ' ' + str(new_ycent) + ' ' + str(new_w) + ' ' + str(new_h) + '\\n'  \n",
    "            outfile.write(outStr)\n",
    "        \n",
    "        predFileR2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='blue'/> Image Manipulation Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Images: Copying, Moving, Deleting from different directoreis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# deleting files that are not in one dir from another\n",
    "fileToKeepDict = {}\n",
    "\n",
    "for fileName in os.listdir('/home/hooman/Downloads/MOTIONMETRICS (2)/FM/preds/'):\n",
    "    fileToKeepDict[fileName[:-3]+'gmp'] = 1\n",
    "\n",
    "\n",
    "    \n",
    "for fileName in os.listdir('/home/hooman/Downloads/MOTIONMETRICS (2)/FM/1947EFC0-16D8-1588-A042-3534DFB3FA0F/'):\n",
    "    if fileName not in fileToKeepDict:\n",
    "        print(fileName)\n",
    "        os.remove('/home/hooman/Downloads/MOTIONMETRICS (2)/FM/1947EFC0-16D8-1588-A042-3534DFB3FA0F/' + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# deleting images that cannot be opened (usaully after augmentation)\n",
    "dirToDeleteFrom = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/randomCroppedImages/'\n",
    "\n",
    "for fileName in os.listdir(dirToDeleteFrom):\n",
    "    try:\n",
    "        img = imread(dirToDeleteFrom + fileName)\n",
    "    except:\n",
    "        os.remove(dirToDeleteFrom + fileName)\n",
    "        os.remove('/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/randomCroppedMasks/' + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# copying files from one dir to another\n",
    "\n",
    "import shutil\n",
    "\n",
    "for imgId in os.listdir('/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/dataCleanup_round1/goodLabels'):\n",
    "    shutil.copy('/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/images/' + imgId, '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/try1_only_dataCleanup_round1_goodImages/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exclusing testSet images from trainingSet for BucketTracking\n",
    "# copying files from one dir to another\n",
    "\n",
    "dirWithListOfimages = \"/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/goodMatInsides_forBBLabelingOfYolo/\"\n",
    "\n",
    "dir2RemoveFrom = \"/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/cable/fmdl-cable-trainingData/images/\"\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "movedLabels = []\n",
    "\n",
    "for imgId in os.listdir(dirWithListOfimages):\n",
    "    \n",
    "    movedLabels.append(imgId)\n",
    "\n",
    "    imgName = imgId.replace('.jpg', '.xml')\n",
    "    \n",
    "    \n",
    "    if os.path.isfile(dir2RemoveFrom + imgId):\n",
    "    #if os.path.isfile(dir2RemoveFrom + imgName):\n",
    "    \n",
    "        #shutil.move(dir2RemoveFrom + imgName, '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/cable/validationsSet_hsPicked_labels')\n",
    "        \n",
    "        shutil.copy(dir2RemoveFrom + imgId, '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/temp')\n",
    "    \n",
    "        #os.remove(dir2RemoveFrom + imgId)\n",
    "        #os.remove(dir2RemoveFrom + imgName)\n",
    "        print(dir2RemoveFrom + imgName)\n",
    "        \n",
    "    \n",
    "print(\"\")\n",
    "print(\"Moved labels and deleted images for  \" + str(len(movedLabels))  + \"  examples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Images: resizing, converting formats and channels, correcting ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Resize and downsample all images to (128, 160, 3)\n",
    "resizedImagesPath = '/home/hooman/dataPreparation/hsTestSet/images0PaddedForUNet/'\n",
    "\n",
    "for fileName in os.listdir(imagesPath):\n",
    "\n",
    "    img = imread(imagesPath + fileName) \n",
    "    \n",
    "    imgResized = cv2.resize(img, (640, 480)) \n",
    "\n",
    "    # you can downsample with numpy too. I use opencv to be consistant.    imgDs = img[::4, ::4, :]\n",
    "    imgDs = cv2.resize(imgResized, (0,0), fx=0.25, fy=0.25) \n",
    "\n",
    "    imgPadded = cv2.copyMakeBorder(imgDs,4,4,0,0,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
    "    \n",
    "    cv2.imwrite(resizedImagesPath + fileName, imgPadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting single channel images to 3 channels\n",
    "imagesPath = '/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/images/'\n",
    "\n",
    "for imId in os.listdir(imagesPath):\n",
    "    try:  \n",
    "        img = imread(imagesPath + imId)\n",
    "    except:\n",
    "        print('couldnt open')\n",
    "        print(imId)\n",
    "        print('\\n')\n",
    "        \n",
    "    if len(img.shape) < 3:\n",
    "        print(imId)\n",
    "        print(img.shape)\n",
    "        print('\\n')\n",
    "        \n",
    "        img3Chan = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "        cv2.imwrite('/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/conv/' + imId, img3Chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting single channel images to 3 channels (LOOPING OVER MULTIPLE FOLDERS)\n",
    "\n",
    "img_dest_dir = '/home/hooman/Desktop/i2lData_cropped/'\n",
    "\n",
    "\n",
    "\n",
    "for mainDir in os.listdir(img_dest_dir):\n",
    "    \n",
    "\n",
    "    for imId in os.listdir(img_dest_dir + mainDir):\n",
    "\n",
    "        img = imread(img_dest_dir + mainDir + '/' + imId)\n",
    "        if len(img.shape) < 3:\n",
    "            img3Chan = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            cv2.imwrite(img_dest_dir + mainDir + '/' + imId, img3Chan)\n",
    "\n",
    "        else:\n",
    "            print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting jpg image to png, and removing the jpgs\n",
    "img_dest_dir = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/allImages/'\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "imageFileNameDic = {}\n",
    "\n",
    "for file in os.listdir(img_dest_dir):\n",
    "    fileName = file.replace(\".jpg\", \"\")\n",
    "    #fileName = file.replace(\".png\", \"\")\n",
    "    if fileName in imageFileNameDic:\n",
    "        imageFileNameDic[fileName] += 1\n",
    "    else:\n",
    "        imageFileNameDic[fileName] = 0\n",
    "\n",
    "        \n",
    "        \n",
    "for name in imageFileNameDic.keys():\n",
    "    print(name)\n",
    "    im = Image.open(img_dest_dir + '/' + name + '.jpg')\n",
    "    im.save(img_dest_dir + '/' + name + '.png')\n",
    "    os.remove(img_dest_dir + '/' + name + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting png image to jpg\n",
    "jpgSavePath = '/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_10_27_2014_images_hsJPG/'\n",
    "pngLoadPath = \"/home/hooman/brainwash_headDetection_dataset/brainwash/brainwash_10_27_2014_images/\"\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "for name in os.listdir(pngLoadPath):\n",
    "    print(name)\n",
    "    im = Image.open(pngLoadPath + name)\n",
    "    im.save(jpgSavePath + name.replace(\".png\",\".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting jpg image to png, and removing the jpgs (looping over multiple folders)\n",
    "\n",
    "img_dest_dir = '/home/hooman/Desktop/i2lData_cropped/'\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "for mainDir in os.listdir(img_dest_dir):\n",
    "\n",
    "\n",
    "    imageFileNameDic = {}\n",
    "\n",
    "    for file in os.listdir(img_dest_dir + mainDir):\n",
    "        \n",
    "        fileName = file.replace(\".jpg\", \"\")\n",
    "        #fileName = file.replace(\".png\", \"\")\n",
    "        if fileName in imageFileNameDic:\n",
    "            imageFileNameDic[fileName] += 1\n",
    "        else:\n",
    "            imageFileNameDic[fileName] = 0\n",
    "\n",
    "\n",
    "\n",
    "    for name in imageFileNameDic.keys():\n",
    "        print(name)\n",
    "        im = Image.open(img_dest_dir + mainDir + '/' + name + '.jpg')\n",
    "        im.save(img_dest_dir + mainDir + '/' + name + '.png')\n",
    "        os.remove(img_dest_dir + mainDir + '/' + name + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compressing png images with jpeg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "saveDir = '/home/hooman/FM_PROJECT/dataPreparation/fmdlTestData-optical-hydraulic/compressedJpeg80/'\n",
    "\n",
    "for imId in os.listdir('/home/hooman/FM_PROJECT/dataPreparation/fmdlTestData-optical-hydraulic/Frame/'):\n",
    "    img = Image.open('/home/hooman/FM_PROJECT/dataPreparation/fmdlTestData-optical-hydraulic/Frame/'+ imId)\n",
    "    \n",
    "    fileName = imId.replace(\".png\", \"\")\n",
    "    \n",
    "    img.save(saveDir + '/' + fileName + '.jpg', quality=80,optimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Correcting image Ids by content matching singleImage\n",
    "#HSNOTE: this does not work with abs error must be squared.\n",
    "\n",
    "pathToCorrectNames = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/input-orig/'\n",
    "pathToWrongNames   = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/outputOfSaveH5/'\n",
    "pathToWrongPreds = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/ouputOfNetworkJustOutput/'\n",
    "\n",
    "\n",
    "for srcId in os.listdir(pathToWrongNames):\n",
    "\n",
    "    minScore = 100000000\n",
    "    minId = ''\n",
    "    \n",
    "    for targId in os.listdir(pathToCorrectNames):\n",
    "        srcIm = imread(pathToWrongNames + srcId)\n",
    "\n",
    "        temp= imread(pathToCorrectNames + targId)\n",
    "        targIm = cv2.resize(temp, (srcIm.shape[1], srcIm.shape[0])) \n",
    "\n",
    "        dif = np.square((srcIm - targIm))\n",
    "        score = np.sum(dif)\n",
    "\n",
    "        if score < minScore:\n",
    "            minScore = score\n",
    "            minId = targId\n",
    "            \n",
    "\n",
    "    print(\"src: \" + srcId + \"  matched with: \" + minId)\n",
    "    os.rename(pathToWrongNames + srcId, pathToWrongNames + minId)\n",
    "    os.rename(pathToWrongPreds + srcId, pathToWrongPreds + minId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Correcting image Ids by content matching allChannels\n",
    "\n",
    "\n",
    "pathToCorrectNames = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/input-orig/'\n",
    "pathToWrongNames   = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/outputOfSaveH5/'\n",
    "\n",
    "pathToCorrectNamesIn = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/ouputOfNetworkAllChannels/' \n",
    "\n",
    "namesDic = {}\n",
    "for srcId in os.listdir(pathToWrongNames):\n",
    "\n",
    "    minScore = 100000000\n",
    "    minId = ''\n",
    "    \n",
    "    for targId in os.listdir(pathToCorrectNames):\n",
    "        srcIm = imread(pathToWrongNames + srcId)\n",
    "\n",
    "        temp= imread(pathToCorrectNames + targId)\n",
    "        targIm = cv2.resize(temp, (srcIm.shape[1], srcIm.shape[0])) \n",
    "\n",
    "        dif = np.square((srcIm - targIm))\n",
    "        score = np.sum(dif)\n",
    "\n",
    "        if score < minScore:\n",
    "            minScore = score\n",
    "            minId = targId\n",
    "            \n",
    "    print(srcId + \"___\" + minId)\n",
    "    namesDic[srcId] = minId\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "for srcId in namesDic.keys():\n",
    "\n",
    "    #print(\"src: \" + srcId + \"  matched with: \" + minId)\n",
    "    \n",
    "    nameAr = srcId.split('_')\n",
    "    nameAr = nameAr[0:2]\n",
    "\n",
    "    shortName = \"\"\n",
    "    for i in range(len(nameAr)):\n",
    "        shortName = shortName + nameAr[i] + '_'\n",
    "\n",
    "    chanFiles = glob.glob1(pathToCorrectNamesIn, shortName + '*')\n",
    "    \n",
    "    for fil in chanFiles:\n",
    "        chan = fil.split('_')[2]\n",
    "        #print(chan)\n",
    "        \n",
    "        if chan[0:2] == \"ch\":\n",
    "            newname = chan + \"_\" + namesDic[srcId]\n",
    "            print(\"renamed CH: \" + fil + \" to: \" + newname + \"\\n\")\n",
    "            os.rename(pathToCorrectNamesIn + fil, pathToCorrectNamesIn + newname)\n",
    "        else:\n",
    "            newname = namesDic[srcId]\n",
    "            print(\"renamed: \" + fil + \" to: \" + newname + \"\\n\")\n",
    "            os.rename(pathToCorrectNamesIn + fil, pathToCorrectNamesIn + namesDic[srcId])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert .JPEG to .JPG\n",
    "path2Jpeg = '/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/jpeg/'\n",
    "path2SaveJpg = '/home/hooman/justHead_testSet_fromTensent/head/gongdi_67603/conv/'\n",
    "\n",
    "for imId in os.listdir(path2Jpeg):\n",
    "    try:\n",
    "        image = imread(path2Jpeg + imId)\n",
    "        imsave(path2SaveJpg + imId.replace('.jpeg', '.jpg'), image)\n",
    "    except:\n",
    "        print(imId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Images: Displaying side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Showing Images side-by-side\n",
    "\n",
    "predsDir1 = '/home/hooman/brainwash_headDetection_dataset/brainwash/10_27_2014_combinedLabels_yoloAndPose_vis/'\n",
    "\n",
    "predsDir2 = '/home/hooman/brainwash_headDetection_dataset/brainwash/10_27_2014_combinedLabels_justPose_vis/'\n",
    "\n",
    "dirToSaveResults = '/home/hooman/brainwash_headDetection_dataset/brainwash/yoloAndPose_sidebySide/'\n",
    "\n",
    "for imgId in os.listdir(predsDir1):  \n",
    "    #if '.png' in imgId:\n",
    "    if '.jpg' in imgId:\n",
    "        pred1 = imread(predsDir1 + imgId)\n",
    "        pred2 = imread(predsDir2 + imgId)\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "            combImg = np.zeros((pred1.shape[0],1400, 3), np.uint8)\n",
    "\n",
    "            combImg[:, 0:pred1.shape[1], :] = pred1\n",
    "            combImg[:, 700:pred2.shape[1]+700, :] = pred2\n",
    "\n",
    "            '''\n",
    "            combImg = np.zeros((pred1.shape[0],1400), np.uint8)\n",
    "\n",
    "            combImg[:, 0:pred1.shape[1]] = pred1\n",
    "            combImg[:, 700:pred2.shape[1]+700] = pred2\n",
    "            '''\n",
    "\n",
    "\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(combImg,'personFromYolo',(30,70), font, 2,(255,255,255), 2, 0)\n",
    "            cv2.putText(combImg,'personFromPose',(730,70), font, 2,(255,255,255), 2, 0)\n",
    "\n",
    "\n",
    "            #plt.imshow(combImg)\n",
    "            #plt.show()\n",
    "            #break\n",
    "\n",
    "            cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "        except:\n",
    "            print(imgId)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Showing Images side-by-side 3 images large\n",
    "predsDir1 = '/home/hooman/justHead_testSet_fromTensent/examples/labels_above40-40/'\n",
    "predsDir2 = '/home/hooman/justHead_testSet_fromTensent/examples/preds_all/'\n",
    "predsDir3 = '/home/hooman/justHead_testSet_fromTensent/examples/preds_onlyAbove40-40/'\n",
    "\n",
    "dirToSaveResults = '/home/hooman/justHead_testSet_fromTensent/examples/side/'\n",
    "\n",
    "for imgId in os.listdir(predsDir1):\n",
    "\n",
    "    pred1 = imread(predsDir1 + imgId)\n",
    "    pred2 = imread(predsDir2 + imgId)\n",
    "    pred3 = imread(predsDir3 + imgId)\n",
    "\n",
    "    totalW = pred1.shape[1] + pred2.shape[1] + pred2.shape[1] + 20\n",
    "    combImg = np.zeros((pred1.shape[0],totalW, 3), np.uint8)\n",
    "\n",
    "    combImg[:, 0:pred1.shape[1], :] = pred1\n",
    "    offset1 = pred1.shape[1] + 10\n",
    "    combImg[:, offset1:pred2.shape[1]+offset1, :] = pred2\n",
    "    offset2 = pred2.shape[1]+offset1 + 10\n",
    "    combImg[:, offset2:pred3.shape[1]+offset2, :] = pred3\n",
    "    \n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    textloc1 = int(pred1.shape[1] / 2) - 30\n",
    "    textloc2 = int((offset1 + pred2.shape[1]+offset1)/2)- 30\n",
    "    textloc3 = int((offset2 + pred3.shape[1]+offset2)/2)- 30\n",
    "    \n",
    "    cv2.putText(combImg,'groundTruth',(textloc1,70), font, 2,(0,255,0), 2, 0)\n",
    "    cv2.putText(combImg,'allPreds',(textloc2,70), font, 2,(0,255,0), 2, 0)\n",
    "    cv2.putText(combImg,'predsAbove40-40',(textloc3,70), font, 2,(0,255,0), 2, 0)\n",
    "\n",
    "    cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "    #cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "    imsave(dirToSaveResults + imgId, combImg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Putting the optical flow and U-Net outputs sidebyside.  \n",
    "\n",
    "#FMDL_2018.04.30_11.38.09.png\n",
    "\n",
    "temp = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/Frame/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "frame = cv2.cvtColor(temp, cv2.COLOR_GRAY2BGR) \n",
    "\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "of = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/OpticalFlowMagnitude/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "plt.imshow(of)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "no = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/NetOut/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "plt.imshow(no)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "temp2 = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/VES_finalOutput/fragmentation_results/all/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "fo = cv2.cvtColor(temp2, cv2.COLOR_GRAY2BGR) \n",
    "\n",
    "plt.imshow(fo)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(np.amax(of))\n",
    "\n",
    "\n",
    "combImg = np.zeros((frame.shape[0],2800, 3), np.uint8)\n",
    "\n",
    "combImg[:, 0:frame.shape[1], :] = frame\n",
    "combImg[:, 700:frame.shape[1]+700, :] = cv2.resize(cv2.cvtColor(of, cv2.COLOR_GRAY2BGR), (frame.shape[1], frame.shape[0])) \n",
    "combImg[:, 1400:frame.shape[1]+1400, :] = cv2.resize(cv2.cvtColor(no, cv2.COLOR_GRAY2BGR), (frame.shape[1], frame.shape[0])) \n",
    "combImg[:, 2100:fo.shape[1]+2100, :] = fo\n",
    "\n",
    "\n",
    "plt.imshow(combImg)\n",
    "plt.show()\n",
    "cv2.imwrite('/home/hooman/' + 'combined_FMDL_2018.04.30_11.38.09.png', combImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Converting legecy fm/wm and fontend stuff to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting .GMP to .FMDL\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "\n",
    "parentInputPath = \"/media/hooman/New Volume/FM_PROJECT_STORAGE/QC-Tests/Hydraulics/OyuTolgoi-CAT6060/good/in/\"\n",
    "\n",
    "outputPath = \"/media/hooman/New Volume/FM_PROJECT_STORAGE/QC-Tests/Hydraulics/OyuTolgoi-CAT6060/good/out/\"\n",
    "\n",
    "#for dires in os.listdir(parentInputPath):\n",
    " #   inputPath = parentInputPath + '/' + dires + '/'\n",
    "    \n",
    "    #files = glob.glob(inputPath+'*.gmp')\n",
    "\n",
    "for filename in os.listdir(parentInputPath):\n",
    "\n",
    "    print(filename)\n",
    "\n",
    "    f = open(parentInputPath + filename,'r')\n",
    "\n",
    "    text = f.read()\n",
    "\n",
    "    outFileName = text[text.find(\"filename\")+11:text.find(\"data\") -3]\n",
    "    print(outFileName)\n",
    "\n",
    "    if (text.find(\"FMDL\") > 0) & (text.find(\"FMDL\") < text.find(\"data\")):\n",
    "\n",
    "        img_txt = text[text.find(\"data\")+7:-4];\n",
    "        print(img_txt)\n",
    "\n",
    "        with open(outputPath+outFileName,\"wb\") as ff:\n",
    "\n",
    "            ff.write(base64.b64decode(img_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YAML to WMDL\n",
    "\n",
    "import glob\n",
    "from base64 import decodestring\n",
    "import base64\n",
    "\n",
    "\n",
    "import zlib\n",
    "import yaml\n",
    "filename = \"C:/Software/Dev/DLWM-20171026-105/resources/CNRL/SH1006/DLWM_reference.yaml\"\n",
    "with open(filename, 'r') as stream:\n",
    "    yamlfile = yaml.load(stream)\n",
    "    \n",
    "stream = open(filename, 'r')\n",
    "\n",
    "import cv2\n",
    "fs_read = cv2.FileStorage(\"D:/temp/test.yml\", cv2.FILE_STORAGE_READ)\n",
    "import yaml\n",
    "filename = \"D:/temp/test.yml\"\n",
    "with open(filename, 'r') as stream:\n",
    "    yamlfile = yaml.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#WMDL to YAML\n",
    "import glob\n",
    "from base64 import decodestring\n",
    "import base64\n",
    "\n",
    "inputPath = \"/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/logs/\"\n",
    "outputPath = \"/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/yaml/\"\n",
    "files = glob.glob(inputPath+'*.wmdl')\n",
    "import zlib  \n",
    "import os\n",
    "for filename in files:\n",
    "    wmdl = open(filename,'rb').read()\n",
    "    wmyaml = zlib.decompress(wmdl)\n",
    "    with open(outputPath+os.path.basename(filename)[:-5]+\".yaml\",\"wb\") as f:\n",
    "        f.write(wmyaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#WMDL GMP            \n",
    "import glob\n",
    "from base64 import decodestring\n",
    "import base64\n",
    "\n",
    "inputPath = \"N:/randd/temp/WMDL_Logs/WMDL_CVE_S20/\"\n",
    "outputPath = \"C:/Software/Dev/DLWM-20180201/resources/CV/SH20/input_2/\"\n",
    "files = glob.glob(inputPath+'*.gmp')\n",
    "for filename in files:\n",
    "    f = open(filename,'r')\n",
    "    text = f.read()\n",
    "    print(text[text.find(\"filename\")+11:text.find(\"data\")])\n",
    "    if (text.find(\"WMDL\") > 0) & (text.find(\"WMDL\") < text.find(\"data\")):\n",
    "        img_txt = text[text.find(\"data\")+7:-4];\n",
    "        with open(outputPath+text[text.find(\"filename\")+11:text.find(\".wmdl\")+5],\"wb\") as f:\n",
    "            f.write(base64.b64decode(img_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# MTDL GMP\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "inputPath = \"N:/temp/MTDL-Sishen/PH03_4100/\"\n",
    "outputPath = \"N:/randd/MachineLearning/Temp/Sishen/PH03_4100/\"\n",
    "files = glob.glob(inputPath+'*.gmp')\n",
    "for filename in files:\n",
    "    f = open(filename,'r')\n",
    "    text = f.read()\n",
    "    if (text.find(\"MTDL-LEGACY\") > 0) & (text.find(\"MTDL-LEGACY\") < text.find(\"data\")):\n",
    "        img_txt = text[text.find(\"data\")+7:-4];\n",
    "        with open(outputPath+text[text.find(\"filename\")+13:text.find(\".jpg\")+4],\"wb\") as f:\n",
    "            f.write(base64.b64decode(img_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hs saving YAML from FMDL\n",
    "import zlib\n",
    "\n",
    "logFile = '/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/logs/WMDL_2017.11.25_08.36.15.wmdl'\n",
    "\n",
    "\n",
    "dl_log = open(logFile, 'rb').read()\n",
    "\n",
    "print(dl_log)\n",
    "\n",
    "#decompressed_log = zlib.decompress(dl_log)\n",
    "\n",
    "#decompressed_log_corrected = decompressed_log.replace(b\"\\t\",b\"\")\n",
    "\n",
    "#decompressed_log = zlib.decompress(dl_log).decode(\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hs Saving Frames from YAML (this works)\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PilImage\n",
    "\n",
    "path2Yamls = '/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/yaml/'\n",
    "path2SaveFrames = '/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/out/'\n",
    "\n",
    "\n",
    "for yaml_file_name in os.listdir(path2Yamls):\n",
    "    if '.yaml' in yaml_file_name:\n",
    "        print('opening file:')\n",
    "        print(path2Yamls + yaml_file_name)\n",
    "\n",
    "        # Read YAML file\n",
    "        with open(path2Yamls + yaml_file_name, 'r') as stream:\n",
    "            data_loaded = yaml.load(stream)\n",
    "\n",
    "        #print(data_loaded['Frame'])\n",
    "\n",
    "        img_byteArray =  base64.b64decode(data_loaded['Frame'])\n",
    "\n",
    "        image = PilImage.open(io.BytesIO(img_byteArray)).convert(\"RGB\")\n",
    "        #image.save(path2SaveFrames + yaml_file_name.replace(\".yaml\", \".png\"),\"PNG\")\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening file:\n",
      "/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/yaml/WMDL_2017.11.25_08.36.15.yaml\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument should be a bytes-like object or ASCII string, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-40d341532de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(data_loaded['Frame'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fullAlgo/lib/python3.5/base64.py\u001b[0m in \u001b[0;36mb64decode\u001b[0;34m(s, altchars, validate)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bytes_from_decode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maltchars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0maltchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bytes_from_decode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maltchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fullAlgo/lib/python3.5/base64.py\u001b[0m in \u001b[0;36m_bytes_from_decode_data\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         raise TypeError(\"argument should be a bytes-like object or ASCII \"\n\u001b[0;32m---> 46\u001b[0;31m                         \"string, not %r\" % s.__class__.__name__) from None\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument should be a bytes-like object or ASCII string, not 'dict'"
     ]
    }
   ],
   "source": [
    "# hs Saving Frames from YAML (This decompresses too but hasn't worked for me yet)\n",
    "import os\n",
    "import io\n",
    "import zlib\n",
    "import base64\n",
    "from base64 import b64encode, b64decode\n",
    "import yaml\n",
    "from PIL import Image as PilImage\n",
    "\n",
    "path2Yamls = '/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/yaml/'\n",
    "path2SaveFrames = '/media/hooman/New Volume/WM_PROJECT_STORAGE/logsFromCobrePanama/Shovel03/oldLog/out/'\n",
    "\n",
    "\n",
    "for yaml_file_name in os.listdir(path2Yamls):\n",
    "    if '.yaml' in yaml_file_name:\n",
    "        print('opening file:')\n",
    "        print(path2Yamls + yaml_file_name)\n",
    "\n",
    "        # Read YAML file\n",
    "        with open(path2Yamls + yaml_file_name, 'r') as stream:\n",
    "            data_loaded = yaml.safe_load(stream)\n",
    "\n",
    "        #print(data_loaded['Frame'])\n",
    "\n",
    "        json_data = b64decode(data_loaded)\n",
    "        json_data = json.loads(zlib.decompress(json_data).decode(\"ascii\"))\n",
    "\n",
    "        img_byteArray =  base64.b64decode(json_data['Frame'])\n",
    "\n",
    "        image = PilImage.open(io.BytesIO(img_byteArray)).convert(\"RGB\")\n",
    "        image.save(path2SaveFrames + yaml_file_name.replace(\".yaml\", \".png\"),\"PNG\")\n",
    "\n",
    "        #plt.imshow(image)\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cleaning up CSVs with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# shuffling a csv  (adds an empty line somewhere, and moves the header)\n",
    "\n",
    "csvRows = readCsvRows('/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/backhoe/boxDetector_V2_multiclass/try2-withCaseObject-newData/trainingSet.csv')\n",
    "\n",
    "\n",
    "\n",
    "# shuffle the rows\n",
    "from random import shuffle\n",
    "shuffle(csvRows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#write the shuffled rows to csv\n",
    "csv_file = open('/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/backhoe/boxDetector_V2_multiclass/try2-withCaseObject-newData/trainingSet_shuffled.csv', \"w\") \n",
    "\n",
    "\n",
    "# write rows\n",
    "for row in csvRows:\n",
    "    csv_file.write(row + '\\n')\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"wrote \" + str(len(csvRows)) + \" rows to csv file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#read rows from the file you wanna append to\n",
    "existingRowsDic = getCertainClassRowsDictFromCsv('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try5/trainSet_multiClass_bucket_fineRock_try5_manuallyCleaned.csv', ['matInside'])\n",
    "\n",
    "\n",
    "\n",
    "#read rows from the file you wanna get the new rows from\n",
    "rowsDicToAddFrom = getCertainClassRowsDictFromCsv('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/unusedCsvFiles/trainSet_multiClass_bucket_fineRockInapp_try3_uncleaned.csv', ['matInside'])\n",
    "\n",
    "\n",
    "\n",
    "#Append the missing rows\n",
    "n = 0\n",
    "rowsDicToAddTo = {}\n",
    "for imId in os.listdir('/home/hooman/dataPreparation/hsTrainingSet/imsToAddMatInsideFor/'):\n",
    "    n += 1\n",
    "    print(imId)\n",
    "    if imId not in existingRowsDic:\n",
    "        if imId in rowsDicToAddFrom:\n",
    "            rowsDicToAddTo[imId] = rowsDicToAddFrom[imId]\n",
    "        else:\n",
    "            print(\"error didn't find:  \" + imId + \"\\n\")\n",
    "    else:\n",
    "        print(\"already there\\n\")\n",
    "\n",
    "print(\"processed \" + str(n) + \" rows\")\n",
    "\n",
    "writeRowDicToCsv(rowsDicToAddTo, '/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try4/trainSet_multiClass_bucket_fineRock_try4_manuallyCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete images from csv\n",
    "imIdsToDelete = os.listdir('/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/examplesToRemove/')\n",
    "\n",
    "csvToWorkWith = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/firstTry_final.csv'\n",
    "\n",
    "\n",
    "existingRows = readCsvRows(csvToWorkWith)\n",
    "\n",
    "n1 = 0\n",
    "for row in existingRows:\n",
    "    vals = row.split(',')\n",
    "\n",
    "    if vals[0] not in imIdsToDelete:\n",
    "        existingRows.remove(row)\n",
    "        n1 += 1\n",
    "        \n",
    "print(\"in the first run deleted \" + str(n1) +' rows\\n')\n",
    "\n",
    "n2 = 0\n",
    "for row in existingRows:\n",
    "    vals = row.split(',')\n",
    "\n",
    "    if vals[0] not in imIdsToDelete:\n",
    "        existingRows.remove(row)\n",
    "        n2 += 1\n",
    "        \n",
    "print(\"in the second run deleted \" + str(n2) +' rows\\n')\n",
    "print(\"deleted \" + str(n1+n2) + \" rows in total\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# open the file\n",
    "csv_file = open(csvToWorkWith, \"w\") \n",
    "\n",
    "# define column names\n",
    "columnTitles = \"filename,pathname,xmins,xmax,ymins,ymax,class\\n\"\n",
    "csv_file.write(columnTitles)\n",
    "\n",
    "# write rows\n",
    "for r in existingRows:\n",
    "    row = r + '\\n'\n",
    "    csv_file.write(row)\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"wrote \" + str(len(existingRows)) + \" rows to csv file\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add no bucket rows to existing csv and shuffle its rows\n",
    "\n",
    "csvRows = readCsvRows('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try5/trainSet_multiClass_bucket_fineRock_try5_manuallyCleaned.csv')\n",
    "\n",
    "#getRid of the empty row at the end\n",
    "csvRows = csvRows[0:len(csvRows)-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add the no bucket rows for the images in dir\n",
    "for imId in os.listdir('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try6/noShovelImagesToAdd/'):\n",
    "    newRow = str(str(imId) + ',' + str(imagesPath) + str(imId) + ',' + '' + ',' + '' + ',' + '' + ',' + '' + ',' + '')\n",
    "    print(newRow)\n",
    "    \n",
    "    csvRows.append(newRow)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# shuffle the rows\n",
    "from random import shuffle\n",
    "shuffle(csvRows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#write the shuffled rows to csv\n",
    "csv_file = open('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try5/trainSet_multiClass_bucket_fineRock_try5_manuallyCleaned_new.csv', \"w\") \n",
    "\n",
    "\n",
    "# write rows\n",
    "for row in csvRows:\n",
    "    csv_file.write(row + '\\n')\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"wrote \" + str(len(csvRows)) + \" rows to csv file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove matInsideBoundary rows from CSV\n",
    "\n",
    "rowsDic = getCertainClassRowsDictFromCsv('/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/trainSet_bucketAndMatInsideBoundaries_allImages_BucAndPnH_cleaned.csv', ['bucket'])\n",
    "\n",
    "writeRowDicToCsv(rowsDic, '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/trainSet_justBucketBoundaries_allImages_BucAndPnH_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "## Converting existing network CSVs to out csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The CSVs produced by the existing networks provide the topLeft cornor's x value, yvalu, a width, and a height. This must be converted to the format used by us which provides top left and bottom right cornors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "existNetRows = readCsvRows(\"/home/hooman/resultsFromExistingNetwork/testSetFregmentationResults/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "existNetRowsDict = {}\n",
    "for row in existNetRows[0:60]:\n",
    "    vals = row.split(',')\n",
    "    existNetRowsDict[vals[0]] = vals[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt_rows = readCsvRows(\"/home/hooman/dataPreparation/testSet_noBbxImagesIncluded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#write rows\n",
    "rows = []\n",
    "for row in gt_rows[1:len(gt_rows)-1]:\n",
    "    vals = row.split(',')\n",
    "    fileName = vals[0]\n",
    "    filePath = imagesPath + fileName\n",
    "    print(\"processing file:\\n\", filePath)\n",
    "    \n",
    "    if fileName in existNetRowsDict:\n",
    "        topx, topy, width, height = existNetRowsDict[fileName]\n",
    "\n",
    "        (topx, topy, width, height) = (int(round(float(topx))), int(round(float(topy))), int(round(float(width))), int(round(float(height)))) \n",
    "\n",
    "        xmin = topx\n",
    "        xmax = topx + width\n",
    "        ymin = topy\n",
    "        ymax = topy + height\n",
    "        \n",
    "        row = fileName + \",\" + filePath + \",\" + str(xmin) +\",\"+ str(xmax) + \",\" + str(ymin) + \",\" + str(ymax) + \",\" + \"bucket\" + \"\\n\"\n",
    "        \n",
    "    else:\n",
    "        row = fileName + \",\" + filePath + \",\" + \"\" +\",\"+ \"\" + \",\" + \"\" + \",\" + \"\" + \",\" + \"bucket\" + \"\\n\"\n",
    "        print(\"Found no bucket boundary\\n\")\n",
    "        \n",
    "    rows.append(row)\n",
    "    \n",
    "print(\"processes \"+ str(len(rows)) + \" rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "writeRowsToCsv(rows, \"/home/hooman/resultsFromExistingNetwork/testSetFregmentationResults/testSetOutputConverted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Converting .pdn files to .png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#converting .pdn (dot net) to pnd\n",
    "\n",
    "import pypdn\n",
    "\n",
    "pdnsDir = \"/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/pdnLabels/\"\n",
    "finalDir = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/regen/'\n",
    "\n",
    "#fileName = '1_20161115-222501_0001n0_16697.pdn'\n",
    "for fileName in os.listdir(pdnsDir):\n",
    "\n",
    "    #read the layer info\n",
    "    layeredImage = pypdn.read(pdnsDir + fileName)\n",
    "    #print(layeredImage)\n",
    "\n",
    "\n",
    "    #make the background invisible\n",
    "    layer = layeredImage.layers[0]\n",
    "    layer.visible = False\n",
    "\n",
    "\n",
    "    #Combine the layers into a numpy image\n",
    "    flatImage = layeredImage.flatten(asByte=True)\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.imshow(flatImage)\n",
    "    #plt.show()\n",
    "\n",
    "    newFileName = fileName.replace('.pdn', '.png')\n",
    "    cv2.imwrite(finalDir + newFileName, flatImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## working with h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import skimage\n",
    "#from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "#from skimage import img_as_ubyte\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "#from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read an H5 file\n",
    "filename = 'C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\oldOutputOfH5maker\\\\60x150_Synt_False_Norm_True_Enhance_False_HistEqu_False_HistNorm_False\\\\Test.h5'\n",
    "file = h5py.File(filename, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Reading and desplaying one image\n",
    "imgnb = 30\n",
    "\n",
    "for imgnb in range(23584):\n",
    "    image10 = list(file['data'][imgnb])\n",
    "    height10 = list(file['height'][imgnb])\n",
    "    label10 = list(file['label'][imgnb])\n",
    "    mask10 = list(file['mask'][imgnb])\n",
    "\n",
    "    img = cv2.cvtColor(image10[0], cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    print(len(img))\n",
    "    print(len(img[0]))\n",
    "\n",
    "    print(height10)\n",
    "    print(label10)\n",
    "    print(mask10)\n",
    "\n",
    "\n",
    "    cv2.circle(img,(30, int((1-height10[0])*150)), 2, (0,255,0), -1)\n",
    "    cv2.circle(img,(30, int((1-height10[1])*150)), 2, (0,0,255), -1)\n",
    "    cv2.circle(img,(30, int((1-height10[2])*150)), 2, (255,0,0), -1)\n",
    "    cv2.circle(img,(30, int((1-height10[3])*150)), 2, (255,255,0), -1)\n",
    "\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Save an array of images into an h5 file\n",
    "outputH5Path = \"N:\\\\randd\\\\MachineLearning\\\\Projects\\\\ShovelMetrics\\\\Optical\\\\Dataset\\\\BucketPattern\\\\nLS_163_Backhoe\\\\WM_FM_TM_EMDUBUO_CACA\\\\h5\\\\Telfer_test_h5\\\\h5\\\\TestB.h5\"\n",
    "with h5py.File(outputH5Path, \"w\") as outputFile:\n",
    "    images = outputFile.create_dataset('data', (108,1,222,254), dtype='f')    \n",
    "    outputFile.flush()\n",
    "    \n",
    "    i = 0\n",
    "    for fImg in imgPyAr:\n",
    "        images[i, :, :, :]  = np.expand_dims(fImg[:,:, 0], axis=0)\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Writing just 100 of images to h5 file\n",
    "\n",
    "inputFile = h5py.File('C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\h5MakerOutput_10-0\\\\60x150_Synt_False_Norm_True_Enhance_False_HistEqu_False_HistNorm_False\\\\Test.h5', 'r')\n",
    "\n",
    "with h5py.File('C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\oldOutputOfH5maker\\\\reduced.h5', \"w\") as outputFile:\n",
    "    images = outputFile.create_dataset('data', (804828,1,150,60), dtype='f')\n",
    "    heights = outputFile.create_dataset('height', (804828,4), dtype='f')\n",
    "    labels = outputFile.create_dataset('label', (804828,4), dtype='f')\n",
    "    masks = outputFile.create_dataset('mask', (804828,4), dtype='f')\n",
    "    \n",
    "    outputFile.flush()\n",
    "    \n",
    "    i = 0\n",
    "    for imgnb in range(0,10000,50):\n",
    "        images[i]  = inputFile['data'][imgnb]\n",
    "        heights[i] = inputFile['height'][imgnb]\n",
    "        labels[i]  = inputFile['label'][imgnb]\n",
    "        masks[i]   = inputFile['mask'][imgnb]\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='red'>To combine multiple files into one use external links. h5 has the functionality that you can make external links to multiple files (no matter whre they are) inside one file. Your progrtams can then use that one file as if it has the entire data in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Legacy U-Net padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Identify top left and bottom right corners of an image with black margin\n",
    "\n",
    "\n",
    "for i in range(222) :\n",
    "    for j in range(254):\n",
    "        flag = False\n",
    "        if not img[i,j,:].all() == 0:\n",
    "            print(i)\n",
    "            print(j)\n",
    "            flag = True\n",
    "            break\n",
    "    if flag:\n",
    "        break\n",
    "        \n",
    "print(\"\\n\\n\\n\")        \n",
    "for i in reversed(range(222)) :\n",
    "    for j in reversed(range(254)):\n",
    "        flag = False\n",
    "        if not img[i,j,:].all() == 0:\n",
    "            print(i)\n",
    "            print(j)\n",
    "            flag = True\n",
    "            break\n",
    "    if flag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resize and pad a directory of images for unet\n",
    "\n",
    "input_dir = \"N:\\\\randd\\\\MachineLearning\\\\Projects\\\\ShovelMetrics\\\\Optical\\\\Dataset\\\\BucketPattern\\\\nLS_163_Backhoe\\\\WM_FM_TM_EMDUBUO_CACA\\\\h5\\\\Telfer_test_h5\\\\image\\\\\"\n",
    "imgPyAr = []\n",
    "\n",
    "for img_name in os.listdir(input_dir):\n",
    "    img = cv2.imread(input_dir + img_name)\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "\n",
    "    orig_h, orig_w, _ = img.shape\n",
    "    print('orig_h: ' + str(orig_h) + '\\norig_w: ' + str(orig_w))\n",
    "\n",
    "\n",
    "\n",
    "    resized_img = cv2.resize(img,(orig_w/4, orig_h/4))\n",
    "    #plt.imshow(resized_img)\n",
    "    #plt.show()\n",
    "\n",
    "    resized_h, resized_w, _ = resized_img.shape\n",
    "    print('resized_h: ' + str(resized_h) + '\\nresized_w: ' + str(resized_w))\n",
    "\n",
    "\n",
    "    final_h = 222\n",
    "    final_w = 254\n",
    "    margin_r = 49\n",
    "    margin_b = 57\n",
    "    final_img = np.zeros((final_h, final_w, 3),dtype = np.float32)\n",
    "    print(final_img.shape)\n",
    "    margin_l = final_w - margin_r - resized_w\n",
    "    margin_t = final_h - margin_b - resized_h\n",
    "    print('margin_l: ' + str(margin_l) + '\\nmargin_t: ' + str(margin_t))\n",
    "\n",
    "\n",
    "    final_img[margin_t:(final_h - margin_b), margin_l:(final_w - margin_r), :] = resized_img[:, :, :]/float(255)\n",
    "\n",
    "    #plt.imshow(final_img)\n",
    "    #plt.show()\n",
    "    imgPyAr.append(final_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Image Augmentation using imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get and resize train images and masks (same as in unet notebook except just one image)\n",
    "TRAIN_PATH = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameNetworkAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/finalTrainingSet_images_fullSize/'\n",
    "\n",
    "MASK_PATH  = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameNetworkAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/finalTrainingSet_masks_fullSize/'\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "train_ids = os.listdir(TRAIN_PATH)\n",
    "train_ids = train_ids[4:6]\n",
    "\n",
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "\n",
    "for n, fileName in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    img = imread(TRAIN_PATH + fileName)[:,:,:IMG_CHANNELS]\n",
    "    \n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[n] = img\n",
    "    \n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    \n",
    "    mask_ = imread(MASK_PATH + fileName )\n",
    "\n",
    "    #expand dim just converts the (h,w,) image to (h,w,1) look at above for experimentation with np.expand_dim\n",
    "    mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                  preserve_range=True), axis=-1)\n",
    "\n",
    "    #hs this in effect adds all the masks together into one mask of the same size.Since masks are bindary.\n",
    "    mask = np.maximum(mask, mask_)\n",
    "    Y_train[n] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check if the data we just loaded looks all right\n",
    "ix = 0\n",
    "imshow(X_train[ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(Y_train[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Generatign FM-FrameSelection Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generatign FM-FrameSelection Plots\n",
    "\n",
    "allFramesDir = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/frameSelection/try7_t40b10_0,150,200,125,1,outSideImageObjRemoved/allFrames_1004_ESP_S04_017/'\n",
    "\n",
    "bufferedFramesDir = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/frameSelection/try7_t40b10_0,150,200,125,1,outSideImageObjRemoved/bufferedFrames_1004_ESP_S04_017/'\n",
    "\n",
    "selectedFramesDir = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/frameSelection/try7_t40b10_0,150,200,125,1,outSideImageObjRemoved/selectedFrames_1004_ESP_S04_017/'\n",
    "\n",
    "vidName = '1004_ESP_S04_017_fmSelection.png'\n",
    "\n",
    "savePath = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/frameSelection/try7_t40b10_0,150,200,125,1,outSideImageObjRemoved/'\n",
    "\n",
    "allFrames = []\n",
    "for imName in os.listdir(allFramesDir):\n",
    "    allFrames.append(int(imName.split('_')[-1][:-4]))\n",
    "    \n",
    "allFrames.sort()\n",
    "\n",
    "\n",
    "\n",
    "bufferedFrames = []\n",
    "for imName in os.listdir(bufferedFramesDir):\n",
    "    bufferedFrames.append(int(imName.split('_')[-1][:-4]))\n",
    "    \n",
    "bufferedFrames.sort()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "selectedFrames = []\n",
    "for imName in os.listdir(selectedFramesDir):\n",
    "    selectedFrames.append(int(imName.split('_')[-1][:-4]))\n",
    "    \n",
    "selectedFrames.sort()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,5))\n",
    "ax = plt.axes()\n",
    "loc = plticker.MultipleLocator(base=1800.0)\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "ax.grid()\n",
    "\n",
    "plt.plot(allFrames, len(allFrames) * [0], label='received')\n",
    "plt.plot(bufferedFrames, len(bufferedFrames) * [0.2], 'bo', label='buffered')\n",
    "plt.plot(selectedFrames, len(selectedFrames) * [0.5], 'go', label='selected')\n",
    "plt.ylabel('buffered=Blue,  Selected=Green')\n",
    "plt.xlabel('frameNumber(1 minute tick)')\n",
    "\n",
    "\n",
    "plt.savefig(savePath + vidName)\n",
    "plt.legend('best')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "hsDic = {'received': len(allFrames), 'buffered':len(bufferedFrames), 'selected':len(selectedFrames)}\n",
    "plt.bar(hsDic.keys(), hsDic.values(), color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# library's example of mask augmentation\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "ia.seed(1)\n",
    "\n",
    "\n",
    "image = X_train[0]\n",
    "segmap = Y_train[0]\n",
    "segmap = ia.SegmentationMapOnImage(segmap, shape=image.shape, nb_classes=1+1)\n",
    "\n",
    "# Define our augmentation pipeline.\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Dropout([0.05, 0.2]),      # drop 5% or 20% of all pixels\n",
    "    iaa.Sharpen((0.0, 1.0)),       # sharpen the image\n",
    "    iaa.Affine(rotate=(-45, 45)),  # rotate by -45 to 45 degrees (affects heatmaps)\n",
    "    iaa.ElasticTransformation(alpha=50, sigma=5)  # apply water effect (affects heatmaps)\n",
    "], random_order=True)\n",
    "\n",
    "# Augment images and heatmaps.\n",
    "images_aug = []\n",
    "segmaps_aug = []\n",
    "for _ in range(5):\n",
    "    seq_det = seq.to_deterministic()\n",
    "    images_aug.append(seq_det.augment_image(image))\n",
    "    segmaps_aug.append(seq_det.augment_segmentation_maps([segmap])[0])\n",
    "\n",
    "# We want to generate an image of original input images and heatmaps before/after augmentation.\n",
    "# It is supposed to have five columns: (1) original image, (2) augmented image,\n",
    "# (3) augmented heatmap on top of augmented image, (4) augmented heatmap on its own in jet\n",
    "# color map, (5) augmented heatmap on its own in intensity colormap,\n",
    "# We now generate the cells of these columns.\n",
    "#\n",
    "# Note that we add a [0] after each heatmap draw command. That's because the heatmaps object\n",
    "# can contain many sub-heatmaps and hence we draw command returns a list of drawn sub-heatmaps.\n",
    "# We only used one sub-heatmap, so our lists always have one entry.\n",
    "cells = []\n",
    "for image_aug, segmap_aug in zip(images_aug, segmaps_aug):\n",
    "    cells.append(image)                                      # column 1\n",
    "    cells.append(segmap.draw_on_image(image))                # column 2\n",
    "    cells.append(image_aug)                                  # column 3\n",
    "    cells.append(segmap_aug.draw_on_image(image_aug))        # column 4\n",
    "    cells.append(segmap_aug.draw(size=image_aug.shape[:2]))  # column 5\n",
    "\n",
    "# Convert cells to grid image and save.\n",
    "grid_image = ia.draw_grid(cells, cols=5)\n",
    "imageio.imwrite(\"/home/hooman/sdc1Storage/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameNetworkAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/example_segmaps.jpg\", grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hs example of mask augmentation\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "ia.seed(1)\n",
    "\n",
    "\n",
    "image = X_train[1]\n",
    "segmap = Y_train[1]\n",
    "segmap = ia.SegmentationMapOnImage(segmap, shape=image.shape, nb_classes=1+1)\n",
    "\n",
    "\n",
    "\n",
    "X_train_final_augmented = []\n",
    "Y_train_final_augmented= []\n",
    "\n",
    "\n",
    "cropAug = iaa.Sequential([\n",
    "    iaa.CropAndPad(\n",
    "    percent=(-0.3, 0.3),\n",
    "    pad_mode=[\"edge\"]),\n",
    "], random_order=False)\n",
    "\n",
    "\n",
    "scaleAug = iaa.Sequential([\n",
    "    iaa.Scale((0.5, 1.0)),\n",
    "], random_order=False)\n",
    "\n",
    "\n",
    "rotateAug = iaa.Sometimes(0.5, iaa.Affine(rotate=(-10, 10)))\n",
    "\n",
    "dropOutAug = iaa.Sometimes(0.5, iaa.Dropout([0.05, 0.1]))\n",
    "\n",
    "pixelValAddAug = iaa.Sometimes(0.5, iaa.Add((-40,40)))\n",
    "\n",
    "sharpenAug = iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.05, 0.2)))\n",
    "\n",
    "contrastNormAug = iaa.Sometimes(0.2, iaa.ContrastNormalization((0.5, 1.5)))\n",
    "\n",
    "\n",
    "    \n",
    "# X translation\n",
    "augResIm = iaa.Affine(translate_px={\"x\":-5}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"x\":-5}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "augResIm = iaa.Affine(translate_px={\"x\":-10}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"x\":-10}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "   \n",
    "augResIm = iaa.Affine(translate_px={\"x\":-15}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"x\":-15}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "augResIm = iaa.Affine(translate_px={\"x\":5}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"x\":5}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "   \n",
    "    \n",
    "augResIm = iaa.Affine(translate_px={\"x\":10}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"x\":10}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "augResIm = iaa.Affine(translate_px={\"x\":15}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"x\":15}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "\n",
    "# Y translation\n",
    "augResIm = iaa.Affine(translate_px={\"y\":5}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"y\":5}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "augResIm = iaa.Affine(translate_px={\"y\":10}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"y\":10}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "augResIm = iaa.Affine(translate_px={\"y\":15}).augment_image(image)\n",
    "augResMask = iaa.Affine(translate_px={\"y\":15}).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "#horizantal flip\n",
    "augResIm = iaa.Fliplr(1).augment_image(image)\n",
    "augResMask = iaa.Fliplr(1).augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "    \n",
    "#Random crop\n",
    "cropAug_det = cropAug.to_deterministic()\n",
    "augResIm = cropAug_det.augment_image(image)\n",
    "augResMask = cropAug_det.augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "\n",
    "#Random rotate or elastically transform\n",
    "rotateAug_det = rotateAug.to_deterministic()\n",
    "augResIm = rotateAug_det.augment_image(image)\n",
    "augResMask = rotateAug_det.augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "#drop some pixels at random 50% of the time\n",
    "dropOutAug_det = dropOutAug.to_deterministic()\n",
    "augResIm = dropOutAug_det.augment_image(image)\n",
    "augResMask = dropOutAug_det.augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "# add rand nb between -40,40 to some pixels 50% of the time \n",
    "pixelValAddAug_det = pixelValAddAug.to_deterministic()\n",
    "augResIm = pixelValAddAug_det.augment_image(image)\n",
    "augResMask = pixelValAddAug_det.augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "#sharpen images 20% of the time\n",
    "sharpenAug_det = sharpenAug.to_deterministic()\n",
    "augResIm = sharpenAug_det.augment_image(image)\n",
    "augResMask = sharpenAug_det.augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "#contrastNorm images 20% of the time\n",
    "contrastNormAug_det = contrastNormAug.to_deterministic()\n",
    "augResIm = contrastNormAug_det.augment_image(image)\n",
    "augResMask = contrastNormAug_det.augment_segmentation_maps([segmap])[0]\n",
    "X_train_final_augmented.append(augResIm)\n",
    "segmaps_aug.append(augResMask.get_arr_int()[...,np.newaxis].astype(np.bool))\n",
    "\n",
    "\n",
    "cells = []\n",
    "for image_aug, segmap_aug in zip(X_train_final_augmented, segmaps_aug):\n",
    "    imshow(image_aug)\n",
    "    plt.show()\n",
    "    imshow(np.squeeze(segmap_aug))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hs list of all augmentations I've looked at\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Dropout([0.0, 0.2]),\n",
    "    iaa.Add((-40,40)),\n",
    "    iaa.AddElementwise((-10, 10)),\n",
    "    iaa.AdditiveGaussianNoise(scale=0.1*255),\n",
    "    iaa.GaussianBlur(sigma=1.0),\n",
    "    iaa.Sharpen(alpha=0.2),\n",
    "    iaa.ElasticTransformation(alpha=50, sigma=5),\n",
    "    iaa.Scale((0.5, 1.0)),\n",
    "    iaa.CropAndPad(\n",
    "    percent=(-0.3, 0.3),\n",
    "    pad_mode=[\"constant\"],\n",
    "    pad_cval=(0, 128)),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.ContrastNormalization((0.5, 1.5)),\n",
    "    iaa.Affine(rotate=(-10, 10)),  # rotate by -10 to 10 degrees (affects heatmaps)\n",
    "    iaa.Affine(translate_px={\"x\":-5}),\n",
    "    iaa.Affine(translate_px={\"x\":-10}),\n",
    "    iaa.Affine(translate_px={\"x\":-15}),\n",
    "    iaa.Affine(translate_px={\"x\":5}),\n",
    "    iaa.Affine(translate_px={\"x\":10}),\n",
    "    iaa.Affine(translate_px={\"x\":15}),\n",
    "    iaa.Affine(translate_px={\"y\":5}),\n",
    "    iaa.Affine(translate_px={\"y\":10}),\n",
    "    iaa.Affine(translate_px={\"y\":15}),\n",
    "], random_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Adding external links to concatenate files\n",
    "\n",
    "import h5py\n",
    "\n",
    "outFile =  h5py.File('C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\oldOutputOfH5maker\\\\combinedTrainingSetJustLinks.h5', \"a\")\n",
    "\n",
    "\n",
    "outFile['data'] = h5py.ExternalLink(\"C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\h5MakerOutput_10-0\\\\60x150_Synt_False_Norm_True_Enhance_False_HistEqu_False_HistNorm_False\\\\Train.h5\", \"/data\")\n",
    "outFile['height'] = h5py.ExternalLink(\"C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\h5MakerOutput_10-0\\\\60x150_Synt_False_Norm_True_Enhance_False_HistEqu_False_HistNorm_False\\\\Train.h5\", \"/height\")\n",
    "outFile['label'] = h5py.ExternalLink(\"C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\h5MakerOutput_10-0\\\\60x150_Synt_False_Norm_True_Enhance_False_HistEqu_False_HistNorm_False\\\\Train.h5\", \"/label\")\n",
    "outFile['mask'] = h5py.ExternalLink(\"C:\\\\Users\\\\hooman\\\\Desktop\\\\WM2.0Project\\\\h5MakerOutput_10-0\\\\60x150_Synt_False_Norm_True_Enhance_False_HistEqu_False_HistNorm_False\\\\Train.h5\", \"/mask\")\n",
    "\n",
    "\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='blue'/> Parking Occupancy demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def loadFrameNames(path2textPreds):\n",
    "    frameNamesDict = {}\n",
    "    for textId in os.listdir(path2textPreds):\n",
    "        frameNb = int(textId.split('_')[-1].replace('.txt',''))\n",
    "        frameNamesDict[frameNb] = textId\n",
    "    return frameNamesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addDistance2zone(det, zone):\n",
    "    dist = np.sqrt( (zone['xcent'] - det['xcent'])**2 + (zone['ycent'] - det['ycent'])**2 )\n",
    "    det['dist'] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def readPredClosest2zoneIntoDict(predLines, imgW, imgH, zone):\n",
    "    finalDet = {'xmin':0,'xmax':0,'ymin':0,'ymax':0,'xcent':0,\\\n",
    "                'ycent':0,'w':0,'h':0,'valid':0, 'validPos':0}\n",
    "    minDistanceSoFar = 100000000\n",
    "    for line in predLines:\n",
    "        labelList = line.split(\" \")\n",
    "        \n",
    "        classId = str(labelList[0])\n",
    "        if classId == '2': #if this is a car\n",
    "            xcent = float(labelList[2]) * imgW\n",
    "            ycent = float(labelList[3]) * imgH\n",
    "            w = float(labelList[4]) * imgW\n",
    "            h = float(labelList[5]) * imgH\n",
    "\n",
    "            xmin = int(xcent - w/2)\n",
    "            ymin = int(ycent - h/2)\n",
    "            xmax = int(xcent + w/2)\n",
    "            ymax = int(ycent + h/2)\n",
    "\n",
    "            thisDet = {\n",
    "                'xmin':xmin,\n",
    "                'xmax':xmax,\n",
    "                'ymin':ymin,\n",
    "                'ymax':ymax,\n",
    "                'xcent':xcent,\n",
    "                'ycent':ycent,\n",
    "                'w':w,\n",
    "                'h':h,\n",
    "            }\n",
    "\n",
    "            addDistance2zone(thisDet, zone)\n",
    "            if thisDet['dist'] < minDistanceSoFar:\n",
    "                minDistanceSoFar = thisDet['dist']\n",
    "                finalDet = thisDet\n",
    "                finalDet['valid'] = 1\n",
    "                finalDet['validPos'] = 0\n",
    "        \n",
    "    return finalDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isStationary(det, previousDet):\n",
    "#     maxAllowedMovement = 30\n",
    "#     dist = np.sqrt( (previousDet['xcent'] - det['xcent'])**2 + (previousDet['ycent'] - det['ycent'])**2 )\n",
    "#     return dist < maxAllowedMovement\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getMovingOutDist(det, previousDet):\n",
    "    dist = det['ycent'] - previousDet['ycent']\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def updateBuffer(buffer, det, previousDet, previousDecision, exitCounter, zone):\n",
    "    from statistics import mode\n",
    "    # returns (noDecision) (Empty) (Occupied) ('Error')\n",
    "    maxAllowedDist2pump = 150#125\n",
    "    bufferSize2makeDecision = 5#5\n",
    "    exitCounterThresh = 3\n",
    "    minMovement = 20#15\n",
    "    \n",
    "    if det['valid'] and det['dist'] < maxAllowedDist2pump:\n",
    "        det['validPos'] = True\n",
    "        \n",
    "    if det['validPos'] and previousDet['validPos'] and\\\n",
    "    getMovingOutDist(det, previousDet) > minMovement and\\\n",
    "    det['ycent'] > zone['ycent']:\n",
    "        print(getMovingOutDist(det, previousDet))\n",
    "        exitCounter = 0\n",
    "        decision2Return = 'EmptyMovement'\n",
    "        buffer.clear()\n",
    "    \n",
    "    else:\n",
    "        occupied = det['validPos'] and isStationary(det, previousDet)\n",
    "        buffer.append(occupied)\n",
    "\n",
    "    #     print('valid')\n",
    "    #     print(det['valid'])\n",
    "    #     if det['valid']:\n",
    "    #         print('dist')\n",
    "    #         print(det['dist'])\n",
    "    #         print('station')\n",
    "    #         print(isStationary(det, previousDet))\n",
    "    #         print('occ')\n",
    "    #         print(occupied)\n",
    "    #     print('\\n')\n",
    "\n",
    "        decision2Return = previousDecision\n",
    "        if len(buffer) >= bufferSize2makeDecision:\n",
    "            m = mode(buffer)\n",
    "            buffer.clear()\n",
    "\n",
    "    #         print('mode')\n",
    "    #         print(m)\n",
    "    #         print('\\n')\n",
    "\n",
    "            if m == 1:\n",
    "                exitCounter = 0\n",
    "                decision2Return = 'Occupied'\n",
    "\n",
    "            elif m == 0:\n",
    "                if exitCounter > exitCounterThresh:\n",
    "                    exitCounter = 0\n",
    "                    decision2Return = 'EmptyCounter'\n",
    "                else:\n",
    "                    exitCounter += 1\n",
    "            else:\n",
    "                print(\"Error in updateBuffer\")\n",
    "\n",
    "    return decision2Return, exitCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def writeResultOnImage(text, detectedBox, decBuff, img, exitCounter):\n",
    "    zone_xmin = int(450)\n",
    "    zone_xmax = int(750)\n",
    "    zone_ymin = int(350)\n",
    "    zone_ymax = int(750)\n",
    "    zone_xcent = int((zone_xmin + zone_xmax)/2)\n",
    "    zone_ycent = int((zone_ymin + zone_ymax)/2)\n",
    "    \n",
    "    cv2.rectangle(img,(zone_xmin, zone_ymin),(zone_xmax, zone_ymax),(255,0,0),1)\n",
    "    cv2.circle(img,(zone_xcent, zone_ycent),5, (255,0,0), -1)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    textlocX_status = 210\n",
    "    textlocY_status = 250\n",
    "    \n",
    "    cv2.putText(img,text,(textlocX_status,textlocY_status), font, 2,(0,255,0), 3, 0)\n",
    "    \n",
    "    cv2.putText(\n",
    "        img, str(decBuff),\n",
    "        (textlocX_status,textlocY_status-40),\n",
    "        font, 2,(0,255,0), 3, 0)\n",
    "    \n",
    "    cv2.putText(\n",
    "        img, str(exitCounter),\n",
    "        (textlocX_status,textlocY_status-70),\n",
    "        font, 2,(0,255,0), 3, 0)\n",
    "    \n",
    "    if detectedBox['valid'] and detectedBox['dist']:\n",
    "        det_xcent = int(detectedBox['xcent'])\n",
    "        det_ycent = int(detectedBox['ycent'])\n",
    "        \n",
    "        cv2.circle(img,(det_xcent, det_ycent),5, (255,255,0), -1)\n",
    "        \n",
    "        textlocX_dist = det_xcent -10\n",
    "        textlocY_dist = det_ycent - 30\n",
    "        cv2.putText(\n",
    "            img,str(int(detectedBox['dist'])),\n",
    "            (textlocX_dist,textlocY_dist), font, 2,(255,255,0), 3, 0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "27.693120000000135\n",
      "708\n",
      "106.15391999999986\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "61.53887999999995\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "29.231040000000007\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "33.84575999999993\n",
      "1067\n",
      "23.076479999999947\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "21.53855999999996\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "33.84575999999993\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "21.53855999999996\n",
      "1653\n",
      "35.384640000000104\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "66.15359999999998\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n"
     ]
    }
   ],
   "source": [
    "path2textPreds = '/home/hooman/gasstation_videos/02010004285000000/predsText_1fps/'\n",
    "path2imagePreds = '/home/hooman/gasstation_videos/02010004285000000/predsImage_1fps/'\n",
    "path2saveOverlays = '/home/hooman/gasstation_videos/02010004285000000/overlays_1fps/'\n",
    "\n",
    "zone = {\n",
    "    'xmin' : int(450),\n",
    "    'xmax' : int(750),\n",
    "    'ymin' : int(350),\n",
    "    'ymax' : int(750),\n",
    "}\n",
    "zone['xcent'] = int((zone['xmin'] + zone['xmax'])/2)\n",
    "zone['ycent'] = int((zone['ymin'] + zone['ymax'])/2) \n",
    "\n",
    "    \n",
    "buffer = []\n",
    "previousDet = {'xmin':0,'xmax':0,'ymin':0,'ymax':0,'xcent':0,\\\n",
    "               'ycent':0,'w':0,'h':0,'valid':0}\n",
    "previousDecision = 'noDecision'\n",
    "exitCounter = 0\n",
    "\n",
    "textPredNamesDict = loadFrameNames(path2textPreds)\n",
    "for frameNb in sorted(list(textPredNamesDict.keys())):\n",
    "    print(frameNb)\n",
    "    textId = textPredNamesDict[frameNb]\n",
    "    labelFile = open(path2textPreds + textId, 'r')\n",
    "    \n",
    "    imageId = textId.replace('.txt', '.jpg')\n",
    "    image = imread(path2imagePreds + imageId)\n",
    "    imH, imW, _ = image.shape\n",
    "    \n",
    "    det = readPredClosest2zoneIntoDict(labelFile.readlines(), imW, imH, zone)\n",
    "    \n",
    "    decision, exitCounter = updateBuffer(\n",
    "        buffer, det, previousDet,\n",
    "        previousDecision, exitCounter, zone\n",
    "    )\n",
    "    previousDet = det\n",
    "    previousDecision = decision\n",
    "    \n",
    "#     print(textId)\n",
    "    writeResultOnImage(decision, det, buffer, image, exitCounter)\n",
    "    imsave(path2saveOverlays + imageId, image)\n",
    "    \n",
    "# to create video use: \n",
    "# ~/gasstation_videos/03000002242000000$ ffmpeg -r 15 -i overlays_1fps/frame_%d.jpg -b 10000k -vcodec mpeg4 -y hsMovie15_10000k.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='blue'/> Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "ia.seed(1)\n",
    "\n",
    "#The repo is https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Example: visualizing augmentations of bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualize Augmented Images\n",
    "images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n",
    "seq = iaa.Sequential([iaa.Fliplr(0.5), iaa.GaussianBlur((0, 3.0))])\n",
    "\n",
    "# Show an image with 8*8 augmented versions of image 0 and 8*8 augmented\n",
    "# versions of image 1. Identical augmentations will be applied to\n",
    "# image 0 and 1.\n",
    "fig = seq.show_grid([images[0], images[1]], cols=8, rows=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# HS define dummy images and BBs, augment and display\n",
    "\n",
    "images = np.zeros((2, 128, 128, 3), dtype=np.uint8)  # two example images\n",
    "images[0, 50:64, 50:64, :] = 128\n",
    "images[1, 50:64, 50:64, :] = 255\n",
    "# print('im0')\n",
    "# f = ia.imshow(images[0])\n",
    "# print('im1')\n",
    "# f = ia.imshow(images[1])\n",
    "\n",
    "bbs = [\n",
    "    [ia.BoundingBox(x1=10.5, y1=15.5, x2=30.5, y2=50.5)],\n",
    "    [ia.BoundingBox(x1=10.5, y1=20.5, x2=50.5, y2=50.5),\n",
    "     ia.BoundingBox(x1=40.5, y1=75.5, x2=70.5, y2=100.5)]\n",
    "]\n",
    "\n",
    "image_with_bbs1 = bbs[0][0].draw_on_image(images[0], color=(0, 255, 0), size=3)\n",
    "ff = ia.imshow(image_with_bbs1)\n",
    "\n",
    "image_with_bbs2 = bbs[1][0].draw_on_image(images[1], color=(0, 255, 0), size=3)\n",
    "image_with_bbs2 = bbs[1][1].draw_on_image(image_with_bbs2, color=(0,255, 0), size=3)\n",
    "ff = ia.imshow(image_with_bbs2)\n",
    "\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Rot90(1)\n",
    "])\n",
    "images_aug, bbs_aug = seq(images=images, bounding_boxes=bbs)\n",
    "\n",
    "\n",
    "augimage_with_bbs1 = bbs_aug[0][0].draw_on_image(images_aug[0], color=(255, 0, 0), size=3)\n",
    "ff = ia.imshow(augimage_with_bbs1)\n",
    "\n",
    "augimage_with_bbs2 = bbs_aug[1][0].draw_on_image(images_aug[1], color=(255, 0, 0), size=3)\n",
    "augimage_with_bbs2 = bbs_aug[1][1].draw_on_image(augimage_with_bbs2, color=(255, 0, 0), size=3)\n",
    "ff = ia.imshow(augimage_with_bbs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# HS case for a single image\n",
    "images = np.zeros((2, 128, 128, 3), dtype=np.uint8)  # two example images\n",
    "images[0,50:64, 50:64, :] = 255\n",
    "\n",
    "\n",
    "bbs = [\n",
    "    [ia.BoundingBox(x1=10.5, y1=20.5, x2=50.5, y2=50.5),\n",
    "     ia.BoundingBox(x1=40.5, y1=75.5, x2=70.5, y2=100.5)],\n",
    "    [],\n",
    "]\n",
    "\n",
    "image_with_bbs2 = bbs[0][0].draw_on_image(images[0], color=(0, 255, 0), size=3)\n",
    "image_with_bbs2 = bbs[0][1].draw_on_image(image_with_bbs2, color=(0,255, 0), size=3)\n",
    "ff = ia.imshow(image_with_bbs2)\n",
    "\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Rot90(1)\n",
    "])\n",
    "images_aug, bbs_aug = seq(images=images, bounding_boxes=bbs)\n",
    "\n",
    "augimage_with_bbs2 = bbs_aug[0][0].draw_on_image(images_aug[0], color=(255, 0, 0), size=3)\n",
    "augimage_with_bbs2 = bbs_aug[0][1].draw_on_image(augimage_with_bbs2, color=(255, 0, 0), size=3)\n",
    "ff = ia.imshow(augimage_with_bbs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Examples: hsUnet (segmentation map labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define augmentations(some need defining rest are done inline when we run the aug)\n",
    "#The repo is https://github.com/aleju/imgaug\n",
    "cropAug = iaa.Sequential([\n",
    "    iaa.CropAndPad(\n",
    "    percent=(-0.3, 0.3),\n",
    "    pad_mode=[\"edge\"]),\n",
    "], random_order=False)\n",
    "\n",
    "\n",
    "scaleAug = iaa.Sequential([\n",
    "    iaa.Scale((0.5, 1.0)),\n",
    "], random_order=False)\n",
    "\n",
    "\n",
    "rotateAug = iaa.Sometimes(0.5, iaa.Affine(rotate=(-10, 10)))\n",
    "\n",
    "dropOutAug = iaa.Sometimes(0.5, iaa.Dropout([0.05, 0.1]))\n",
    "\n",
    "pixelValAddAug = iaa.Sometimes(0.5, iaa.Add((-40,40)))\n",
    "\n",
    "sharpenAug = iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.05, 0.2)))\n",
    "\n",
    "contrastNormAug = iaa.Sometimes(0.2, iaa.ContrastNormalization((0.5, 1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Perform augmentation with segmentation maps\n",
    "for i, _ in tqdm(enumerate(train_ids_fullSize), total=len(train_ids_fullSize)):\n",
    "    #get current image and segmap and add them to the set\n",
    "    image = X_train_fullSize[i]\n",
    "    segmap = Y_train_fullSize[i]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, image[np.newaxis,...]), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, segmap[np.newaxis,...]), axis=0)\n",
    "\n",
    "    #convert to imaug's segmap class\n",
    "    segmap = ia.SegmentationMapOnImage(segmap, shape=image.shape, nb_classes=1+1)\n",
    "\n",
    "\n",
    "    # X translation\n",
    "    augResIm = iaa.Affine(translate_px={\"x\":-5}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"x\":-5}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"x\":-10}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"x\":-10}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"x\":-15}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"x\":-15}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"x\":5}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"x\":5}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"x\":10}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"x\":10}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"x\":15}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"x\":15}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Y translation\n",
    "    augResIm = iaa.Affine(translate_px={\"y\":5}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"y\":5}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"y\":10}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"y\":10}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "    augResIm = iaa.Affine(translate_px={\"y\":15}).augment_image(image)\n",
    "    augResMask = iaa.Affine(translate_px={\"y\":15}).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "    #horizantal flip\n",
    "    augResIm = iaa.Fliplr(1).augment_image(image)\n",
    "    augResMask = iaa.Fliplr(1).augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    #Random crop\n",
    "    cropAug_det = cropAug.to_deterministic()\n",
    "    augResIm = cropAug_det.augment_image(image)\n",
    "    augResMask = cropAug_det.augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    #Random rotate or elastically transform\n",
    "    rotateAug_det = rotateAug.to_deterministic()\n",
    "    augResIm = rotateAug_det.augment_image(image)\n",
    "    augResMask = rotateAug_det.augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "    #drop some pixels at random 50% of the time\n",
    "    dropOutAug_det = dropOutAug.to_deterministic()\n",
    "    augResIm = dropOutAug_det.augment_image(image)\n",
    "    augResMask = dropOutAug_det.augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    # add rand nb between -40,40 to some pixels 50% of the time \n",
    "    pixelValAddAug_det = pixelValAddAug.to_deterministic()\n",
    "    augResIm = pixelValAddAug_det.augment_image(image)\n",
    "    augResMask = pixelValAddAug_det.augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    #sharpen images 20% of the time\n",
    "    sharpenAug_det = sharpenAug.to_deterministic()\n",
    "    augResIm = sharpenAug_det.augment_image(image)\n",
    "    augResMask = sharpenAug_det.augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "\n",
    "    #contrastNorm images 20% of the time\n",
    "    contrastNormAug_det = contrastNormAug.to_deterministic()\n",
    "    augResIm = contrastNormAug_det.augment_image(image)\n",
    "    augResMask = contrastNormAug_det.augment_segmentation_maps([segmap])[0]\n",
    "    X_train_final_augmented = np.concatenate( (X_train_final_augmented, augResIm[np.newaxis,...].astype(np.uint8)), axis=0)\n",
    "    Y_train_final_augmented = np.concatenate( (Y_train_final_augmented, augResMask.get_arr_int()[np.newaxis,...,np.newaxis].astype(np.bool)), axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "# free the memory because these are already in final augmented set\n",
    "del X_train_fullSize\n",
    "del Y_train_fullSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Augs for headAndTorso Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getAugmentationBb(labelList, IMG_WIDTH, IMG_HEIGHT):\n",
    "    classId = str(labelList[0])\n",
    "    xcent = float(labelList[1]) * IMG_WIDTH\n",
    "    ycent = float(labelList[2]) * IMG_HEIGHT\n",
    "    w = float(labelList[3]) * IMG_WIDTH\n",
    "    h = float(labelList[4]) * IMG_HEIGHT\n",
    "    \n",
    "    xmin = int(xcent - w/2)\n",
    "    ymin = int(ycent - h/2)\n",
    "    xmax = int(xcent + w/2)\n",
    "    ymax = int(ycent + h/2)\n",
    "    \n",
    "#     return ia.BoundingBox(x1=xcent, y1=ycent, x2=w, y2=h) # this does not work\n",
    "    return (ia.BoundingBox(x1=xmin, y1=ymin, x2=xmax, y2=ymax), classId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualizeAugmentedBb(augBb, image, classId):\n",
    "    xmin = int(augBb.x1)\n",
    "    ymin = int(augBb.y1)\n",
    "    xmax = int(augBb.x2)\n",
    "    ymax = int(augBb.y2)\n",
    "    \n",
    "    if classId == '0':\n",
    "        cv2.rectangle(image,(xmin, ymin),(xmax, ymax),(255,0,0),1)\n",
    "    elif classId == '1':\n",
    "        cv2.rectangle(image,(xmin, ymin),(xmax, ymax),(0,255,0),1)\n",
    "    elif classId == '2':\n",
    "        cv2.rectangle(image,(xmin, ymin),(xmax, ymax),(0,0,255),1)\n",
    "    else:\n",
    "        cv2.rectangle(image,(xmin, ymin),(xmax, ymax),(255,255,0),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def writeAugmentedLabelFile(labelId, path2StoreLabels, augmentedBbs, bbClassIds, imageW, imageH):\n",
    "    outFile = open(path2StoreLabels + labelId, 'w')\n",
    "    for i in range(len(bbClassIds)):\n",
    "        classId = bbClassIds[i]\n",
    "        xmin = float(augmentedBbs[i].x1) / imageW\n",
    "        ymin = float(augmentedBbs[i].y1) / imageH\n",
    "        xmax = float(augmentedBbs[i].x2) / imageW\n",
    "        ymax = float(augmentedBbs[i].y2) / imageH\n",
    "        \n",
    "        h = (ymax - ymin)\n",
    "        w = (xmax - xmin)\n",
    "        xcent = xmin + w/2\n",
    "        ycent = ymin + h/2\n",
    "        \n",
    "        strL = str(classId) + ' ' + str(xcent) + ' ' + str(ycent) + ' ' +\\\n",
    "        str(w) + ' ' + str(h) + '\\n' \n",
    "        outFile.write(strL)\n",
    "    \n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load single image/bb augment and visualize (for debug)\n",
    "path2images = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_1200/'\n",
    "path2labels  = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_hs_1200/'\n",
    "path2AugImages = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "path2AugLabels = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_aug/'\n",
    "\n",
    "image_ids = os.listdir(path2images)\n",
    "\n",
    "#for n, imageName in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
    "imageName = image_ids[0]\n",
    "#load the image\n",
    "img = imread(path2images + imageName)\n",
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = img.shape\n",
    "imageAr = np.zeros((2, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "imageAr[0,:,:,:] = img\n",
    "\n",
    "#load the label bbs\n",
    "bbs = []\n",
    "classIds = []\n",
    "labelFile = open(path2labels + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "labelLines = labelFile.readlines()\n",
    "for line in labelLines:\n",
    "    labelBB, classId = getAugmentationBb(line.split(\" \"), IMG_WIDTH, IMG_HEIGHT)\n",
    "    classIds.append(classId)\n",
    "    bbs.append(labelBB)\n",
    "bbsAr = [bbs]\n",
    "bbsAr.append([])\n",
    "\n",
    "#augment\n",
    "augPrefix2Add2Names = 'rot90_'\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Rot90(1)\n",
    "])\n",
    "images_aug, bbs_aug = seq(images=imageAr, bounding_boxes=bbsAr)\n",
    "\n",
    "# store the augmented labelFile\n",
    "writeAugmentedLabelFile(\n",
    "    augPrefix2Add2Names + imageName.replace(\".jpg\",\".txt\"),\n",
    "    path2AugLabels, bbs_aug[0], classIds, IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# visualizes the aug bbs method2\n",
    "for i in range(len(classIds)):\n",
    "    visualizeAugmentedBb(bbs_aug[0][i], images_aug[0], classIds[i])\n",
    "imshow(images_aug[0])\n",
    "plt.show()\n",
    "\n",
    "imsave(path2AugImages + augPrefix2Add2Names + imageName, images_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# augment rot90\n",
    "path2images = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_1200/'\n",
    "path2labels  = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_hs_1200/'\n",
    "path2AugImages = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "path2AugLabels = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_aug/'\n",
    "\n",
    "image_ids = os.listdir(path2images)\n",
    "\n",
    "for n, imageName in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
    "    #load the image\n",
    "    img = imread(path2images + imageName)\n",
    "    IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = img.shape\n",
    "    imageAr = np.zeros((2, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    imageAr[0,:,:,:] = img\n",
    "\n",
    "    #load the label bbs\n",
    "    bbs = []\n",
    "    classIds = []\n",
    "    labelFile = open(path2labels + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        labelBB, classId = getAugmentationBb(line.split(\" \"), IMG_WIDTH, IMG_HEIGHT)\n",
    "        classIds.append(classId)\n",
    "        bbs.append(labelBB)\n",
    "    bbsAr = [bbs]\n",
    "    bbsAr.append([])\n",
    "\n",
    "    #augment\n",
    "    augPrefix2Add2Names = 'rot90_'\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Rot90(1)\n",
    "    ])\n",
    "    images_aug, bbs_aug = seq(images=imageAr, bounding_boxes=bbsAr)\n",
    "\n",
    "    # store the augmented labelFile\n",
    "    writeAugmentedLabelFile(\n",
    "        augPrefix2Add2Names + imageName.replace(\".jpg\",\".txt\"),\n",
    "        path2AugLabels, bbs_aug[0], classIds, IMG_WIDTH, IMG_HEIGHT)\n",
    "    \n",
    "    # store the augmented image\n",
    "    imsave(path2AugImages + augPrefix2Add2Names + imageName, images_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# augment rot270\n",
    "path2images = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_1200/'\n",
    "path2labels  = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_hs_1200/'\n",
    "path2AugImages = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "path2AugLabels = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_aug/'\n",
    "augPrefix2Add2Names = 'rot270_'\n",
    "\n",
    "image_ids = os.listdir(path2images)\n",
    "\n",
    "for n, imageName in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
    "    #load the image\n",
    "    img = imread(path2images + imageName)\n",
    "    IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = img.shape\n",
    "    imageAr = np.zeros((2, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    imageAr[0,:,:,:] = img\n",
    "\n",
    "    #load the label bbs\n",
    "    bbs = []\n",
    "    classIds = []\n",
    "    labelFile = open(path2labels + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        labelBB, classId = getAugmentationBb(line.split(\" \"), IMG_WIDTH, IMG_HEIGHT)\n",
    "        classIds.append(classId)\n",
    "        bbs.append(labelBB)\n",
    "    bbsAr = [bbs]\n",
    "    bbsAr.append([])\n",
    "\n",
    "    #augment\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Rot90(3)\n",
    "    ])\n",
    "    images_aug, bbs_aug = seq(images=imageAr, bounding_boxes=bbsAr)\n",
    "\n",
    "    # store the augmented labelFile\n",
    "    writeAugmentedLabelFile(\n",
    "        augPrefix2Add2Names + imageName.replace(\".jpg\",\".txt\"),\n",
    "        path2AugLabels, bbs_aug[0], classIds, IMG_WIDTH, IMG_HEIGHT)\n",
    "    \n",
    "    # store the augmented image\n",
    "    imsave(path2AugImages + augPrefix2Add2Names + imageName, images_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# augment imagesOnly GrayScale\n",
    "path2images = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_1200/'\n",
    "path2labels  = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_hs_1200/'\n",
    "path2AugImages = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "\n",
    "image_ids = os.listdir(path2images)\n",
    "\n",
    "for n, imageName in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
    "    #load the image\n",
    "    img = imread(path2images + imageName)\n",
    "    IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = img.shape\n",
    "    imageAr = np.zeros((2, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    imageAr[0,:,:,:] = img\n",
    "\n",
    "    #load the label bbs\n",
    "    bbs = []\n",
    "    classIds = []\n",
    "    labelFile = open(path2labels + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        labelBB, classId = getAugmentationBb(line.split(\" \"), IMG_WIDTH, IMG_HEIGHT)\n",
    "        classIds.append(classId)\n",
    "        bbs.append(labelBB)\n",
    "    bbsAr = [bbs]\n",
    "    bbsAr.append([])\n",
    "\n",
    "    #augment\n",
    "    augPrefix2Add2Names = ''\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Grayscale(alpha=(1.0, 1.0)),\n",
    "    ])\n",
    "    images_aug, bbs_aug = seq(images=imageAr, bounding_boxes=bbsAr)\n",
    "\n",
    "    # store the augmented image\n",
    "    imsave(path2AugImages + augPrefix2Add2Names + imageName, images_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# augment imagesOnly contrastNorm\n",
    "path2images = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_1200/'\n",
    "path2labels  = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_hs_1200/'\n",
    "path2AugImages = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "\n",
    "image_ids = os.listdir(path2images)\n",
    "\n",
    "for n, imageName in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
    "    #load the image\n",
    "    img = imread(path2images + imageName)\n",
    "    IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = img.shape\n",
    "    imageAr = np.zeros((2, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    imageAr[0,:,:,:] = img\n",
    "\n",
    "    #load the label bbs\n",
    "    bbs = []\n",
    "    classIds = []\n",
    "    labelFile = open(path2labels + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        labelBB, classId = getAugmentationBb(line.split(\" \"), IMG_WIDTH, IMG_HEIGHT)\n",
    "        classIds.append(classId)\n",
    "        bbs.append(labelBB)\n",
    "    bbsAr = [bbs]\n",
    "    bbsAr.append([])\n",
    "\n",
    "    #augment\n",
    "    augPrefix2Add2Names = ''\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.ContrastNormalization((0.2, 0.8)),\n",
    "    ])\n",
    "    images_aug, bbs_aug = seq(images=imageAr, bounding_boxes=bbsAr)\n",
    "\n",
    "    # store the augmented image\n",
    "    imsave(path2AugImages + augPrefix2Add2Names + imageName, images_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# augment imagesOnly Fog\n",
    "path2images = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_1200/'\n",
    "path2labels  = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_hs_1200/'\n",
    "path2AugImages = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "\n",
    "image_ids = os.listdir(path2images)\n",
    "\n",
    "for n, imageName in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
    "    #load the image\n",
    "    img = imread(path2images + imageName)\n",
    "    IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = img.shape\n",
    "    imageAr = np.zeros((2, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    imageAr[0,:,:,:] = img\n",
    "\n",
    "    #load the label bbs\n",
    "    bbs = []\n",
    "    classIds = []\n",
    "    labelFile = open(path2labels + imageName.replace(\".jpg\",\".txt\"), 'r')\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        labelBB, classId = getAugmentationBb(line.split(\" \"), IMG_WIDTH, IMG_HEIGHT)\n",
    "        classIds.append(classId)\n",
    "        bbs.append(labelBB)\n",
    "    bbsAr = [bbs]\n",
    "    bbsAr.append([])\n",
    "\n",
    "    #augment\n",
    "    augPrefix2Add2Names = 'rot90_'\n",
    "#     seq = iaa.Sequential([\n",
    "#         iaa.Sharpen(alpha=(0.05, 0.2)),\n",
    "#         iaa.ContrastNormalization((0.5, 1.5)),\n",
    "#         iaa.Grayscale(alpha=(0.05, 0.2)),\n",
    "#     ])\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Fog(),\n",
    "    ])\n",
    "    images_aug, bbs_aug = seq(images=imageAr, bounding_boxes=bbsAr)\n",
    "\n",
    "    # store the augmented image\n",
    "    imsave(path2AugImages + augPrefix2Add2Names + imageName, images_aug[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='blue'/> FMDL Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Method Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sortLabelsDic(labelsDic, imgLabel, img=[]):\n",
    "\n",
    " #get the boundary for each label\n",
    "    sortedLabelColsDic = {}\n",
    "    sortedLabelRowsDic = {}\n",
    "    for k in labelsDic.keys():\n",
    "        if len(labelsDic[k][0]) > 0:\n",
    "            sortedLabelColsDic[k+'Cols'] = np.sort(labelsDic[k][0]),\n",
    "            sortedLabelRowsDic[k+'Rows'] = np.sort(labelsDic[k][1]),\n",
    "\n",
    "            \n",
    "    '''\n",
    "    if len(img) >0 :\n",
    "        print(sortedLabelColsDic)\n",
    "        print(\"\\n\")\n",
    "        print(sortedLabelRowsDic)\n",
    "        print(\"\\n\")\n",
    "    ''' \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ############## Bucket Boundary ###############\n",
    "    ##############################################\n",
    "    #Get the min and max for rows and columns to caluclate bucket boundaries\n",
    "    lowColVals = []\n",
    "    highColVals = []\n",
    "    \n",
    "    lowColValsTeeth = []\n",
    "    highColValsTeeth = []\n",
    "    for itemKey in sortedLabelColsDic.keys():\n",
    "        if itemKey == 'teethCols':\n",
    "            labV = sortedLabelColsDic[itemKey]\n",
    "            for v in labV:\n",
    "                lowColValsTeeth.append(v[0])\n",
    "                highColValsTeeth.append(v[len(v)-1])\n",
    "        \n",
    "        labV = sortedLabelColsDic[itemKey]\n",
    "        for v in labV:\n",
    "            lowColVals.append(v[0])\n",
    "            highColVals.append(v[len(v)-1])\n",
    "\n",
    "\n",
    "    lowRowVals = []\n",
    "    highRowVals = []\n",
    "    \n",
    "    lowRowValsTeeth = []\n",
    "    highRowValsTeeth = []\n",
    "    for itemKey in sortedLabelRowsDic.keys():\n",
    "        if itemKey == 'teethRows':\n",
    "            labV = sortedLabelRowsDic[itemKey]\n",
    "            for v in labV:\n",
    "                lowRowValsTeeth.append(v[0])\n",
    "                highRowValsTeeth.append(v[len(v)-1])\n",
    "\n",
    "        labV = sortedLabelRowsDic[itemKey]\n",
    "        for v in labV:\n",
    "            lowRowVals.append(v[0])\n",
    "            highRowVals.append(v[len(v)-1])\n",
    "\n",
    "    \n",
    "\n",
    "    #get the bucket boundary\n",
    "    xmins = np.sort(np.array(lowColVals))\n",
    "    xmaxs = np.sort(np.array(highColVals))\n",
    "    ymins = np.sort(np.array(lowRowVals))\n",
    "    ymaxs = np.sort(np.array(highRowVals))\n",
    "    \n",
    "    yminsTeeth = np.sort(np.array(lowRowValsTeeth))\n",
    "    ymaxsTeeth = np.sort(np.array(highRowValsTeeth))\n",
    "    \n",
    "    '''\n",
    "    print('lowColValsTeeth:')\n",
    "    print(lowColValsTeeth)\n",
    "    print('highColValsTeeth:')\n",
    "    print(highColValsTeeth)\n",
    "    \n",
    "    print('imageShape')\n",
    "    print(imgLabel.shape)\n",
    "    '''\n",
    "    \n",
    "    return xmins, xmaxs, ymins, ymaxs, yminsTeeth, ymaxsTeeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBucketBoundariesTooth2ToothV2(imgLabel, img=[]):\n",
    "\n",
    "    boundariesDict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ########## Reading Labeled Pixels ############\n",
    "    ##############################################\n",
    "    labelsDic = {\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        \n",
    "        #for Cable\n",
    "        'sheave'     :np.where(np.all(imgLabel == sheave, axis=-1)),\n",
    "        'teeth'     :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'fmInapp'     :np.where(np.all(imgLabel == fmInapp, axis=-1)),\n",
    "        'fineInside'     :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'emptyInside'     :np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'     :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'dust'     :np.where(np.all(imgLabel == dust, axis=-1)),\n",
    "        'rockInside'     :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'wmInapp'     :np.where(np.all(imgLabel == wmInapp, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1))\n",
    "        '''\n",
    "        \n",
    "        #for Hydraulics\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        'emptyInside':np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'   :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'teeth'      :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "        'wmInapp':np.where(np.all(imgLabel == wmInapp, axis=-1)),\n",
    "        'fineInside2' : np.where(np.all(imgLabel == fineInside2, axis=-1))\n",
    "    }\n",
    "    \n",
    "    if len(img) >0 :\n",
    "        plt.imshow(imgLabel)\n",
    "        plt.show()\n",
    "        print(labelsDic)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    xmins, xmaxs, ymins, ymaxs, yminsTeeth, ymaxsTeeth = sortLabelsDic(labelsDic, imgLabel, img)\n",
    "    \n",
    "          \n",
    "    if len(xmins) > 0 and len(xmaxs) > 0 and len(ymins) > 0 and len(ymaxs) > 0:\n",
    "          \n",
    "        if len(yminsTeeth) > 0 and len(ymaxsTeeth) > 0 and (ymaxsTeeth[len(ymaxsTeeth)-1] - yminsTeeth[0]) > (0.5 * imgLabel.shape[1]):\n",
    "            boundariesDict['bucketInit'] = [xmins[0], xmaxs[len(xmaxs)-1], yminsTeeth[0], ymaxsTeeth[len(ymaxsTeeth)-1]]\n",
    "        \n",
    "        else:\n",
    "            boundariesDict['bucketInit'] = [xmins[0], xmaxs[len(xmaxs)-1], ymins[0], ymaxs[len(ymaxs)-1]]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'bucketInit' in boundariesDict:\n",
    "        #(xmin, xmax, ymin, ymax) = boundariesDict['bucketInit']\n",
    "        #cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(255,0,0),3)\n",
    "        \n",
    "        bucketInitHeight = boundariesDict['bucketInit'][1] - boundariesDict['bucketInit'][0]\n",
    "        quart = int(boundariesDict['bucketInit'][0] + bucketInitHeight * 0.4)\n",
    "        \n",
    "        labelsDic2 = {\n",
    "            'fineInside_q' :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == fineInside, axis=-1)),\n",
    "            'fineInside2_q' :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == fineInside2, axis=-1)),\n",
    "            'inapInside_q' :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == inapInside, axis=-1)),\n",
    "            'emptyInside_q':np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == emptyInside, axis=-1)),\n",
    "            'wmInside_q'   :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == wmInside, axis=-1)),\n",
    "            'teeth_q'      :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == teeth, axis=-1)),\n",
    "            'shadow_q'     :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == shadow, axis=-1)),\n",
    "            'wmInapp_q':np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == wmInapp, axis=-1)),\n",
    "            \n",
    "            'rockInside_q' : np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == rockInside, axis=-1)),\n",
    "            #'dust_q' :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == dust, axis=-1)),\n",
    "            #'case_q' :np.where(np.all(imgLabel[boundariesDict['bucketInit'][0]:quart,:,:] == case, axis=-1)),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        xmins2, xmaxs2, ymins2, ymaxs2, yminsTeeth2, ymaxsTeeth2 = sortLabelsDic(labelsDic2, imgLabel, img)\n",
    "        \n",
    "        if len(xmins) > 0 and len(xmaxs) > 0 and len(ymins2) > 0 and len(ymaxs2) > 0:\n",
    "\n",
    "            if len(yminsTeeth2) > 0 and len(ymaxsTeeth2) > 0 and (ymaxsTeeth2[len(ymaxsTeeth2)-1] - yminsTeeth2[0]) > (0.5 * imgLabel.shape[1]):\n",
    "                boundariesDict['bucket'] = [xmins[0], xmaxs[len(xmaxs)-1], yminsTeeth2[0], ymaxsTeeth2[len(ymaxsTeeth2)-1]]\n",
    "\n",
    "            else:\n",
    "                boundariesDict['bucket'] = [xmins[0], xmaxs[len(xmaxs)-1], ymins2[0], ymaxs2[len(ymaxs2)-1]]\n",
    "        \n",
    "                \n",
    "\n",
    "        ##############################################\n",
    "        ######### Draw Boundaries if Verbose #########\n",
    "        ##############################################\n",
    "        if len(img) > 0:\n",
    "            print(boundariesDict)\n",
    "\n",
    "\n",
    "            #draw bucket boundaries\n",
    "            if 'bucket' in boundariesDict:\n",
    "                (xmin, xmax, ymin, ymax) = boundariesDict['bucket']\n",
    "                cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(0,255,255),3)\n",
    "\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        return boundariesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getMatInsideBoundaries(imgLabel, img=[]):\n",
    "    '''\n",
    "    HS: \n",
    "    ---This method returs a list of boundaries: 1 matInside boundary (if present), which is combination of fineInside, rockInside and inappropritate.\n",
    "    '''\n",
    "    boundariesDict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ########## Reading Labeled Pixels ############\n",
    "    ##############################################\n",
    "    labelsDic = {\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        #For Cable  FMDL3.1\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'fmInapp' :np.where(np.all(imgLabel == fmInapp, axis=-1)),\n",
    "\n",
    "        'dust' :np.where(np.all(imgLabel == dust, axis=-1)),\n",
    "        '''\n",
    "        \n",
    "        #For Hydraulics  FMDL3.1\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        \n",
    "        #with the new labels this is the new fmAppropriate mat\n",
    "        'fineInside2' :np.where(np.all(imgLabel == fineInside2, axis=-1))\n",
    "\n",
    "       \n",
    "    }\n",
    "    \n",
    "    if len(img) >0 :\n",
    "        plt.imshow(imgLabel)\n",
    "        plt.show()\n",
    "        print(labelsDic)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    #get the boundary for each label\n",
    "    sortedLabelColsDic = {}\n",
    "    sortedLabelRowsDic = {}\n",
    "    for k in labelsDic.keys():\n",
    "        if len(labelsDic[k][0]) > 0:\n",
    "            sortedLabelColsDic[k+'Cols'] = np.sort(labelsDic[k][0]),\n",
    "            sortedLabelRowsDic[k+'Rows'] = np.sort(labelsDic[k][1]),\n",
    "\n",
    "    if len(img) >0 :\n",
    "        print(sortedLabelColsDic)\n",
    "        print(\"\\n\")\n",
    "        print(sortedLabelRowsDic)\n",
    "        print(\"\\n\")\n",
    "         \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ############## matInside Boundary ###############\n",
    "    ##############################################\n",
    "    #Get the min and max for rows and columns to caluclate bucket boundaries\n",
    "    lowColVals = []\n",
    "    highColVals = []\n",
    "    for labV in sortedLabelColsDic.values():\n",
    "        for v in labV:\n",
    "            lowColVals.append(v[0])\n",
    "            highColVals.append(v[len(v)-1])\n",
    "\n",
    "    lowRowVals = []\n",
    "    highRowVals = []\n",
    "    for labV in sortedLabelRowsDic.values():\n",
    "        for v in labV:\n",
    "            lowRowVals.append(v[0])\n",
    "            highRowVals.append(v[len(v)-1])\n",
    "\n",
    "    \n",
    "\n",
    "    #get the boundary\n",
    "    xmins = np.sort(np.array(lowColVals))\n",
    "    xmaxs = np.sort(np.array(highColVals))\n",
    "    ymins = np.sort(np.array(lowRowVals))\n",
    "    ymaxs = np.sort(np.array(highRowVals))\n",
    "    \n",
    "    if len(xmins) > 0 and len(xmaxs) > 0 and len(ymins) > 0 and len(ymaxs) > 0 : \n",
    "        boundariesDict['matInside'] = [xmins[0], xmaxs[len(xmaxs)-1], ymins[0], ymaxs[len(ymaxs)-1]]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "    ##############################################\n",
    "    ######### Draw Boundaries if Verbose #########\n",
    "    ##############################################\n",
    "    if len(img) > 0:\n",
    "        print(boundariesDict)\n",
    "        \n",
    "        \n",
    "        #draw boundaries\n",
    "        if 'matInside' in boundariesDict:\n",
    "            (xmin, xmax, ymin, ymax) = boundariesDict['matInside']\n",
    "            cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(0,0,255),3)\n",
    "        \n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return boundariesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBucketBoundariesTooth2Tooth(imgLabel, img=[]):\n",
    "\n",
    "    boundariesDict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ########## Reading Labeled Pixels ############\n",
    "    ##############################################\n",
    "    labelsDic = {\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        'emptyInside':np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'   :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'teeth'      :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "        'wmInapp':np.where(np.all(imgLabel == wmInapp, axis=-1)),\n",
    "        'fineInside2' : np.where(np.all(imgLabel == fineInside2, axis=-1)),\n",
    "        \n",
    "        #for Bucyrus\n",
    "        #'sheave'     :np.where(np.all(imgLabel == sheave, axis=-1)),\n",
    "        #'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "    }\n",
    "    \n",
    "    if len(img) >0 :\n",
    "        plt.imshow(imgLabel)\n",
    "        plt.show()\n",
    "        print(labelsDic)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    #get the boundary for each label\n",
    "    sortedLabelColsDic = {}\n",
    "    sortedLabelRowsDic = {}\n",
    "    for k in labelsDic.keys():\n",
    "        if len(labelsDic[k][0]) > 0:\n",
    "            sortedLabelColsDic[k+'Cols'] = np.sort(labelsDic[k][0]),\n",
    "            sortedLabelRowsDic[k+'Rows'] = np.sort(labelsDic[k][1]),\n",
    "\n",
    "    if len(img) >0 :\n",
    "        print(sortedLabelColsDic)\n",
    "        print(\"\\n\")\n",
    "        print(sortedLabelRowsDic)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ############## Bucket Boundary ###############\n",
    "    ##############################################\n",
    "    #Get the min and max for rows and columns to caluclate bucket boundaries\n",
    "    lowColVals = []\n",
    "    highColVals = []\n",
    "    \n",
    "    lowColValsTeeth = []\n",
    "    highColValsTeeth = []\n",
    "    for itemKey in sortedLabelColsDic.keys():\n",
    "        if itemKey == 'teethCols':\n",
    "            labV = sortedLabelColsDic[itemKey]\n",
    "            for v in labV:\n",
    "                lowColValsTeeth.append(v[0])\n",
    "                highColValsTeeth.append(v[len(v)-1])\n",
    "        \n",
    "        labV = sortedLabelColsDic[itemKey]\n",
    "        for v in labV:\n",
    "            lowColVals.append(v[0])\n",
    "            highColVals.append(v[len(v)-1])\n",
    "\n",
    "\n",
    "    lowRowVals = []\n",
    "    highRowVals = []\n",
    "    \n",
    "    lowRowValsTeeth = []\n",
    "    highRowValsTeeth = []\n",
    "    for itemKey in sortedLabelRowsDic.keys():\n",
    "        if itemKey == 'teethRows':\n",
    "            labV = sortedLabelRowsDic[itemKey]\n",
    "            for v in labV:\n",
    "                lowRowValsTeeth.append(v[0])\n",
    "                highRowValsTeeth.append(v[len(v)-1])\n",
    "\n",
    "        labV = sortedLabelRowsDic[itemKey]\n",
    "        for v in labV:\n",
    "            lowRowVals.append(v[0])\n",
    "            highRowVals.append(v[len(v)-1])\n",
    "\n",
    "    \n",
    "\n",
    "    #get the bucket boundary\n",
    "    xmins = np.sort(np.array(lowColVals))\n",
    "    xmaxs = np.sort(np.array(highColVals))\n",
    "    ymins = np.sort(np.array(lowRowVals))\n",
    "    ymaxs = np.sort(np.array(highRowVals))\n",
    "    \n",
    "    yminsTeeth = np.sort(np.array(lowRowValsTeeth))\n",
    "    ymaxsTeeth = np.sort(np.array(highRowValsTeeth))\n",
    "    \n",
    "    print('lowColValsTeeth:')\n",
    "    print(lowColValsTeeth)\n",
    "    print('highColValsTeeth:')\n",
    "    print(highColValsTeeth)\n",
    "    \n",
    "    print('imageShape')\n",
    "    print(imgLabel.shape)\n",
    "    \n",
    "          \n",
    "    if len(xmins) > 0 and len(xmaxs) > 0 and len(ymins) > 0 and len(ymaxs) > 0:\n",
    "          \n",
    "        if len(yminsTeeth) > 0 and len(ymaxsTeeth) > 0 and (ymaxsTeeth[len(ymaxsTeeth)-1] - yminsTeeth[0]) > (0.5 * imgLabel.shape[1]):\n",
    "            boundariesDict['bucket'] = [xmins[0], xmaxs[len(xmaxs)-1], yminsTeeth[0], ymaxsTeeth[len(ymaxsTeeth)-1]]\n",
    "        \n",
    "        else:\n",
    "            boundariesDict['bucket'] = [xmins[0], xmaxs[len(xmaxs)-1], ymins[0], ymaxs[len(ymaxs)-1]]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "    ##############################################\n",
    "    ######### Draw Boundaries if Verbose #########\n",
    "    ##############################################\n",
    "    if len(img) > 0:\n",
    "        print(boundariesDict)\n",
    "        \n",
    "        \n",
    "        #draw bucket boundaries\n",
    "        if 'bucket' in boundariesDict:\n",
    "            (xmin, xmax, ymin, ymax) = boundariesDict['bucket']\n",
    "            cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(0,0,255),3)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return boundariesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBucketBoundaries(imgLabel, img=[]):\n",
    "\n",
    "    boundariesDict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ########## Reading Labeled Pixels ############\n",
    "    ##############################################\n",
    "    labelsDic = {\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        'emptyInside':np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'   :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'teeth'      :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "        \n",
    "        #for Bucyrus\n",
    "        #'sheave'     :np.where(np.all(imgLabel == sheave, axis=-1)),\n",
    "        #'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "    }\n",
    "    \n",
    "    if len(img) >0 :\n",
    "        plt.imshow(imgLabel)\n",
    "        plt.show()\n",
    "        print(labelsDic)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    #get the boundary for each label\n",
    "    sortedLabelColsDic = {}\n",
    "    sortedLabelRowsDic = {}\n",
    "    for k in labelsDic.keys():\n",
    "        if len(labelsDic[k][0]) > 0:\n",
    "            sortedLabelColsDic[k+'Cols'] = np.sort(labelsDic[k][0]),\n",
    "            sortedLabelRowsDic[k+'Rows'] = np.sort(labelsDic[k][1]),\n",
    "\n",
    "    if len(img) >0 :\n",
    "        print(sortedLabelColsDic)\n",
    "        print(\"\\n\")\n",
    "        print(sortedLabelRowsDic)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ############## Bucket Boundary ###############\n",
    "    ##############################################\n",
    "    #Get the min and max for rows and columns to caluclate bucket boundaries\n",
    "    lowColVals = []\n",
    "    highColVals = []\n",
    "    for labV in sortedLabelColsDic.values():\n",
    "        for v in labV:\n",
    "            lowColVals.append(v[0])\n",
    "            highColVals.append(v[len(v)-1])\n",
    "\n",
    "    lowRowVals = []\n",
    "    highRowVals = []\n",
    "    for labV in sortedLabelRowsDic.values():\n",
    "        for v in labV:\n",
    "            lowRowVals.append(v[0])\n",
    "            highRowVals.append(v[len(v)-1])\n",
    "\n",
    "    \n",
    "\n",
    "    #get the bucket boundary\n",
    "    xmins = np.sort(np.array(lowColVals))\n",
    "    xmaxs = np.sort(np.array(highColVals))\n",
    "    ymins = np.sort(np.array(lowRowVals))\n",
    "    ymaxs = np.sort(np.array(highRowVals))\n",
    "    \n",
    "    if len(xmins) > 0 and len(xmaxs) > 0 and len(ymins) > 0 and len(ymaxs) > 0 : \n",
    "        boundariesDict['bucket'] = [xmins[0], xmaxs[len(xmaxs)-1], ymins[0], ymaxs[len(ymaxs)-1]]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "    ##############################################\n",
    "    ######### Draw Boundaries if Verbose #########\n",
    "    ##############################################\n",
    "    if len(img) > 0:\n",
    "        print(boundariesDict)\n",
    "        \n",
    "        \n",
    "        #draw bucket boundaries\n",
    "        (xmin, xmax, ymin, ymax) = boundariesDict['bucket']\n",
    "        cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(0,0,255),3)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return boundariesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getAllBoundaries(imgLabel, img=[]):\n",
    "    '''\n",
    "    HS: \n",
    "    ---This method returs a list of boundaries: 1 bucket boundary, 1 fineInside boundary (if present) and several     rockInside boundaries (if present). \n",
    "    '''\n",
    "    boundariesDict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ########## Reading Labeled Pixels ############\n",
    "    ##############################################\n",
    "    labelsDic = {\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        'emptyInside':np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'   :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'teeth'      :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "        \n",
    "        #for Bucyrus\n",
    "        #'sheave'     :np.where(np.all(imgLabel == sheave, axis=-1)),\n",
    "        #'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "    }\n",
    "    \n",
    "    if len(img) >0 :\n",
    "        plt.imshow(imgLabel)\n",
    "        plt.show()\n",
    "        print(labelsDic)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    #get the boundary for each label\n",
    "    sortedLabelColsDic = {}\n",
    "    sortedLabelRowsDic = {}\n",
    "    for k in labelsDic.keys():\n",
    "        if len(labelsDic[k][0]) > 0:\n",
    "            sortedLabelColsDic[k+'Cols'] = np.sort(labelsDic[k][0]),\n",
    "            sortedLabelRowsDic[k+'Rows'] = np.sort(labelsDic[k][1]),\n",
    "\n",
    "    if len(img) >0 :\n",
    "        print(sortedLabelColsDic)\n",
    "        print(\"\\n\")\n",
    "        print(sortedLabelRowsDic)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ##############################################\n",
    "    ############# FineInside Boundary ############\n",
    "    ##############################################'\n",
    "    if 'fineInsideCols' in sortedLabelColsDic and 'fineInsideRows' in sortedLabelRowsDic:\n",
    "        boundariesDict['fineInside'] = [\n",
    "            int(sortedLabelColsDic[\"fineInsideCols\"][0][0]),\n",
    "            int(sortedLabelColsDic[\"fineInsideCols\"][0][len(sortedLabelRowsDic[\"fineInsideRows\"][0])-1]),\n",
    "            int(sortedLabelRowsDic[\"fineInsideRows\"][0][0]),\n",
    "            int(sortedLabelRowsDic[\"fineInsideRows\"][0][len(sortedLabelColsDic[\"fineInsideCols\"][0])-1]),\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ############# teeth Boundary ############\n",
    "    ##############################################\n",
    "    if 'teethCols' in sortedLabelColsDic and 'teethRows' in sortedLabelRowsDic:\n",
    "        boundariesDict['teeth'] = [\n",
    "            int(sortedLabelColsDic[\"teethCols\"][0][0]),\n",
    "            int(sortedLabelColsDic[\"teethCols\"][0][len(sortedLabelRowsDic[\"teethRows\"][0])-1]),\n",
    "            int(sortedLabelRowsDic[\"teethRows\"][0][0]),\n",
    "            int(sortedLabelRowsDic[\"teethRows\"][0][len(sortedLabelColsDic[\"teethCols\"][0])-1]),\n",
    "        ]   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    ############## Bucket Boundary ###############\n",
    "    ##############################################\n",
    "    #Get the min and max for rows and columns to caluclate bucket boundaries\n",
    "    lowColVals = []\n",
    "    highColVals = []\n",
    "    for labV in sortedLabelColsDic.values():\n",
    "        for v in labV:\n",
    "            lowColVals.append(v[0])\n",
    "            highColVals.append(v[len(v)-1])\n",
    "\n",
    "    lowRowVals = []\n",
    "    highRowVals = []\n",
    "    for labV in sortedLabelRowsDic.values():\n",
    "        for v in labV:\n",
    "            lowRowVals.append(v[0])\n",
    "            highRowVals.append(v[len(v)-1])\n",
    "\n",
    "    \n",
    "\n",
    "    #get the bucket boundary\n",
    "    xmins = np.sort(np.array(lowColVals))\n",
    "    xmaxs = np.sort(np.array(highColVals))\n",
    "    ymins = np.sort(np.array(lowRowVals))\n",
    "    ymaxs = np.sort(np.array(highRowVals))\n",
    "    \n",
    "    if len(xmins) > 0 and len(xmaxs) > 0 and len(ymins) > 0 and len(ymaxs) > 0 : \n",
    "        boundariesDict['bucket'] = [xmins[0], xmaxs[len(xmaxs)-1], ymins[0], ymaxs[len(ymaxs)-1]]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    ##############################################\n",
    "    ############# RockInside Boundary ############\n",
    "    ##############################################\n",
    "    if 'rockInsideCols' in sortedLabelColsDic and 'rockInsideRows' in sortedLabelRowsDic:\n",
    "        # Getting the rock maks used to find rock boundaries        \n",
    "        rockMask = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "        rockMask[labelsDic['rockInside']] = 1\n",
    "\n",
    "        # This converts any np.array to opencv image.\n",
    "        cv_rockMask = img_as_ubyte(rockMask)\n",
    "\n",
    "        contours, _ = cv2.findContours(cv_rockMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Getting bounding boxes from contours\n",
    "        rockBoundaries = []\n",
    "        for cnt in contours:\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            (xmin, xmax, ymin, ymax) = (y, (y+h), x, (x+w))\n",
    "\n",
    "            # only consider large boundaries.\n",
    "            if h>3 and w >3:\n",
    "                rockBoundaries.append([xmin, xmax, ymin, ymax])\n",
    "\n",
    "        boundariesDict['rockInside'] = rockBoundaries\n",
    "\n",
    "\n",
    "    \n",
    "                \n",
    "\n",
    "    ##############################################\n",
    "    ######### Draw Boundaries if Verbose #########\n",
    "    ##############################################\n",
    "    if len(img) > 0:\n",
    "        print(boundariesDict)\n",
    "        \n",
    "        \n",
    "        #draw bucket boundaries\n",
    "        (xmin, xmax, ymin, ymax) = boundariesDict['bucket']\n",
    "        cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(0,0,255),3)\n",
    "        \n",
    "        \n",
    "        #draw rockInside boundaries\n",
    "        for rockBb in boundariesDict['rockInside']:\n",
    "            cv2.rectangle(img,(rockBb[2], rockBb[0]),(rockBb[3], rockBb[1]),(255,0,0),3)\n",
    "\n",
    "        \n",
    "        #draw fineInside boundaries\n",
    "        (xmin, xmax, ymin, ymax) = boundariesDict['fineInside']\n",
    "        cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(0,255,0),3)\n",
    "        \n",
    "        \n",
    "        #draw teeth boundaries\n",
    "        (xmin, xmax, ymin, ymax) = boundariesDict['teeth']\n",
    "        cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(255,255,0),3)\n",
    "        \n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return boundariesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def writeRowsToCsv(rows, csvFullPath):\n",
    "    # open the file\n",
    "    csv_file = open(csvFullPath, \"w\") \n",
    "    \n",
    "    # define column names\n",
    "    columnTitles = \"filename,pathname,xmins,xmax,ymins,ymax,class\\n\"\n",
    "    csv_file.write(columnTitles)\n",
    "\n",
    "    # write rows\n",
    "    for row in rows:\n",
    "        csv_file.write(row)\n",
    "\n",
    "    csv_file.close()\n",
    "    \n",
    "    print(\"wrote \" + str(len(rows)) + \" rows to csv file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def writeRowDicToCsv(rowsDic, csvFullPath):\n",
    "    # open the file\n",
    "    #use w for mode to override existing\n",
    "    csv_file = open(csvFullPath, \"a\") \n",
    "    \n",
    "    # define column names\n",
    "    columnTitles = \"filename,pathname,xmins,xmax,ymins,ymax,class\\n\"\n",
    "    csv_file.write(columnTitles)\n",
    "\n",
    "    # write rows\n",
    "    for imId in rowsDic:\n",
    "        if imId != 'fileName':\n",
    "            row = rowsDic[imId] + '\\n'\n",
    "            csv_file.write(row)\n",
    "\n",
    "    csv_file.close()\n",
    "    \n",
    "    print(\"wrote \" + str(len(rowsDic)) + \" rows to csv file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def readCsvRows(fullCsvPath):\n",
    "    # open the file\n",
    "    csv_file = open(fullCsvPath, \"r\") \n",
    "    data = csv_file.read()\n",
    "    csv_file.close()\n",
    "    \n",
    "    rows = data.split('\\n')\n",
    "    \n",
    "    print(\"read \" + str(len(rows)) + \" rows\")\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getRowsDictFromCsv(fullCsvPath):\n",
    "    csv_file = open(fullCsvPath, \"r\") \n",
    "    data = csv_file.read()\n",
    "    csv_file.close()\n",
    "\n",
    "    rows = data.split('\\n')\n",
    "\n",
    "    rowsDict = {}\n",
    "\n",
    "    for row in rows[1 : len(rows)-1]:\n",
    "        vals = row.split(',')\n",
    "\n",
    "        if vals[0] not in rowsDict:\n",
    "            rowsDict[vals[0]] = []\n",
    "\n",
    "        rowsDict[vals[0]].append(vals[2:7])\n",
    "        \n",
    "    print(\"read \" + str(len(rowsDict)) + \" examples\")\n",
    "    return rowsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getRowsDictFromCsvSaveJustBucketBoundingBox(fullCsvPath):\n",
    "    csv_file = open(fullCsvPath, \"r\") \n",
    "    data = csv_file.read()\n",
    "    csv_file.close()\n",
    "\n",
    "    rows = data.split('\\n')\n",
    "\n",
    "    rowsDict = {}\n",
    "\n",
    "    for row in rows[1 : len(rows)-1]:\n",
    "        vals = row.split(',')\n",
    "\n",
    "        if vals[-1] == 'bucket':\n",
    "            rowsDict[vals[0]] = vals[2:7]\n",
    "        \n",
    "    print(\"read \" + str(len(rowsDict)) + \" examples\")\n",
    "    return rowsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getCertainClassRowsDictFromCsv(fullCsvPath, classesToLookFor):\n",
    "    csv_file = open(fullCsvPath, \"r\") \n",
    "    data = csv_file.read()\n",
    "    csv_file.close()\n",
    "\n",
    "    rows = data.split('\\n')\n",
    "\n",
    "    rowsDict = {}\n",
    "\n",
    "    for row in rows[1 : len(rows)-1]:\n",
    "        vals = row.split(',')\n",
    "        \n",
    "        if vals[6] in classesToLookFor:\n",
    "\n",
    "            if vals[0] not in rowsDict:\n",
    "                rowsDict[vals[0]] = row\n",
    "            else:\n",
    "                print(\"error. duplicateRow. This shouldn't happen\")\n",
    "        \n",
    "    print(\"read \" + str(len(rowsDict)) + \" examples\")\n",
    "    return rowsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualizeRowDict(rowDict, writeToDisk=False, outputDirPath=\"\", saveLabelToo=False):\n",
    "    for imgId in rowsDict:\n",
    "\n",
    "        img = imread(imagesPath + imgId)\n",
    "        if saveLabelToo==True:\n",
    "            label = imread(labelsPath + imgId)\n",
    "\n",
    "        for box in rowsDict[imgId]:\n",
    "            \n",
    "            if(box[0] != \"\"):\n",
    "\n",
    "                if box[4] == 'bucket':\n",
    "                    (xmin, xmax, ymin, ymax) = box[0:4]\n",
    "                    int(round(float(xmin)))\n",
    "                    cv2.rectangle(img,(int(round(float(xmin))), int(round(float(ymin)))),(int(round(float(xmax))), int(round(float(ymax)))),(0,0,255),3)\n",
    "                    \n",
    "                    \n",
    "                if box[4] == 'matInside':\n",
    "                    (xmin, xmax, ymin, ymax) = box[0:4]\n",
    "                    cv2.rectangle(img,(int(round(float(xmin))), int(round(float(ymin)))),(int(round(float(xmax))), int(round(float(ymax)))),(0,255,0),3)\n",
    "\n",
    "\n",
    "                if box[4] == 'fineInside':\n",
    "                    (xmin, xmax, ymin, ymax) = box[0:4]\n",
    "                    cv2.rectangle(img,(int(round(float(xmin))), int(round(float(ymin)))),(int(round(float(xmax))), int(round(float(ymax)))),(0,255,0),3)\n",
    "\n",
    "\n",
    "                if box[4] == 'rockInside':\n",
    "                    (xmin, xmax, ymin, ymax) = box[0:4]\n",
    "                    cv2.rectangle(img,(int(round(float(xmin))), int(round(float(ymin)))),(int(round(float(xmax))), int(round(float(ymax)))),(255,0,0),3)\n",
    "\n",
    "\n",
    "        if writeToDisk==True and outputDirPath != \"\":\n",
    "            cv2.imwrite(outputDirPath + imgId, img)\n",
    "            \n",
    "            if saveLabelToo==True:\n",
    "                cv2.imwrite(outputDirPath + \"_label_\" + imgId, label)\n",
    "        else:\n",
    "            print(imgId)\n",
    "            if saveLabelToo==True:\n",
    "                plt.imshow(label)\n",
    "                plt.show()\n",
    "\n",
    "            plt.imshow(img)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualizeRow(row):\n",
    "    vals = row.split(',')\n",
    "    \n",
    "    img = imread(imagesPath + vals[0])\n",
    "    label = imread(labelsPath + vals[0])\n",
    "    \n",
    "    xmin, xmax, ymin, ymax = vals[2:6]\n",
    "\n",
    "    if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "        (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "        \n",
    "        \n",
    "        if vals[6] == 'matInside':\n",
    "            cv2.rectangle(img,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(0,255,0),3)\n",
    "            cv2.rectangle(label,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(0,255,0),3)\n",
    "            \n",
    "        if vals[6] == 'matInside':\n",
    "            cv2.rectangle(img,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(0,0,255),3)\n",
    "            cv2.rectangle(label,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(0,0,255),3)\n",
    "            \n",
    "        else:\n",
    "            #read in notebook\n",
    "            cv2.rectangle(img,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(255,0,0),3)\n",
    "            cv2.rectangle(label,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(255,0,0),3)\n",
    "            \n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(label)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def writeRowToDisk(row, writeLabelsToo=False):\n",
    "    vals = row.split(',')\n",
    "    \n",
    "    img = imread(imagesPath + vals[0])\n",
    "    \n",
    "    xmin, xmax, ymin, ymax = vals[2:6]\n",
    "\n",
    "    cv2.rectangle(img,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(255,0,0),3)\n",
    "    \n",
    "    cv2.imwrite(imgBoundariesPath + vals[0], img)\n",
    "    \n",
    "    if writeLabelsToo:\n",
    "        label = imread(labelsPath + vals[0])\n",
    "        cv2.rectangle(label,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(255,0,0),3)\n",
    "        cv2.imwrite(labelBoundariesPath + vals[0], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualizeImg(imgId):\n",
    "    img = imread(imagesPath + imgId)\n",
    "    label = imread(labelsPath + imgId)\n",
    "    \n",
    "    foundBucketBoundary, xmin, xmax, ymin, ymax = getBucketBoundaries(label)\n",
    "    \n",
    "    if(foundBucketBoundary):\n",
    "        cv2.rectangle(img,(ymin, xmin),(ymax, xmax),(255,0,0),3)\n",
    "        cv2.rectangle(label,(ymin, xmin),(ymax, xmax),(255,0,0),3)\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(label)\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"could not find bucket boundary\\n\")\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(label)\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBbxMask(row, writeToDisk=False, mask_direct_path=None, verbose=False):\n",
    "    vals = row.split(',')\n",
    "    \n",
    "    img = imread(imagesPath + vals[0])\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), bool)\n",
    "    \n",
    "    xmin, xmax, ymin, ymax = vals[2:6]\n",
    "    if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "        (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "        mask[ymin:ymax, xmin:xmax] = 1\n",
    "    \n",
    "    if writeToDisk == True and mask_direct_path != None:\n",
    "        mask.dtype='uint8'\n",
    "        cv2.imwrite(mask_direct_path + vals[0], mask)\n",
    "        \n",
    "    if verbose:\n",
    "        if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "            cv2.rectangle(img,(xmin, ymin),(xmax, ymax),(255,0,0),3)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calcPerformance(pred_rows, gt_rows, predictedBbMasks_path, gtBbMaks_path, verbose=False, predictedRowDict=None):\n",
    "    total_tn = 0\n",
    "    total_tp = 0\n",
    "    total_fn = 0\n",
    "    total_fp = 0\n",
    "    \n",
    "    rowCount = 0\n",
    "\n",
    "    if predictedRowDict == None:\n",
    "        print(\"using pred_rows\")\n",
    "        for pred_row, gt_row in zip(pred_rows, gt_rows):\n",
    "            pred_vals = pred_row.split(',')\n",
    "            gt_vals = gt_row.split(',')\n",
    "\n",
    "            if gt_vals[0] == pred_vals[0]:\n",
    "                pred_mask = imread(predictedBbMasks_path + pred_vals[0])\n",
    "                gt_mask = imread(gtBbMaks_path + gt_vals[0])\n",
    "\n",
    "                pos_preds = np.where(pred_mask == 1)\n",
    "                neg_preds = np.where(pred_mask == 0)\n",
    "\n",
    "                pos_overlap = pred_mask * gt_mask\n",
    "                neg_overlap = np.logical_not(pred_mask) * np.logical_not(gt_mask)\n",
    "\n",
    "                tp = np.count_nonzero(pos_overlap)\n",
    "                tn = np.count_nonzero(neg_overlap)\n",
    "                fp = len(pos_preds[0]) - tp\n",
    "                fn = len(neg_preds[0]) - tn\n",
    "\n",
    "                total_fp += fp\n",
    "                total_fn += fn\n",
    "                total_tp += tp\n",
    "                total_tn += tn\n",
    "                \n",
    "                rowCount += 1\n",
    "\n",
    "            else:\n",
    "                print(\"ERROR image id's don't match between gt csv file and predictions csv file\")\n",
    "\n",
    "    else:\n",
    "        print(\"using predictedRowDict\")\n",
    "        for gt_row in gt_rows:\n",
    "            gt_vals = gt_row.split(',')\n",
    "\n",
    "            if gt_vals[0] in predictedRowDict:\n",
    "                pred_vals = predictedRowDict[gt_vals[0]]\n",
    "                pred_mask = imread(predictedBbMasks_path + gt_vals[0])\n",
    "                gt_mask = imread(gtBbMaks_path + gt_vals[0])\n",
    "\n",
    "                pos_preds = np.where(pred_mask == 1)\n",
    "                neg_preds = np.where(pred_mask == 0)\n",
    "\n",
    "                pos_overlap = pred_mask * gt_mask\n",
    "                neg_overlap = np.logical_not(pred_mask) * np.logical_not(gt_mask)\n",
    "\n",
    "                tp = np.count_nonzero(pos_overlap)\n",
    "                tn = np.count_nonzero(neg_overlap)\n",
    "                fp = len(pos_preds[0]) - tp\n",
    "                fn = len(neg_preds[0]) - tn\n",
    "\n",
    "                total_fp += fp\n",
    "                total_fn += fn\n",
    "                total_tp += tp\n",
    "                total_tn += tn\n",
    "                \n",
    "                rowCount += 1\n",
    "                \n",
    "        \n",
    "    print(\"Processed \" + str(rowCount) + \" rows:\" )\n",
    "                \n",
    "\n",
    "                \n",
    "    if verbose:\n",
    "        plt.imshow(pred_mask)\n",
    "        plt.title(\"pred\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(gt_mask)\n",
    "        plt.title(\"gt\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(pos_overlap)\n",
    "        plt.title(\"pos_overlap\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(neg_overlap)\n",
    "        plt.title(\"neg_overlap\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"tp: \" + str(tp) + \" ,    fp: \" + str(fp) + \" ,    tn: \" + str(tn) + \" ,    fn: \" + str(fn) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    sensitivity = float(total_tp) / (total_tp + total_fn)\n",
    "    specificity = float(total_tn) / (total_tn + total_fp)\n",
    "    precision = float(total_tp) / (total_tp + total_fp)\n",
    "    f_score = 2 * precision * sensitivity / (precision + sensitivity)\n",
    "\n",
    "    print(\"-Sensitivity: \" + str(sensitivity) +\n",
    "          \" ,    -Specificity: \" + str(specificity) +\n",
    "          \" ,    -Precision: \" + str(precision) +\n",
    "          \" ,    -F_score: \" + str(f_score)\n",
    "         )\n",
    "    \n",
    "    return sensitivity, specificity, precision, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calcPerformance_insideBucket_multiClass(unet_pathToSavedResults, unet_pathTo1ChanLabels, verbose = False):\n",
    "\n",
    "    total_tn = 0\n",
    "    total_tp = 0\n",
    "    total_fn = 0\n",
    "    total_fp = 0\n",
    "\n",
    "    rowCount = 0\n",
    "\n",
    "\n",
    "    for imgId in os.listdir(unet_pathTo1ChanLabels):\n",
    "\n",
    "        label = imread(unet_pathTo1ChanLabels + imgId)\n",
    "        pred = imread(unet_pathToSavedResults + imgId)\n",
    "\n",
    "        labelMask = np.zeros((label.shape[0], label.shape[1]), bool)\n",
    "        predMask = np.zeros((pred.shape[0], pred.shape[1]), bool)\n",
    "\n",
    "\n",
    "\n",
    "        labelPixels = np.where(label == 2)\n",
    "        predPixels = np.where(pred == 2)\n",
    "\n",
    "        labelMask[labelPixels] = 1\n",
    "        predMask[predPixels] = 1\n",
    "\n",
    "\n",
    "\n",
    "        pos_preds = np.where(predMask == 1)\n",
    "        neg_preds = np.where(predMask == 0)\n",
    "\n",
    "        pos_overlap = predMask * labelMask\n",
    "        neg_overlap = np.logical_not(predMask) * np.logical_not(labelMask)\n",
    "\n",
    "\n",
    "        if verbose==True:\n",
    "            imshow(label)\n",
    "            plt.title('label')\n",
    "            plt.show()\n",
    "\n",
    "            imshow(labelMask)\n",
    "            plt.title('labelMask')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            imshow(pred)\n",
    "            plt.title('pred')\n",
    "            plt.show()\n",
    "\n",
    "            imshow(predMask)\n",
    "            plt.title('pred')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        tp = np.count_nonzero(pos_overlap)\n",
    "        tn = np.count_nonzero(neg_overlap)\n",
    "        fp = len(pos_preds[0]) - tp\n",
    "        fn = len(neg_preds[0]) - tn\n",
    "\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        total_tp += tp\n",
    "        total_tn += tn\n",
    "\n",
    "        rowCount += 1\n",
    "\n",
    "\n",
    "\n",
    "    sensitivity = float(total_tp) / (total_tp + total_fn)\n",
    "    specificity = float(total_tn) / (total_tn + total_fp)\n",
    "    precision = float(total_tp) / (total_tp + total_fp)\n",
    "    f_score = 2 * precision * sensitivity / (precision + sensitivity)\n",
    "\n",
    "    print(\"-Sensitivity: \" + str(sensitivity) +\n",
    "          \" ,    -Specificity: \" + str(specificity) +\n",
    "          \" ,    -Precision: \" + str(precision) +\n",
    "          \" ,    -F_score: \" + str(f_score)\n",
    "         )\n",
    "\n",
    "\n",
    "    return sensitivity, specificity, precision, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cropImgFromRow(row, showResult = True , saveResult=False, cropImgPath = \"\", cropLabelPath = \"\"):\n",
    "    vals = row.split(',')\n",
    "\n",
    "    img = imread(imagesPath + vals[0])\n",
    "    label = imread(labelsPath + vals[0])\n",
    "\n",
    "    xmin, xmax, ymin, ymax = vals[2:6]\n",
    "\n",
    "    if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "        (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "\n",
    "        img = img[ymin:ymax, xmin:xmax,]\n",
    "        label = label[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        \n",
    "        \n",
    "    if saveResult==True:\n",
    "        cv2.imwrite(cropImgPath + vals[0], img)\n",
    "        cv2.imwrite(cropLabelPath + vals[0], label)\n",
    "\n",
    "        \n",
    "    if showResult==True:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(label)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cropImgFromRowV2(vals, imgName, margin, showResult = True , saveResult=False, cropImgPath = \"\", cropLabelPath = \"\"):\n",
    "\n",
    "    bestVal = vals[0]\n",
    "    for val in vals:\n",
    "        if val[4] == 'matInside':\n",
    "            bestVal = val\n",
    "    \n",
    "    img = imread(imagesPath + imgName)\n",
    "    label = imread(labelsPath + imgName)\n",
    "\n",
    "    xmin, xmax, ymin, ymax = bestVal[0:4]\n",
    "\n",
    "    if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "        (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "\n",
    "        \n",
    "        \n",
    "        if (ymin-margin) > 0:\n",
    "            yminAdj = (ymin-margin)\n",
    "        else:\n",
    "            yminAdj = ymin\n",
    "\n",
    "\n",
    "        if (xmin-margin) > 0:\n",
    "            xminAdj = (xmin-margin)\n",
    "        else:\n",
    "            xminAdj = xmin\n",
    "\n",
    "\n",
    "\n",
    "        if (ymax + margin) < img.shape[0]:\n",
    "            ymaxAdj = (ymax + margin)\n",
    "        else:\n",
    "            ymaxAdj = ymax\n",
    "\n",
    "\n",
    "\n",
    "        if (xmax + margin) < img.shape[1]:\n",
    "            xmaxAdj = (xmax + margin)\n",
    "        else:\n",
    "            xmaxAdj = xmax\n",
    "\n",
    "\n",
    "        img = img[yminAdj:ymaxAdj, xminAdj:xmaxAdj,]\n",
    "        label = label[yminAdj:ymaxAdj, xminAdj:xmaxAdj]\n",
    "\n",
    "        \n",
    "\n",
    "    if saveResult==True:\n",
    "        cv2.imwrite(cropImgPath + imgName, img)\n",
    "        cv2.imwrite(cropLabelPath + imgName, label)\n",
    "\n",
    "\n",
    "    if showResult==True:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(label)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def randomCropImgFromRow(row, showResult = True , saveResult=False, cropImgPath = \"\", cropLabelPath = \"\", offsetsToApply = []):\n",
    "    vals = row.split(',')\n",
    "\n",
    "    img = imread(imagesPath + vals[0])\n",
    "    label = imread(labelsPath + vals[0])\n",
    "\n",
    "    xmin, xmax, ymin, ymax = vals[2:6]\n",
    "\n",
    "    if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "        (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "\n",
    "        imgWidth = img.shape[1]\n",
    "        imgHeight = img.shape[0]\n",
    "        \n",
    "        if showResult==True:\n",
    "            plt.imshow(img)\n",
    "            plt.title('imgOrig')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "        imgAct = img[ymin:ymax, xmin:xmax,]\n",
    "        labelAct = label[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        if saveResult==True:\n",
    "            cv2.imwrite(cropImgPath + vals[0].replace('.png', '_RCAct.png'), imgAct)\n",
    "            cv2.imwrite(cropLabelPath + vals[0].replace('.png', '_RCAct.png'), labelAct)\n",
    "\n",
    "        if showResult==True:\n",
    "            plt.imshow(imgAct)\n",
    "            plt.title('imgAct')\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "            \n",
    "        for offset in offsetsToApply:\n",
    "            \n",
    "            img1 =     img[ymin + offset:ymax + offset, xmin:xmax]\n",
    "            label1 = label[ymin + offset:ymax + offset, xmin:xmax]\n",
    "            \n",
    "            if saveResult==True:\n",
    "                cv2.imwrite(cropImgPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_1.png', img1)\n",
    "                cv2.imwrite(cropLabelPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_1.png', label1)\n",
    "            \n",
    "            if showResult==True:\n",
    "                plt.imshow(img1)\n",
    "                plt.title('img1')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            img2 =     img[ymin:ymax, xmin + offset:xmax + offset]\n",
    "            label2 = label[ymin:ymax, xmin + offset:xmax + offset]\n",
    "\n",
    "            if saveResult==True:\n",
    "                cv2.imwrite(cropImgPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_2.png', img2)\n",
    "                cv2.imwrite(cropLabelPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_2.png', label2)\n",
    "            \n",
    "            if showResult==True:\n",
    "                plt.imshow(img2)\n",
    "                plt.title('img2')\n",
    "                plt.show()\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            if (ymin - offset) >= 0:\n",
    "                img3 =     img[ymin - offset:ymax - offset, xmin:xmax]\n",
    "                label3 = label[ymin - offset:ymax - offset, xmin:xmax]\n",
    "\n",
    "                if saveResult==True:\n",
    "                    cv2.imwrite(cropImgPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_3.png', img3)\n",
    "                    cv2.imwrite(cropLabelPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_3.png', label3)\n",
    "\n",
    "                if showResult==True:\n",
    "                    plt.imshow(img3)\n",
    "                    plt.title('img3')\n",
    "                    plt.show()\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            if (xmin - offset) >= 0:\n",
    "                img4 =     img[ymin:ymax, xmin - offset:xmax - offset]\n",
    "                label4 = label[ymin:ymax, xmin - offset:xmax - offset]\n",
    "\n",
    "                if saveResult==True:\n",
    "                    cv2.imwrite(cropImgPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_4.png', img4)\n",
    "                    cv2.imwrite(cropLabelPath + vals[0].replace('.png', '') + '_RC' + str(offset) + '_4.png', label4)\n",
    "\n",
    "                if showResult==True:\n",
    "                    plt.imshow(img4)\n",
    "                    plt.title('img4')\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## SSD Data Generation: Cable and Hydraulics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### FMDL 3.1 Automated data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Automatically generating training data from directory of images\n",
    "imagesPath = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/hydraulics/images_fromJira/'\n",
    "labelsPath = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/hydraulics/labels_fromJira/'\n",
    "\n",
    "dir2PutRejectedImages='/home/hooman/Desktop/deleteThis/negExamples/'\n",
    "dir2PutAcceptedImages='/home/hooman/Desktop/deleteThis/goodExamples/'\n",
    "dir2PutBadMatInsideImages='/home/hooman/Desktop/deleteThis/badExamples/'\n",
    "verbose=False\n",
    "\n",
    "\n",
    "rows = []\n",
    "for fileName in os.listdir(imagesPath):\n",
    "    \n",
    "    filePath = labelsPath + fileName\n",
    "    \n",
    "    imgLabel = imread(filePath)\n",
    "    \n",
    "    #I added this for try3  ssd-multiclass-tries 1 and 2 didnt have this.\n",
    "    imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGBA2RGB)\n",
    "    img = imread(imagesPath + fileName)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"processing file:\\n\" + filePath + '\\n')\n",
    "    \n",
    "    boundariesDict = seperateTrainingImages_hydraulics(imgLabel, img, fileName, dir2PutRejectedImages, dir2PutAcceptedImages, dir2PutBadMatInsideImages, verbose)\n",
    "\n",
    "    print('\\nboundariesDict:')\n",
    "    print(boundariesDict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if boundariesDict and ('bucket' in boundariesDict):\n",
    "        # write up the rows\n",
    "        (xmin, xmax, ymin, ymax) = boundariesDict['bucket']\n",
    "        row = fileName + \",\" + filePath + \",\" + str(ymin) +\",\"+ str(ymax) + \",\" + str(xmin) + \",\" + str(xmax) + \",\" + \"bucket\" + \"\\n\"\n",
    "    \n",
    "        rows.append(row)\n",
    "\n",
    "        \n",
    "        if('matInside' in boundariesDict):\n",
    "            if len(boundariesDict['matInside']) == 4:\n",
    "                (xmin, xmax, ymin, ymax) = boundariesDict['matInside']\n",
    "                row = fileName + \",\" + filePath + \",\" + str(ymin) +\",\"+ str(ymax) + \",\" + str(xmin) + \",\" + str(xmax) + \",\" + \"matInside\" + \"\\n\"\n",
    "            else:\n",
    "                print(\"ERROR: Found more than one matInside Boundaries. This should NOT happen\\n\")\n",
    "\n",
    "            rows.append(row)  \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "print(\"processed \"+ str(len(rows)) + \" rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "writeRowsToCsv(rows, \"/home/hooman/Desktop/deleteThis/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for row in rows[0:38]:\n",
    "    visualizeRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualizing existing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rowsDict = getRowsDictFromCsv(\"/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/backhoe/fmdl-backhoe-trainingData-new_may10-2019/trainingSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualizeRowDict(rowsDict, True, '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/backhoe/fmdl-backhoe-trainingData-new_may10-2019/boundariesVisualized/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## U-Net Data Generation: Cable and Hydraulics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cropping images to contain only ROI (V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rowDict = getRowsDictFromCsv('/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameNetworkAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/unetBoxes_finalTrainingSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for imgName in os.listdir(imagesPath):\n",
    "    cropImgFromRowV2(rowDict[imgName],imgName, 100, False , True,'/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameNetworkAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/finalTrainingSet_images_croped/','/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameNetworkAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/finalTrainingSet_masks_croped/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random Cropping images for U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt_rows = readCsvRows('/home/hooman/backhoeOpticalScene/roiDelineators/try1-csvFrom-ssdTry2/fromSSdTry2_fineAndRockAndInapInMatInside_backhoe_shuffled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# random crop images and masks\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "offsetsToApply = [50,75, 100, 125, 150, 175]\n",
    "dirToSaveImages = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/randomCroppedImages/'\n",
    "dirToSaveMasks = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/randomCroppedMasks/'\n",
    "\n",
    "\n",
    "dirToReadImages = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/allImages/'\n",
    "dirToReadMasks = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/fullMaks/'\n",
    "\n",
    "\n",
    "for row in gt_rows[1:len(gt_rows)-1]:\n",
    "    \n",
    "    vals = row.split(',')\n",
    "\n",
    "    if os.path.exists(dirToReadImages + vals[0]) and os.path.exists(dirToReadMasks + vals[0]):\n",
    "        \n",
    "        #This causes a \"Too many open files error\"\n",
    "        #img = imread(dirToReadImages + vals[0])\n",
    "        #label = imread(dirToReadMasks + vals[0])\n",
    "        \n",
    "        imgPil = Image.open(dirToReadImages + vals[0])\n",
    "        img = np.array(imgPil) \n",
    "        imgPil.close()\n",
    "\n",
    "        labelPil = Image.open(dirToReadMasks + vals[0])\n",
    "        label = np.array(labelPil)\n",
    "        labelPil.close()\n",
    "        \n",
    "        \n",
    "\n",
    "        xmin, xmax, ymin, ymax = vals[2:6]\n",
    "\n",
    "        if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "            (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "\n",
    "            imgWidth = img.shape[1]\n",
    "            imgHeight = img.shape[0]\n",
    "\n",
    "\n",
    "            imgAct = img[ymin:ymax, xmin:xmax,]\n",
    "            labelAct = label[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if imgAct.shape[0] > 0 and imgAct.shape[1] > 0 and labelAct.shape[0] > 0 and labelAct.shape[1] > 0: \n",
    "                cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '_RCAct.png'), imgAct)\n",
    "                cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '_RCAct.png'), labelAct)\n",
    "\n",
    "\n",
    "\n",
    "            for offset in offsetsToApply:\n",
    "\n",
    "                img1 =     img[ymin + offset:ymax + offset, xmin:xmax]\n",
    "                label1 = label[ymin + offset:ymax + offset, xmin:xmax]\n",
    "                \n",
    "                if img1.shape[0] > 0 and img1.shape[1] > 0 and label1.shape[0] > 0 and label1.shape[1] > 0: \n",
    "                    cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_1.png', img1)\n",
    "                    cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_1.png', label1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                img2 =     img[ymin:ymax, xmin + offset:xmax + offset]\n",
    "                label2 = label[ymin:ymax, xmin + offset:xmax + offset]\n",
    "         \n",
    "                if img2.shape[0] > 0 and img2.shape[1] > 0 and label2.shape[0] > 0 and label2.shape[1] > 0: \n",
    "                    cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_2.png', img2)\n",
    "                    cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_2.png', label2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if (ymin - offset) > 0:\n",
    "                    img3 =     img[ymin - offset:ymax - offset, xmin:xmax]\n",
    "                    label3 = label[ymin - offset:ymax - offset, xmin:xmax]\n",
    "                    \n",
    "                    if img3.shape[0] > 0 and img3.shape[1] > 0 and label3.shape[0] > 0 and label3.shape[1] > 0: \n",
    "                        cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_3.png', img3)\n",
    "                        cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_3.png', label3)\n",
    "\n",
    "\n",
    "\n",
    "                if (xmin - offset) > 0:\n",
    "                    img4 =     img[ymin:ymax, xmin - offset:xmax - offset]\n",
    "                    label4 = label[ymin:ymax, xmin - offset:xmax - offset]\n",
    "                    \n",
    "                    if img4.shape[0] > 0 and img4.shape[1] > 0 and label4.shape[0] > 0 and label4.shape[1] > 0: \n",
    "\n",
    "                        cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_4.png', img4)\n",
    "                        cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_4.png', label4)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"one of the provided directories doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#random cropping images and labels\n",
    "\n",
    "offsetsToApply = [50,75, 100, 125, 150, 175]\n",
    "dirToSaveImages = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCroppedImages/'\n",
    "dirToSaveLabels = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCroppedLabels/'\n",
    "\n",
    "for row in gt_rows[1:len(gt_rows)-1]:\n",
    "    randomCropImgFromRow(row, False , True, dirToSaveImages, dirToSaveLabels, offsetsToApply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating semantic segmentation Masks for U-Net from Cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# creating masks for u-net CORRECT (using BGR for labels) more efficent to use with augmentations quickly\n",
    "\n",
    "\n",
    "croppedImagesPath = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCropped_images/'\n",
    "croppedLabelsPath = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCropped_labels/'\n",
    "croppedMasksPath  = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCropped_masks/'\n",
    "\n",
    "\n",
    "n = 0\n",
    "for fileName in os.listdir(croppedLabelsPath):\n",
    "    \n",
    "    try:\n",
    "        img = imread(croppedImagesPath + fileName)\n",
    "        imgLabel = imread(croppedLabelsPath + fileName)\n",
    "        #imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGB2BGR)  This is wrong too\n",
    "        imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "\n",
    "        labelsDic = {\n",
    "            'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "            'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "            'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        #    'emptyInside' :np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        }\n",
    "\n",
    "\n",
    "        #print(labelsDic)\n",
    "\n",
    "        mask = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "        mask[labelsDic['fineInside']] = 1\n",
    "        mask[labelsDic['rockInside']] = 1\n",
    "        mask[labelsDic['inapInside']] = 1\n",
    "        #mask[labelsDic['emptyInside']] = 1\n",
    "\n",
    "\n",
    "        mask.dtype='uint8'\n",
    "        cv2.imwrite(croppedMasksPath + fileName, mask)\n",
    "\n",
    "        n += 1\n",
    "    except:\n",
    "        print(\"couldnt open file: \" + fileName)\n",
    "\n",
    "print(\"created  \" + str(n) + \"  binary masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# creating masks for u-net CORRECT (using BGR for labels) with overlay\n",
    "\n",
    "'''\n",
    "croppedImagesPath = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/images/'\n",
    "croppedLabelsPath = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/labels/'\n",
    "croppedMasksPath  = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/masks/'\n",
    "croppedOverlayedPath = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/overlayed/'\n",
    "'''\n",
    "\n",
    "'''\n",
    "croppedImagesPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/finalTrainingSet_images/'\n",
    "croppedLabelsPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/labels_fromJira/'\n",
    "\n",
    "croppedMasksPath  = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/images_frmJira__fullSizeImages_masks/'\n",
    "croppedOverlayedPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/images_frmJira__fullSizeImages_overlays/'\n",
    "'''\n",
    "\n",
    "croppedImagesPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/images_manuallyLabeled/'\n",
    "\n",
    "croppedLabelsPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/labeles_manuallyLabeled/'\n",
    "\n",
    "croppedMasksPath  = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/manuallyLabeled_masks/'\n",
    "\n",
    "croppedOverlayedPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/manuallyLabeled_overlays/'\n",
    "\n",
    "\n",
    "\n",
    "n = 0\n",
    "for fileName in os.listdir(croppedImagesPath):\n",
    "    img = imread(croppedImagesPath + fileName)\n",
    "    imgLabel = imread(croppedLabelsPath + fileName)\n",
    "    #imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGB2BGR)  This is wrong too\n",
    "    imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "\n",
    "    labelsDic = {\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        \n",
    "        #for V3 with the new labels this is the new fmAppropriate mat\n",
    "        'fineInside2' :np.where(np.all(imgLabel == fineInside2, axis=-1)),\n",
    "        \n",
    "        #V2 for Hydraulics U-Net I had the inapInside labels but not for BucyrucAndPnH  for V3 no inapp.\n",
    "        #'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "        \n",
    "    #    'emptyInside' :np.where(np.all(imgLabel == emptyInside, axis=-1)), bad idea\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    #print(labelsDic)\n",
    "    #print(img.shape)\n",
    "\n",
    "    mask = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "    mask[labelsDic['fineInside']] = 1\n",
    "    mask[labelsDic['fineInside2']] = 1\n",
    "    mask[labelsDic['rockInside']] = 1\n",
    "    #mask[labelsDic['inapInside']] = 1\n",
    "    #mask[labelsDic['emptyInside']] = 1\n",
    "\n",
    "\n",
    "\n",
    "    mask.dtype='uint8'\n",
    "    maskOverlay = cv2.cvtColor(mask*255, cv2.COLOR_GRAY2BGR)\n",
    "    maskOverlay[:,:,0] = 0\n",
    "    maskOverlay[:,:,2] = 0\n",
    "\n",
    "\n",
    "    opacity = 0.2\n",
    "    cv2.addWeighted(maskOverlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "\n",
    "\n",
    "    mask.dtype='uint8'\n",
    "    cv2.imwrite(croppedMasksPath + fileName, mask)\n",
    "    cv2.imwrite(croppedOverlayedPath + fileName, img)\n",
    "\n",
    "    n += 1\n",
    "\n",
    "print(\"created  \" + str(n) + \"  binary masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# debugging UNET data generation\n",
    "\n",
    "# creating masks for u-net CORRECT (using BGR for labels) with overlay\n",
    "\n",
    "'''\n",
    "croppedImagesPath = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/images/'\n",
    "croppedLabelsPath = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/labels/'\n",
    "croppedMasksPath  = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/masks/'\n",
    "croppedOverlayedPath = '/home/hooman/dataPreparation/hsTrainingSet/trainingSetForHsUNetCroppedImages-CleanedUp/overlayed/'\n",
    "'''\n",
    "\n",
    "croppedImagesPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/images_fromJira/'\n",
    "croppedLabelsPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/labels_fromJira/'\n",
    "\n",
    "croppedMasksPath  = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/images_frmJira__fullSizeImages_masks/'\n",
    "croppedOverlayedPath = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/UNet_Hydraulics/hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/dataFor__hsUnet_try0_sameAs_try21_1chan_roiDelineatorVersion-1/images_frmJira__fullSizeImages_overlays/'\n",
    "\n",
    "\n",
    "\n",
    "fileName = '1_20161115-073100_0001n0_11017.png'\n",
    "\n",
    "\n",
    "\n",
    "img = imread(croppedImagesPath + fileName)\n",
    "imgLabel = imread(croppedLabelsPath + fileName)\n",
    "#imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGB2BGR)  This is wrong too\n",
    "imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "\n",
    "labelsDic = {\n",
    "    'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "    'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "\n",
    "    #for V3 with the new labels this is the new fmAppropriate mat\n",
    "    'fineInside2' :np.where(np.all(imgLabel == fineInside2, axis=-1)),\n",
    "\n",
    "    #V2 for Hydraulics U-Net I had the inapInside labels but not for BucyrucAndPnH  for V3 no inapp.\n",
    "    #'inapInside' :np.where(np.all(imgLabel == inapInside, axis=-1)),\n",
    "\n",
    "#    'emptyInside' :np.where(np.all(imgLabel == emptyInside, axis=-1)), bad idea\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#print(labelsDic)\n",
    "#print(img.shape)\n",
    "\n",
    "mask = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "mask[labelsDic['fineInside']] = 1\n",
    "mask[labelsDic['fineInside2']] = 1\n",
    "mask[labelsDic['rockInside']] = 1\n",
    "#mask[labelsDic['inapInside']] = 1\n",
    "#mask[labelsDic['emptyInside']] = 1\n",
    "\n",
    "\n",
    "\n",
    "mask.dtype='uint8'\n",
    "maskOverlay = cv2.cvtColor(mask*255, cv2.COLOR_GRAY2BGR)\n",
    "maskOverlay[:,:,0] = 0\n",
    "maskOverlay[:,:,2] = 0\n",
    "\n",
    "\n",
    "opacity = 0.2\n",
    "cv2.addWeighted(maskOverlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "\n",
    "\n",
    "mask.dtype='uint8'\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "#cv2.imwrite(croppedMasksPath + fileName, mask)\n",
    "#cv2.imwrite(croppedOverlayedPath + fileName, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating padded images for U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating the (128, 160, 1) labels \n",
    "generatedMaskPath = '/home/hooman/dataPreparation/hsTestSet/masks0PaddedForUNet/'\n",
    "\n",
    "for fileName in os.listdir(labelsPath):\n",
    "    \n",
    "    label = imread(labelsPath + fileName) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #resize the label map.\n",
    "    imgLabelResized = cv2.resize(label, (640, 480)) \n",
    "\n",
    "    # you can downsample with numpy too. I use opencv to be consistant.    imgDs = img[::4, ::4, :]\n",
    "    imgLabelDs = cv2.resize(imgLabelResized, (0,0), fx=0.25, fy=0.25) \n",
    "\n",
    "    imgLabel = cv2.copyMakeBorder(imgLabelDs,4,4,0,0,cv2.BORDER_CONSTANT,value=(0,0,0,0))\n",
    "    \n",
    "    \n",
    "\n",
    "    mask = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), int)\n",
    "\n",
    "\n",
    "    labelsDic = {\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'emptyInside':np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'   :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'teeth'      :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'case'       :np.where(np.all(imgLabel == case, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "        'dust'       :np.where(np.all(imgLabel == dust, axis=-1)),\n",
    "    }\n",
    "\n",
    "    #channel 1\n",
    "    if 'wmInside' in labelsDic:\n",
    "        mask[labelsDic['wmInside']] = 1\n",
    "\n",
    "\n",
    "    #channel 2\n",
    "    if 'rockInside' in labelsDic:\n",
    "        mask[labelsDic['rockInside']] = 2\n",
    "\n",
    "    if 'fineInside' in labelsDic:\n",
    "        mask[labelsDic['fineInside']] = 2\n",
    "\n",
    "\n",
    "    #channel 3\n",
    "    if 'teeth' in labelsDic:\n",
    "        mask[labelsDic['teeth']] = 3\n",
    "\n",
    "\n",
    "    #channel 4\n",
    "    if 'emptyInside' in labelsDic:\n",
    "        mask[labelsDic['emptyInside']] = 4\n",
    "\n",
    "    if 'shadow' in labelsDic:\n",
    "        mask[labelsDic['shadow']] = 4\n",
    "\n",
    "    if 'dust' in labelsDic:\n",
    "        mask[labelsDic['dust']] = 4\n",
    "\n",
    "\n",
    "    #channel 5\n",
    "    if 'case' in labelsDic:\n",
    "        mask[labelsDic['case']] = 5\n",
    "\n",
    "        \n",
    "    cv2.imwrite(generatedMaskPath + fileName, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Correct Creating the (128, 160, 6) labels \n",
    "generatedMaskPath = '/home/hooman/dataPreparation/hsTrainingSet/masks6Chan/'\n",
    "\n",
    "for fileName in os.listdir(labelsPath):\n",
    "    \n",
    "    label = imread(labelsPath + fileName) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #resize the label map.\n",
    "    imgLabelResized = cv2.resize(label, (640, 480)) \n",
    "\n",
    "    # you can downsample with numpy too. I use opencv to be consistant.    imgDs = img[::4, ::4, :]\n",
    "    imgLabelDs = cv2.resize(imgLabelResized, (0,0), fx=0.25, fy=0.25) \n",
    "\n",
    "    imgLabel = cv2.copyMakeBorder(imgLabelDs,4,4,0,0,cv2.BORDER_CONSTANT,value=(0,0,0,0))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    mask = np.zeros((imgLabel.shape[0], imgLabel.shape[1], 6), bool)\n",
    "\n",
    "    backGroundMask = np.ones((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "\n",
    "    \n",
    "    \n",
    "    labelsDic = {\n",
    "        'rockInside' :np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' :np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'emptyInside':np.where(np.all(imgLabel == emptyInside, axis=-1)),\n",
    "        'wmInside'   :np.where(np.all(imgLabel == wmInside, axis=-1)),\n",
    "        'teeth'      :np.where(np.all(imgLabel == teeth, axis=-1)),\n",
    "        'case'       :np.where(np.all(imgLabel == case, axis=-1)),\n",
    "        'shadow'     :np.where(np.all(imgLabel == shadow, axis=-1)),\n",
    "        'dust'       :np.where(np.all(imgLabel == dust, axis=-1)),\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    #channel 1\n",
    "    if 'wmInside' in labelsDic:\n",
    "        mask[labelsDic['wmInside'][0], labelsDic['wmInside'][1], 1] = 1\n",
    "        backGroundMask[labelsDic['wmInside']] = 0\n",
    "\n",
    "\n",
    "    #channel 2\n",
    "    if 'rockInside' in labelsDic:\n",
    "        mask[labelsDic['rockInside'][0], labelsDic['rockInside'][1], 2] = 1\n",
    "        backGroundMask[labelsDic['rockInside']] = 0\n",
    "\n",
    "    if 'fineInside' in labelsDic:\n",
    "        mask[labelsDic['fineInside'][0], labelsDic['fineInside'][1], 2] = 1\n",
    "        backGroundMask[labelsDic['fineInside']] = 0\n",
    "\n",
    "\n",
    "    #channel 3\n",
    "    if 'teeth' in labelsDic:\n",
    "        mask[labelsDic['teeth'][0], labelsDic['teeth'][1], 3] = 1\n",
    "        backGroundMask[labelsDic['teeth']] = 0\n",
    "\n",
    "\n",
    "    #channel 4\n",
    "    if 'emptyInside' in labelsDic:\n",
    "        mask[labelsDic['emptyInside'][0], labelsDic['emptyInside'][1], 4] = 1\n",
    "        backGroundMask[labelsDic['emptyInside']] = 0\n",
    "\n",
    "    if 'shadow' in labelsDic:\n",
    "        mask[labelsDic['shadow'][0], labelsDic['shadow'][1], 4] = 1\n",
    "        backGroundMask[labelsDic['shadow']] = 0\n",
    "\n",
    "    if 'dust' in labelsDic:\n",
    "        mask[labelsDic['dust'][0], labelsDic['dust'][1], 4] = 1\n",
    "        backGroundMask[labelsDic['dust']] = 0\n",
    "\n",
    "\n",
    "    #channel 5\n",
    "    if 'case' in labelsDic:\n",
    "        mask[labelsDic['case'][0], labelsDic['case'][1], 5] = 1\n",
    "        backGroundMask[labelsDic['case']] = 0\n",
    "\n",
    "\n",
    "    #channel 0\n",
    "    mask[:,:, 0] = backGroundMask\n",
    "    \n",
    "    \n",
    "    np.save(generatedMaskPath + fileName.replace(\".png\",\"\"), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Overlaying Segmentation results for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Overlaying predicted segmentation resutls\n",
    "unet_pathToSavedResults = '/home/hooman/UNet/hsUnet_try13/predicted1chanImages/'\n",
    "testImagesPath = '/home/hooman/dataPreparation/hsTestSet/cropped/croppedImages/'\n",
    "\n",
    "# set this to '' to not save results but desplay the images instead\n",
    "pathToSaveOverlayResults = '/home/hooman/UNet/hsUnet_try13/croppedImagesPredictionsOverlayed/'\n",
    "\n",
    "\n",
    "for imgId in os.listdir(testImagesPath):\n",
    "\n",
    "    img = imread(testImagesPath + imgId)\n",
    "\n",
    "    pred = imread(unet_pathToSavedResults + imgId)\n",
    "    predRes = cv2.resize(pred, (img.shape[1], img.shape[0])) \n",
    "\n",
    "    predCol = cv2.cvtColor(predRes*255, cv2.COLOR_GRAY2BGR)\n",
    "    predCol[:,:,0] = 0\n",
    "    predCol[:,:,2] = 0\n",
    "\n",
    "    \n",
    "    opacity = 0.1\n",
    "    overIm = cv2.addWeighted(predCol, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "    \n",
    "    if pathToSaveOverlayResults != '':\n",
    "        cv2.imwrite(pathToSaveOverlayResults + imgId, overIm)\n",
    "    else:\n",
    "        imshow(overIm)\n",
    "        plt.title('overLayedImage')\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    imgRes = cv2.resize(img, (128, 128)) \n",
    "    imshow(img)\n",
    "    plt.title('img')\n",
    "    plt.show()\n",
    "\n",
    "    imshow(imgRes)\n",
    "    plt.title('imgRes')\n",
    "    plt.show()\n",
    "\n",
    "    imshow(pred)\n",
    "    plt.title('pred')\n",
    "    plt.show()\n",
    "\n",
    "    imshow(predRes)\n",
    "    plt.title('predRes')\n",
    "    plt.show()\n",
    "\n",
    "    imshow(predCol)\n",
    "    plt.title('predCol')\n",
    "    plt.show()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Overlaying Ground Truth Masks multiple images\n",
    "pathToMasks =  '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCropped_masks/'\n",
    "pathToImages = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCropped_images/'\n",
    "pathToSaveOverlayResults = '/home/hooman/dataPreparation/hsTrainingSetWithRelabeled/cropped/randomCropped_overlay/'\n",
    "\n",
    "\n",
    "for imgId in os.listdir(pathToImages):\n",
    "\n",
    "    img = imread(pathToImages + imgId)\n",
    "    mask = imread(pathToMasks + imgId)\n",
    "\n",
    "    mask.dtype='uint8'\n",
    "    maskOverlay = cv2.cvtColor(mask*255, cv2.COLOR_GRAY2BGR)\n",
    "    maskOverlay[:,:,0] = 0\n",
    "    maskOverlay[:,:,2] = 0\n",
    "\n",
    "\n",
    "    opacity = 0.2\n",
    "    cv2.addWeighted(maskOverlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "    \n",
    "\n",
    "    mask.dtype='uint8'\n",
    "    \n",
    "    \n",
    "    cv2.imwrite(pathToSaveOverlayResults + imgId, img)\n",
    "    \n",
    "    #imshow(img)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Overlaying Ground Truth Masks single image\n",
    "\n",
    "pathToMasks = '/home/hooman/dataPreparation/hsTrainingSet/cropped/wrong_croppedMasks/'\n",
    "pathToImages = '/home/hooman/dataPreparation/hsTrainingSet/cropped/croppedImages/'\n",
    "\n",
    "imgId = '1_20161116-155500_0001n0_20737.png'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img = imread(pathToImages + imgId)\n",
    "mask = imread(pathToMasks + imgId)\n",
    "\n",
    "mask.dtype='uint8'\n",
    "maskOverlay = cv2.cvtColor(mask*255, cv2.COLOR_GRAY2BGR)\n",
    "maskOverlay[:,:,0] = 0\n",
    "maskOverlay[:,:,2] = 0\n",
    "\n",
    "\n",
    "opacity = 0.2\n",
    "cv2.addWeighted(maskOverlay, opacity, img, 1 - opacity, 0, img)\n",
    "\n",
    "\n",
    "\n",
    "mask.dtype='uint8'\n",
    "\n",
    "imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Correcting ROI Points to avoid self intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Converting ROI points from ratio to abs and drawing them on image\n",
    "\n",
    "im = cv2.imread('/media/hooman/New Volume/FM_PROJECT_STORAGE/productionBugs/invalidROI_dueToselfIntersecting/RE__Invalid_FM_ROI/download.jpg')\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "jq = [\n",
    "    (0.1390625,0.5333333333333333),\n",
    "    (0.175,0.9979166666666667),\n",
    "    (0.1625,0.9145833333333333),\n",
    "    (0.209375,0.8958333333333334),\n",
    "    (0.1765625,0.825),\n",
    "    (0.26875,0.7854166666666667),\n",
    "    (0.2421875,0.7020833333333333),\n",
    "    (0.33125,0.6833333333333333),\n",
    "    (0.26875,0.71875),\n",
    "    (0.35,0.7375),\n",
    "    (0.4703125,0.6125),\n",
    "    (0.5265625,0.6145833333333334),\n",
    "    (0.5390625,0.7145833333333333),\n",
    "    (0.503125,0.7208333333333333),\n",
    "    (0.690625,0.825),\n",
    "    (0.6453125,0.8916666666666667),\n",
    "    (0.8328125,0.8604166666666667),\n",
    "    (0.809375,0.5541666666666667),\n",
    "    (0.728125,0.5520833333333334),\n",
    "    (0.6953125,0.47291666666666665),\n",
    "    (0.340625,0.38333333333333336)\n",
    "]\n",
    "\n",
    "\n",
    "jqConv = []\n",
    "for el in jq:\n",
    "    jqConv.append( ((int(el[0]*im.shape[1])),int(el[1]*im.shape[0])) )\n",
    "\n",
    "\n",
    "print(im.shape)\n",
    "print(jqConv)\n",
    "\n",
    "\n",
    "for i in range(len(jqConv)-1):\n",
    "    cv2.line(im, jqConv[i], jqConv[i+1], (255,0,0),2)\n",
    "    \n",
    "cv2.line(im, jqConv[len(jqConv)-1], jqConv[0], (255,0,0),2)\n",
    "\n",
    "cv2.imwrite('/home/hooman/Downloads/RE__Invalid_FM_ROI/res.png', im)\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#don't draw the edge that intersects boundary\n",
    "\n",
    "im = cv2.imread('/home/hooman/Downloads/RE__Invalid_FM_ROI/download.jpg')\n",
    "\n",
    "\n",
    "lines = []\n",
    "for i in range(len(jqConv)):\n",
    "    if i < len(jqConv)-1:\n",
    "        newLine = (jqConv[i], jqConv[i+1])\n",
    "    else:\n",
    "        newLine = (jqConv[len(jqConv)-1], jqConv[0])\n",
    "    \n",
    "    isOk = True\n",
    "    for oldLine in lines[:-1]:\n",
    "        if linesIntersect(oldLine, newLine):\n",
    "            print(i)\n",
    "            isOk = False\n",
    "            \n",
    "    if isOk:\n",
    "        cv2.line(im, newLine[0], newLine[1], (255,0,0),2)\n",
    "        lines.append( newLine )\n",
    "\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#function checking if two lines intersec\n",
    "\n",
    "def pointsCross(A,B,C):\n",
    "    return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])\n",
    "\n",
    "# Return true if line segments AB and CD intersect\n",
    "def linesIntersect(line1, line2):\n",
    "    A = np.array(line1[0])\n",
    "    B = np.array(line1[1])\n",
    "\n",
    "    C = np.array(line2[0])\n",
    "    D = np.array(line2[1])\n",
    "    \n",
    "    return pointsCross(A,C,D) != pointsCross(B,C,D) and pointsCross(A,B,C) != pointsCross(A,B,D)\n",
    "\n",
    "def unitTest():\n",
    "    line1 = ( (0, 10), (10,0) )\n",
    "    line2 = ( (15, 12), (10,10) ) \n",
    "    \n",
    "    line3 = ((89, 256), (112, 479))\n",
    "    line4 = ((104, 439), (134, 430))\n",
    "\n",
    "    assert linesIntersect(line1,line2) == False\n",
    "    assert linesIntersect(line3,line4) == True\n",
    "    \n",
    "unitTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert the ROI to numpy and save\n",
    "k = np.zeros((21,1,2), np.int32)\n",
    "\n",
    "for i in range(0,21,1):\n",
    "    k[i, 0, :] = jqConv[i]\n",
    "    \n",
    "np.save('path/selfIntersectingROI',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def areSelfIntersecting_list(boundary_points):\n",
    "    \n",
    "    lines = []\n",
    "\n",
    "    for i in range(len(boundary_points)-1):\n",
    "        newLine = (boundary_points[i], boundary_points[i+1])\n",
    "\n",
    "        for oldLine in lines[:-1]:\n",
    "            if linesIntersect(oldLine, newLine):\n",
    "                return True\n",
    "\n",
    "        lines.append( newLine )\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def areSelfIntersecting_numpy(boundary_points):\n",
    "    \n",
    "    if len(boundary_points) <= 0 or boundary_points.shape[0] < 3:\n",
    "        print(\"The areRoiBoundaryPointsIntersecting received an invalid roi boundary.\\n\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for i in range(len(boundary_points)-1):\n",
    "        newLine = (boundary_points[i], boundary_points[i+1])\n",
    "\n",
    "        for oldLine in lines[:-1]:\n",
    "            if linesIntersect(oldLine, newLine):\n",
    "                print(\"The areRoiBoundaryPointsIntersecting observed that the reduced ROI\\\n",
    "                 boundary self intersects.\\n\")\n",
    "                return True\n",
    "\n",
    "        lines.append( newLine )\n",
    "        \n",
    "    print(\"The areRoiBoundaryPointsIntersecting validated that the reduced ROI boundary\\\n",
    "     does not self intersect.\\n\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test the functions\n",
    "print(areSelfIntersecting(jqConv[2:]))\n",
    "print(areSelfIntersecting(jqConv))\n",
    "\n",
    "print(areRoiBoundaryPointsIntersecting(k[2:,0,:]))\n",
    "print(areRoiBoundaryPointsIntersecting(k[:,0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt_rows = readCsvRows(\"/home/hooman/backhoeOpticalScene/boxDetectors/try3_ssdMultiClass_withInappInMatInside_upsideDownShovelsRemoved/fineAndRockAndInapInMatInside_backhoe_shuffled_UpsideDownShovelsRemoved.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating full size masks (used for FMDL 3.0)\n",
    "\n",
    "imagesPath = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/backhoe/fmdl-backhoe-trainingData-new_may10-2019/images_fromJira/'\n",
    "labelsPath = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/backhoe/fmdl-backhoe-trainingData-new_may10-2019/labels_fromJira/'\n",
    "\n",
    "masksSavePath = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/backhoe/fmdl-backhoe-trainingData-new_may10-2019/full_masks/'\n",
    "\n",
    "overlaySavePath = '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/backhoe/fmdl-backhoe-trainingData-new_may10-2019/full_overlays/'\n",
    "\n",
    "\n",
    "for fileName in os.listdir(imagesPath):\n",
    "\n",
    "    print(\"prcessing imgId: \" + fileName + \"\\n\")\n",
    "\n",
    "    #read the image from csv row\n",
    "    img = imread(imagesPath + fileName)\n",
    "    \n",
    "    imgLabel = imread(labelsPath + fileName)\n",
    "    imgLabel = cv2.cvtColor(imgLabel, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "\n",
    "    #Create a mas from cropped image\n",
    "    labelsDic = {\n",
    "        'rockInside' : np.where(np.all(imgLabel == rockInside, axis=-1)),\n",
    "        'fineInside' : np.where(np.all(imgLabel == fineInside, axis=-1)),\n",
    "        'fmInapp'    : np.where(np.all(imgLabel == fmInapp, axis=-1)),\n",
    "    }\n",
    "\n",
    "\n",
    "    mask = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "    mask2 = np.zeros((imgLabel.shape[0], imgLabel.shape[1]), bool)\n",
    "    mask.dtype='uint8'\n",
    "    mask2.dtype='uint8'\n",
    "\n",
    "    mask[labelsDic['fineInside']] = 1\n",
    "    mask[labelsDic['rockInside']] = 1\n",
    "    mask[labelsDic['fmInapp']] = 1\n",
    "\n",
    "\n",
    "\n",
    "    #Remove the points that are not in the largest contour from mask\n",
    "    _,contours,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE, offset=(0, 0))\n",
    "\n",
    "    if contours:\n",
    "        max_cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # cv2.drawContours(mask2, max_cnt, -1, 1, cv2.FILLED, 8) thos does NOT fill up the inside of contour\n",
    "        cv2.drawContours(mask2, [max_cnt], -1, 1, cv2.FILLED, 8)\n",
    "\n",
    "\n",
    "    #if no contours found, mask2 will remain as all 0s\n",
    "    cv2.imwrite(masksSavePath + fileName, mask2)\n",
    "\n",
    "\n",
    "    #get the overlay\n",
    "    maskOverlay = cv2.cvtColor(mask2*255, cv2.COLOR_GRAY2BGR)\n",
    "    maskOverlay[:,:,0] = 0\n",
    "    maskOverlay[:,:,2] = 0\n",
    "\n",
    "    opacity = 0.2\n",
    "    cv2.addWeighted(maskOverlay, opacity, img, 1 - opacity, 0, img)\n",
    "    \n",
    "    cv2.imwrite(overlaySavePath + fileName, img)\n",
    "\n",
    "\n",
    "    '''        \n",
    "    plt.imshow(mask)\n",
    "    plt.title('mask1')\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(mask2)\n",
    "    plt.title('mask2 final')\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title('overlayed img')\n",
    "    plt.show()\n",
    "    break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# random crop the FULL masks create above. You need FULL images and Masks here not cropped. \n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "offsetsToApply = [50,75, 100, 125, 150, 175]\n",
    "\n",
    "dirToSaveImages = '/home/hooman/backhoeOpticalScene/roiDelineators/try4-csvFrom-ssdTry3-withInapp-reducedNumberOfempyBuckets-BatchSize4/randomCroppedImages/'\n",
    "\n",
    "dirToSaveMasks = '/home/hooman/backhoeOpticalScene/roiDelineators/try4-csvFrom-ssdTry3-withInapp-reducedNumberOfempyBuckets-BatchSize4/randomCroppedMasks/'\n",
    "\n",
    "\n",
    "dirToReadImages = '/home/hooman/backhoeOpticalScene/roiDelineators/try4-csvFrom-ssdTry3-withInapp-reducedNumberOfempyBuckets-BatchSize4/images/'\n",
    "\n",
    "dirToReadMasks = '/home/hooman/backhoeOpticalScene/roiDelineators/try4-csvFrom-ssdTry3-withInapp-reducedNumberOfempyBuckets-BatchSize4/full-masks_withInapp/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for row in gt_rows[1:len(gt_rows)-1]:\n",
    "    \n",
    "    vals = row.split(',')\n",
    "\n",
    "    if os.path.exists(dirToReadImages + vals[0]) and os.path.exists(dirToReadMasks + vals[0]):\n",
    "        \n",
    "        try:\n",
    "            imgPil = Image.open(dirToReadImages + vals[0])\n",
    "            img = np.array(imgPil) \n",
    "            imgPil.close()\n",
    "\n",
    "            labelPil = Image.open(dirToReadMasks + vals[0])\n",
    "            label = np.array(labelPil)\n",
    "            labelPil.close()\n",
    "\n",
    "\n",
    "\n",
    "            xmin, xmax, ymin, ymax = vals[2:6]\n",
    "\n",
    "            if xmin != \"\" and xmax != \"\" and ymin != \"\" and ymax != \"\":\n",
    "                (xmin, xmax, ymin, ymax) = (int(round(float(xmin))), int(round(float(xmax))), int(round(float(ymin))), int(round(float(ymax))))\n",
    "\n",
    "                imgWidth = img.shape[1]\n",
    "                imgHeight = img.shape[0]\n",
    "\n",
    "\n",
    "                imgAct = img[ymin:ymax, xmin:xmax,]\n",
    "                labelAct = label[ymin:ymax, xmin:xmax]\n",
    "\n",
    "                if imgAct.shape[0] > 0 and imgAct.shape[1] > 0 and labelAct.shape[0] > 0 and labelAct.shape[1] > 0: \n",
    "                    cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '_RCAct.png'), imgAct)\n",
    "                    cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '_RCAct.png'), labelAct)\n",
    "\n",
    "\n",
    "\n",
    "                for offset in offsetsToApply:\n",
    "\n",
    "                    img1 =     img[ymin + offset:ymax + offset, xmin:xmax]\n",
    "                    label1 = label[ymin + offset:ymax + offset, xmin:xmax]\n",
    "\n",
    "                    if img1.shape[0] > 0 and img1.shape[1] > 0 and label1.shape[0] > 0 and label1.shape[1] > 0: \n",
    "                        cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_1.png', img1)\n",
    "                        cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_1.png', label1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    img2 =     img[ymin:ymax, xmin + offset:xmax + offset]\n",
    "                    label2 = label[ymin:ymax, xmin + offset:xmax + offset]\n",
    "\n",
    "                    if img2.shape[0] > 0 and img2.shape[1] > 0 and label2.shape[0] > 0 and label2.shape[1] > 0: \n",
    "                        cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_2.png', img2)\n",
    "                        cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_2.png', label2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    if (ymin - offset) > 0:\n",
    "                        img3 =     img[ymin - offset:ymax - offset, xmin:xmax]\n",
    "                        label3 = label[ymin - offset:ymax - offset, xmin:xmax]\n",
    "\n",
    "                        if img3.shape[0] > 0 and img3.shape[1] > 0 and label3.shape[0] > 0 and label3.shape[1] > 0: \n",
    "                            cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_3.png', img3)\n",
    "                            cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_3.png', label3)\n",
    "\n",
    "\n",
    "\n",
    "                    if (xmin - offset) > 0:\n",
    "                        img4 =     img[ymin:ymax, xmin - offset:xmax - offset]\n",
    "                        label4 = label[ymin:ymax, xmin - offset:xmax - offset]\n",
    "\n",
    "                        if img4.shape[0] > 0 and img4.shape[1] > 0 and label4.shape[0] > 0 and label4.shape[1] > 0: \n",
    "\n",
    "                            cv2.imwrite(dirToSaveImages + vals[0].replace('.png', '') + '_RC' + str(offset) + '_4.png', img4)\n",
    "                            cv2.imwrite(dirToSaveMasks + vals[0].replace('.png', '') + '_RC' + str(offset) + '_4.png', label4)\n",
    "        \n",
    "        except:\n",
    "            print(\"coud not open file: \" + vals[0] + \"\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"one of the provided directories doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='red'> Sandbox Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Showing Images side-by-side for mahdi\n",
    "predsDir1 = '/home/hooman/justHead_testSet_fromTensent/examples/labels_above40-40/'\n",
    "predsDir2 = '/home/hooman/justHead_testSet_fromTensent/examples/preds_all/'\n",
    "predsDir3 = '/home/hooman/justHead_testSet_fromTensent/examples/preds_onlyAbove40-40/'\n",
    "\n",
    "dirToSaveResults = '/home/hooman/justHead_testSet_fromTensent/examples/side/'\n",
    "\n",
    "for imgId in os.listdir('/home/hooman/justHead_testSet_fromTensent/examples/good1/'):\n",
    "\n",
    "    pred1 = imread(predsDir1 + imgId)\n",
    "    pred2 = imread(predsDir2 + imgId)\n",
    "    pred3 = imread(predsDir3 + imgId)\n",
    "\n",
    "    totalW = pred1.shape[1] + pred2.shape[1] + pred2.shape[1] + 20\n",
    "    combImg = np.zeros((pred1.shape[0],totalW, 3), np.uint8)\n",
    "\n",
    "    combImg[:, 0:pred1.shape[1], :] = pred1\n",
    "    offset1 = pred1.shape[1] + 10\n",
    "    combImg[:, offset1:pred2.shape[1]+offset1, :] = pred2\n",
    "    offset2 = pred2.shape[1]+offset1 + 10\n",
    "    combImg[:, offset2:pred3.shape[1]+offset2, :] = pred3\n",
    "    \n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    textloc1 = int(pred1.shape[1] / 2) - 70\n",
    "    textloc2 = int((offset1 + pred2.shape[1]+offset1)/2)- 50\n",
    "    textloc3 = int((offset2 + pred3.shape[1]+offset2)/2)- 90\n",
    "    \n",
    "    cv2.putText(combImg,'groundTruth',(textloc1,7), font, 1,(0,255,0), 2, 0)\n",
    "    cv2.putText(combImg,'allPreds',(textloc2,7), font, 1,(0,255,0), 2, 0)\n",
    "    cv2.putText(combImg,'predsAbove40-40',(textloc3,7), font, 1,(0,255,0), 2, 0)\n",
    "\n",
    "    cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "    #cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "    imsave(dirToSaveResults + imgId, combImg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#debug split\n",
    "imagesPath = '/home/hooman/justHead_testSet_fromTensent/images/'\n",
    "splitImagesPath = '/home/hooman/justHead_testSet_fromTensent/predsImage/'\n",
    "imageName = '000368.jpg'\n",
    "\n",
    "image = imread(imagesPath + imageName)\n",
    "\n",
    "imH, imW, _ = image.shape\n",
    "hh = int(imH/2)\n",
    "hw = int(imW/2)\n",
    "overlap = 55\n",
    "\n",
    "imL1 = image[0:hh+overlap,0:hw+overlap,:]\n",
    "imL2 = image[hh:imH,0:hw+overlap,:]\n",
    "imR1 = image[0:hh+overlap,hw:imW,:]\n",
    "imR2 = image[hh:imH,hw:imW,:]\n",
    "\n",
    "imshow(image)\n",
    "plt.show()\n",
    "\n",
    "imsave(splitImagesPath + 'imL1_' + imageName, imL1)\n",
    "imshow(imL1)\n",
    "plt.show()\n",
    "\n",
    "imsave(splitImagesPath + 'imL2_' + imageName, imL2)\n",
    "imshow(imL2)\n",
    "plt.show()\n",
    "\n",
    "imsave(splitImagesPath + 'imR1_' + imageName, imR1)\n",
    "imshow(imR1)\n",
    "plt.show()\n",
    "\n",
    "imsave(splitImagesPath + 'imR2_' + imageName, imR2)\n",
    "imshow(imR2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "outFile = open(\"/home/hooman/justHead_testSet_fromTensent/testSet_containingHead.txt\", 'w')\n",
    "locPath = \"/home/hooman/justHead_testSet_fromTensent/images_containingHead/\"\n",
    "putInFilePath = \"/home/hooman/justHead_testSet_fromTensent/together_imagesContainingHead_LabelsAllSize/\"\n",
    "\n",
    "namesList = os.listdir(locPath)\n",
    "\n",
    "for fileName in namesList:\n",
    "    outStr = putInFilePath + fileName + '\\n'\n",
    "    outFile.write(outStr)\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imagesPath = \"/home/hooman/justHead_testSet_fromTensent/images_containingHeadGT/\"\n",
    "labelsPath = \"/home/hooman/justHead_testSet_fromTensent/yoloLabels_40-40/\"\n",
    "path2MvImagesWithNoLabels = '/home/hooman/justHead_testSet_fromTensent/labels_above40x40_containingHeadGT/'\n",
    "\n",
    "labelsDic = {}\n",
    "for labelName in os.listdir(imagesPath):\n",
    "    labelsDic[labelName.replace(\".jpg\", \".txt\")] = 1\n",
    "    \n",
    "for imgName in labelsDic.keys():\n",
    "    shutil.copy(labelsPath + imgName, path2MvImagesWithNoLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predsDir1 = '/home/hooman/justHead_testSet_fromTensent/vis/'\n",
    "predsDir2 = '/home/hooman/justHead_testSet_fromTensent/vis_GT_40-40/'\n",
    "\n",
    "dirToSaveResults = '/home/hooman/justHead_testSet_fromTensent/co/'\n",
    "\n",
    "for imgId in os.listdir(predsDir1):\n",
    "\n",
    "    pred1 = imread(predsDir1 + imgId)\n",
    "    pred2 = imread(predsDir2 + imgId)\n",
    "\n",
    "    totalW = pred1.shape[1] + pred2.shape[1] + 20\n",
    "    combImg = np.zeros((pred1.shape[0],totalW, 3), np.uint8)\n",
    "\n",
    "    combImg[:, 0:pred1.shape[1], :] = pred1\n",
    "    offset1 = pred1.shape[1] + 10\n",
    "    combImg[:, offset1:pred2.shape[1]+offset1, :] = pred2\n",
    "    \n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    textloc1 = int(pred1.shape[1] / 2) - 70\n",
    "    textloc2 = int((offset1 + pred2.shape[1]+offset1)/2)- 50\n",
    "    \n",
    "    cv2.putText(combImg,'allPreds',(textloc1,7), font, 1,(0,255,0), 2, 0)\n",
    "    cv2.putText(combImg,'groundTruth',(textloc2,7), font, 1,(0,255,0), 2, 0)\n",
    "\n",
    "    cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "    #cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "    imsave(dirToSaveResults + imgId, combImg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pathIm = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/images_aug/'\n",
    "\n",
    "pathLabels = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_aug/'\n",
    "\n",
    "path2Vis = '/home/hooman/hardHat_dataset/augmented_yoloFormat_1200_1000here-200FromOther/labels_aug/'\n",
    "\n",
    "for imId in os.listdir(pathIm):\n",
    "    img = imread(pathIm + imId)\n",
    "    \n",
    "    try:\n",
    "        labelFile = open(pathLabels + imId.replace(\".jpg\",\".txt\"), 'r')\n",
    "    except:\n",
    "        print(\"Error didnot find labels for image\")\n",
    "        print(imId)\n",
    "#         shutil.copy(pathIm + imId, \"/home/hooman/brainwash_headDetection_dataset/brainwash/\"\\\n",
    "#                     \"11_13_2014_imagesWithNoCombinedLabels/\" +imId)\n",
    "        break\n",
    "\n",
    "    labelLines = labelFile.readlines()\n",
    "    for line in labelLines:\n",
    "        visualizeLabelLine(line.split(\" \"), img)\n",
    "\n",
    "    #print(path2Vis + imagePath.split(\"/\")[-1])\n",
    "    try:\n",
    "        imsave(path2Vis + imId, img)\n",
    "    except:\n",
    "        print(\"Error could not save image\")\n",
    "        print(imId)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zone = {\n",
    "    'xmin' : int(450),\n",
    "    'xmax' : int(750),\n",
    "    'ymin' : int(350),\n",
    "    'ymax' : int(750),\n",
    "}\n",
    "zone['xcent'] = int((zone['xmin'] + zone['xmax'])/2)\n",
    "zone['ycent'] = int((zone['ymin'] + zone['ymax'])/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xmin': 450,\n",
       " 'xmax': 750,\n",
       " 'ymin': 350,\n",
       " 'ymax': 750,\n",
       " 'xcent': 600,\n",
       " 'ycent': 550}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zone['ycent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
